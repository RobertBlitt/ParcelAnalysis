{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Hawaii Cesspool Matrix Analysis\n",
      "Master Parcel Attribute Table Creation\n",
      "======================================================================\n",
      "Academic Framework - University of Hawaii WRRC\n",
      "Started: 2025-09-25 15:27:52\n",
      "Project workspace: C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\n",
      "Data directory: C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\data\\gis_downloads\n",
      "Outputs directory: C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\outputs\n",
      "Using existing: C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\outputs\\foundation\n",
      "\n",
      "==================================================\n",
      "STEP 1: VERIFY AVAILABLE DATA\n",
      "==================================================\n",
      "Available layers in your map:\n",
      "   1. World Terrain Reference\n",
      "   2. World Terrain Base\n",
      "   3. World Hillshade\n",
      "\n",
      "‚úÖ Found 3 layers\n"
     ]
    }
   ],
   "source": [
    "# Hawaii Cesspool Matrix Analysis - Master Parcel Attribute Table\n",
    "# University of Hawaii Water Resources Research Center\n",
    "# Creates comprehensive parcel attribute table for Matrix sieve analysis\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Hawaii Cesspool Matrix Analysis\")\n",
    "print(\"Master Parcel Attribute Table Creation\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Academic Framework - University of Hawaii WRRC\")\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import arcpy\n",
    "\n",
    "print(f\"Started: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Set up workspace and environment\n",
    "arcpy.env.overwriteOutput = True\n",
    "workspace = arcpy.mp.ArcGISProject(\"CURRENT\").homeFolder\n",
    "data_dir = os.path.join(workspace, \"data\", \"gis_downloads\")\n",
    "outputs_dir = os.path.join(workspace, \"outputs\")\n",
    "\n",
    "print(f\"Project workspace: {workspace}\")\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "print(f\"Outputs directory: {outputs_dir}\")\n",
    "\n",
    "# Create outputs structure\n",
    "foundation_dir = os.path.join(outputs_dir, \"foundation\")\n",
    "if not os.path.exists(foundation_dir):\n",
    "    os.makedirs(foundation_dir)\n",
    "    print(f\"Created: {foundation_dir}\")\n",
    "else:\n",
    "    print(f\"Using existing: {foundation_dir}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEP 1: VERIFY AVAILABLE DATA\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check what layers are available in your map\n",
    "map_obj = arcpy.mp.ArcGISProject(\"CURRENT\").activeMap\n",
    "available_layers = [layer.name for layer in map_obj.listLayers()]\n",
    "\n",
    "print(\"Available layers in your map:\")\n",
    "for i, layer in enumerate(available_layers, 1):\n",
    "    print(f\"  {i:2d}. {layer}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Found {len(available_layers)} layers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STEP 2: LOCATE DATA FILES\n",
      "==================================================\n",
      "Searching in: C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\data\\gis_downloads\\wells\n",
      "‚úÖ Wells directory found\n",
      "Found 2 shapefiles in wells directory:\n",
      "  1. CPs_Distance_to_Domestic_Wells.shp\n",
      "     ‚Üí Likely DOMESTIC wells data\n",
      "  2. CPs_Distance_to_Municipal_Wells.shp\n",
      "     ‚Üí Likely MUNICIPAL wells data\n",
      "\n",
      "Searching for soils in: C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\data\\gis_downloads\\soils\n",
      "‚úÖ Soils directory found\n",
      "Found 2 soil shapefiles:\n",
      "  1. CPs_Soil_Suitability_Rank.shp\n",
      "  2. HIstate_nrcs_join2.shp\n",
      "\n",
      "Checking for geodatabases in project...\n",
      "Found 2 geodatabases:\n",
      "  1. ParcelAnalysis.gdb\n",
      "     Contains 5 feature classes:\n",
      "       - Parcels_Hawaii\n",
      "       - tmk_state\n",
      "       - Parcels_Honolulu\n",
      "       - Parcels_Kauai\n",
      "       - Parcels_Maui\n",
      "  2. mlra_a_hi.gdb\n",
      "     No feature classes found\n",
      "\n",
      "üéØ Data inventory complete - ready to identify our key files!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEP 2: LOCATE DATA FILES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Search for well distance data files\n",
    "wells_dir = os.path.join(data_dir, \"wells\")\n",
    "print(f\"Searching in: {wells_dir}\")\n",
    "\n",
    "if os.path.exists(wells_dir):\n",
    "    print(\"‚úÖ Wells directory found\")\n",
    "    \n",
    "    # Find shapefiles in wells directory\n",
    "    wells_files = []\n",
    "    for root, dirs, files in os.walk(wells_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.shp'):\n",
    "                wells_files.append(os.path.join(root, file))\n",
    "    \n",
    "    print(f\"Found {len(wells_files)} shapefiles in wells directory:\")\n",
    "    for i, file in enumerate(wells_files, 1):\n",
    "        filename = os.path.basename(file)\n",
    "        print(f\"  {i}. {filename}\")\n",
    "        \n",
    "        # Try to identify municipal vs domestic\n",
    "        if 'municipal' in filename.lower() or 'public' in filename.lower():\n",
    "            print(f\"     ‚Üí Likely MUNICIPAL wells data\")\n",
    "        elif 'domestic' in filename.lower() or 'private' in filename.lower():\n",
    "            print(f\"     ‚Üí Likely DOMESTIC wells data\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Wells directory not found, checking other locations...\")\n",
    "\n",
    "# Search for soils data\n",
    "soils_dir = os.path.join(data_dir, \"soils\")\n",
    "print(f\"\\nSearching for soils in: {soils_dir}\")\n",
    "\n",
    "if os.path.exists(soils_dir):\n",
    "    print(\"‚úÖ Soils directory found\")\n",
    "    \n",
    "    soils_files = []\n",
    "    for root, dirs, files in os.walk(soils_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.shp'):\n",
    "                soils_files.append(os.path.join(root, file))\n",
    "    \n",
    "    print(f\"Found {len(soils_files)} soil shapefiles:\")\n",
    "    for i, file in enumerate(soils_files, 1):\n",
    "        filename = os.path.basename(file)\n",
    "        print(f\"  {i}. {filename}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Soils directory not found\")\n",
    "\n",
    "# Also check if we have any geodatabase files\n",
    "print(f\"\\nChecking for geodatabases in project...\")\n",
    "gdb_files = []\n",
    "for root, dirs, files in os.walk(workspace):\n",
    "    for dir_name in dirs:\n",
    "        if dir_name.endswith('.gdb'):\n",
    "            gdb_path = os.path.join(root, dir_name)\n",
    "            gdb_files.append(gdb_path)\n",
    "\n",
    "if gdb_files:\n",
    "    print(f\"Found {len(gdb_files)} geodatabases:\")\n",
    "    for i, gdb in enumerate(gdb_files, 1):\n",
    "        gdb_name = os.path.basename(gdb)\n",
    "        print(f\"  {i}. {gdb_name}\")\n",
    "        \n",
    "        # List feature classes in the geodatabase\n",
    "        arcpy.env.workspace = gdb\n",
    "        feature_classes = arcpy.ListFeatureClasses()\n",
    "        if feature_classes:\n",
    "            print(f\"     Contains {len(feature_classes)} feature classes:\")\n",
    "            for fc in feature_classes[:5]:  # Show first 5\n",
    "                print(f\"       - {fc}\")\n",
    "            if len(feature_classes) > 5:\n",
    "                print(f\"       ... and {len(feature_classes) - 5} more\")\n",
    "        else:\n",
    "            print(\"     No feature classes found\")\n",
    "\n",
    "print(f\"\\nüéØ Data inventory complete - ready to identify our key files!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STEP 3: VERIFY FILE PATHS AND EXAMINE DATA\n",
      "==================================================\n",
      "Verifying file paths:\n",
      "Wells directory: C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\data\\gis_downloads\\wells\n",
      "Soils directory: C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\data\\gis_downloads\\soils\n",
      "\n",
      "Files in wells directory:\n",
      "\n",
      "Files in soils directory:\n",
      "‚ùå No shapefile found in wells directory\n",
      "\n",
      "üîß Troubleshooting complete - identifying the issue...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEP 3: VERIFY FILE PATHS AND EXAMINE DATA\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# First, let's verify the exact file paths\n",
    "wells_dir = os.path.join(data_dir, \"wells\")\n",
    "soils_dir = os.path.join(data_dir, \"soils\")\n",
    "\n",
    "print(\"Verifying file paths:\")\n",
    "print(f\"Wells directory: {wells_dir}\")\n",
    "print(f\"Soils directory: {soils_dir}\")\n",
    "\n",
    "# List actual files to confirm exact names\n",
    "print(f\"\\nFiles in wells directory:\")\n",
    "if os.path.exists(wells_dir):\n",
    "    wells_files = [f for f in os.listdir(wells_dir) if f.endswith('.shp')]\n",
    "    for file in wells_files:\n",
    "        full_path = os.path.join(wells_dir, file)\n",
    "        file_exists = os.path.exists(full_path)\n",
    "        print(f\"  {file} - Exists: {file_exists}\")\n",
    "        if file_exists:\n",
    "            print(f\"    Full path: {full_path}\")\n",
    "else:\n",
    "    print(\"  Directory not found!\")\n",
    "\n",
    "print(f\"\\nFiles in soils directory:\")\n",
    "if os.path.exists(soils_dir):\n",
    "    soils_files = [f for f in os.listdir(soils_dir) if f.endswith('.shp')]\n",
    "    for file in soils_files:\n",
    "        full_path = os.path.join(soils_dir, file)\n",
    "        file_exists = os.path.exists(full_path)\n",
    "        print(f\"  {file} - Exists: {file_exists}\")\n",
    "        if file_exists:\n",
    "            print(f\"    Full path: {full_path}\")\n",
    "else:\n",
    "    print(\"  Directory not found!\")\n",
    "\n",
    "# Try to access the first wells file we find\n",
    "if os.path.exists(wells_dir):\n",
    "    wells_shapefiles = [f for f in os.listdir(wells_dir) if f.endswith('.shp')]\n",
    "    \n",
    "    if wells_shapefiles:\n",
    "        # Try the municipal wells file\n",
    "        municipal_wells_file = None\n",
    "        for file in wells_shapefiles:\n",
    "            if 'municipal' in file.lower():\n",
    "                municipal_wells_file = os.path.join(wells_dir, file)\n",
    "                break\n",
    "        \n",
    "        if municipal_wells_file and os.path.exists(municipal_wells_file):\n",
    "            print(f\"\\nüìä Testing file access: {os.path.basename(municipal_wells_file)}\")\n",
    "            try:\n",
    "                # Test if ArcPy can read the file\n",
    "                test_count = int(arcpy.management.GetCount(municipal_wells_file)[0])\n",
    "                print(f\"‚úÖ Success! Records: {test_count:,}\")\n",
    "                \n",
    "                # Get field info\n",
    "                fields = arcpy.ListFields(municipal_wells_file)\n",
    "                field_names = [f.name for f in fields]\n",
    "                print(f\"‚úÖ Fields ({len(field_names)}): {field_names}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error accessing file: {str(e)}\")\n",
    "        else:\n",
    "            print(\"‚ùå Municipal wells file not found or inaccessible\")\n",
    "    else:\n",
    "        print(\"‚ùå No shapefile found in wells directory\")\n",
    "\n",
    "print(f\"\\nüîß Troubleshooting complete - identifying the issue...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STEP 3: COMPREHENSIVE DATA SEARCH\n",
      "==================================================\n",
      "============================================================\n",
      "COMPREHENSIVE DATA INVENTORY\n",
      "============================================================\n",
      "üîç Searching recursively in: C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\data\\gis_downloads\n",
      "\n",
      "üèõÔ∏è MUNICIPAL WELLS DATA:\n",
      "----------------------------------------\n",
      "  1. CPs_Distance_to_Municipal_Wells.shp\n",
      "     üìÇ wells\\statewide\\CPs_Distance_to_Municipal_Wells.shp\n",
      "     ‚≠ê Selected as primary municipal wells file\n",
      "\n",
      "üè† DOMESTIC WELLS DATA:\n",
      "----------------------------------------\n",
      "  1. CPs_Distance_to_Domestic_Wells.shp\n",
      "     üìÇ wells\\statewide\\CPs_Distance_to_Domestic_Wells.shp\n",
      "     ‚≠ê Selected as primary domestic wells file\n",
      "\n",
      "üå± NRCS SOILS DATA:\n",
      "----------------------------------------\n",
      "  1. HIstate_nrcs_join2.shp\n",
      "     üìÇ soils\\Statewide\\HIstate_nrcs_join2\\HIstate_nrcs_join2.shp\n",
      "     ‚≠ê Selected as primary soils nrcs file\n",
      "\n",
      "üìä HCPT SOILS DATA:\n",
      "----------------------------------------\n",
      "  1. CPs_Soil_Suitability_Rank.shp\n",
      "     üìÇ soils\\Statewide\\CPs_Soil_Suitablity_Rank\\CPs_Soil_Suitability_Rank.shp\n",
      "     ‚≠ê Selected as primary soils hcpt file\n",
      "\n",
      "============================================================\n",
      "TESTING DATA ACCESS\n",
      "============================================================\n",
      "\n",
      "üìä Testing Municipal Wells: CPs_Distance_to_Municipal_Wells.shp\n",
      "   ‚úÖ Records: 82,141\n",
      "   ‚úÖ Fields: 8 total\n",
      "   üîë Key fields: ['Island', 'TMK', 'dist2_MunW']\n",
      "\n",
      "üìä Testing Domestic Wells: CPs_Distance_to_Domestic_Wells.shp\n",
      "   ‚úÖ Records: 82,141\n",
      "   ‚úÖ Fields: 8 total\n",
      "   üîë Key fields: ['Island', 'TMK', 'dist2_DomW']\n",
      "\n",
      "üìä Testing Soils Nrcs: HIstate_nrcs_join2.shp\n",
      "   ‚úÖ Records: 20,743\n",
      "   ‚úÖ Fields: 22 total\n",
      "   üîë Key fields: ['slopegradw', 'ksat_h', 'ksat_l', 'ksat_r']\n",
      "\n",
      "üéØ DATA INVENTORY COMPLETE!\n",
      "Ready to create master attribute table with recursive file detection.\n",
      "\n",
      "üìã KEY FILES IDENTIFIED:\n",
      "   Municipal Wells: C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\data\\gis_downloads\\wells\\statewide\\CPs_Distance_to_Municipal_Wells.shp\n",
      "   Domestic Wells: C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\data\\gis_downloads\\wells\\statewide\\CPs_Distance_to_Domestic_Wells.shp\n",
      "   NRCS Soils: C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\data\\gis_downloads\\soils\\Statewide\\HIstate_nrcs_join2\\HIstate_nrcs_join2.shp\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEP 3: COMPREHENSIVE DATA SEARCH\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def find_data_files(base_directory, file_patterns=None, file_types=None):\n",
    "    \"\"\"\n",
    "    Comprehensive recursive search for data files\n",
    "    Returns organized dictionary of found files by category\n",
    "    \"\"\"\n",
    "    if file_patterns is None:\n",
    "        file_patterns = ['municipal', 'domestic', 'soil', 'nrcs', 'wells', 'distance']\n",
    "    if file_types is None:\n",
    "        file_types = ['.shp', '.gdb']\n",
    "    \n",
    "    found_files = {\n",
    "        'municipal_wells': [],\n",
    "        'domestic_wells': [], \n",
    "        'soils_nrcs': [],\n",
    "        'soils_hcpt': [],\n",
    "        'other': []\n",
    "    }\n",
    "    \n",
    "    if not os.path.exists(base_directory):\n",
    "        return found_files\n",
    "    \n",
    "    print(f\"üîç Searching recursively in: {base_directory}\")\n",
    "    \n",
    "    # Search through all subdirectories\n",
    "    for root, dirs, files in os.walk(base_directory):\n",
    "        # Skip hidden directories\n",
    "        dirs[:] = [d for d in dirs if not d.startswith('.')]\n",
    "        \n",
    "        for file in files:\n",
    "            if any(file.lower().endswith(ext) for ext in file_types):\n",
    "                full_path = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(full_path, base_directory)\n",
    "                \n",
    "                # Categorize files\n",
    "                filename_lower = file.lower()\n",
    "                \n",
    "                if 'municipal' in filename_lower and 'well' in filename_lower:\n",
    "                    found_files['municipal_wells'].append((file, full_path, relative_path))\n",
    "                elif 'domestic' in filename_lower and 'well' in filename_lower:\n",
    "                    found_files['domestic_wells'].append((file, full_path, relative_path))\n",
    "                elif 'nrcs' in filename_lower or 'histate' in filename_lower:\n",
    "                    found_files['soils_nrcs'].append((file, full_path, relative_path))\n",
    "                elif 'soil' in filename_lower and 'suitability' in filename_lower:\n",
    "                    found_files['soils_hcpt'].append((file, full_path, relative_path))\n",
    "                elif any(pattern in filename_lower for pattern in file_patterns):\n",
    "                    found_files['other'].append((file, full_path, relative_path))\n",
    "    \n",
    "    return found_files\n",
    "\n",
    "# Search all data directories\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPREHENSIVE DATA INVENTORY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Search main data directory\n",
    "all_data = find_data_files(data_dir)\n",
    "\n",
    "# Display results by category\n",
    "categories = {\n",
    "    'municipal_wells': 'üèõÔ∏è MUNICIPAL WELLS DATA',\n",
    "    'domestic_wells': 'üè† DOMESTIC WELLS DATA', \n",
    "    'soils_nrcs': 'üå± NRCS SOILS DATA',\n",
    "    'soils_hcpt': 'üìä HCPT SOILS DATA',\n",
    "    'other': 'üìÅ OTHER RELEVANT FILES'\n",
    "}\n",
    "\n",
    "key_files = {}\n",
    "\n",
    "for category, files in all_data.items():\n",
    "    if files:\n",
    "        print(f\"\\n{categories[category]}:\")\n",
    "        print(\"-\" * 40)\n",
    "        for i, (filename, full_path, rel_path) in enumerate(files, 1):\n",
    "            print(f\"  {i}. {filename}\")\n",
    "            print(f\"     üìÇ {rel_path}\")\n",
    "            \n",
    "            # Store first file of each category as key file\n",
    "            if category not in key_files:\n",
    "                key_files[category] = full_path\n",
    "                print(f\"     ‚≠ê Selected as primary {category.replace('_', ' ')} file\")\n",
    "\n",
    "# Test access to key files\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"TESTING DATA ACCESS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for category, file_path in key_files.items():\n",
    "    if category in ['municipal_wells', 'domestic_wells', 'soils_nrcs']:\n",
    "        print(f\"\\nüìä Testing {category.replace('_', ' ').title()}: {os.path.basename(file_path)}\")\n",
    "        \n",
    "        try:\n",
    "            # Test basic access\n",
    "            if arcpy.Exists(file_path):\n",
    "                record_count = int(arcpy.management.GetCount(file_path)[0])\n",
    "                fields = [f.name for f in arcpy.ListFields(file_path)]\n",
    "                \n",
    "                print(f\"   ‚úÖ Records: {record_count:,}\")\n",
    "                print(f\"   ‚úÖ Fields: {len(fields)} total\")\n",
    "                \n",
    "                # Show key fields\n",
    "                key_field_patterns = ['tmk', 'dist', 'ksat', 'slope', 'perc', 'island']\n",
    "                key_fields = [f for f in fields if any(pattern in f.lower() for pattern in key_field_patterns)]\n",
    "                if key_fields:\n",
    "                    print(f\"   üîë Key fields: {key_fields}\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"   ‚ùå File not accessible to ArcPy\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error: {str(e)}\")\n",
    "\n",
    "print(f\"\\nüéØ DATA INVENTORY COMPLETE!\")\n",
    "print(f\"Ready to create master attribute table with recursive file detection.\")\n",
    "\n",
    "# Store key file paths for next steps\n",
    "if 'municipal_wells' in key_files:\n",
    "    municipal_wells_path = key_files['municipal_wells']\n",
    "    print(f\"\\nüìã KEY FILES IDENTIFIED:\")\n",
    "    print(f\"   Municipal Wells: {municipal_wells_path}\")\n",
    "    \n",
    "if 'domestic_wells' in key_files:\n",
    "    domestic_wells_path = key_files['domestic_wells'] \n",
    "    print(f\"   Domestic Wells: {domestic_wells_path}\")\n",
    "    \n",
    "if 'soils_nrcs' in key_files:\n",
    "    soils_path = key_files['soils_nrcs']\n",
    "    print(f\"   NRCS Soils: {soils_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STEP 4: CREATE MASTER ATTRIBUTE TABLE (FIXED)\n",
      "==================================================\n",
      "Creating master attribute table from municipal wells foundation...\n",
      "Foundation created: 82,141 TMK records\n",
      "Location: C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\outputs\\foundation\\TMK_Master_Attributes.shp\n",
      "Current fields: ['FID', 'Shape', 'X', 'Y', 'Island', 'TMK', 'Uid', 'dist2_MunW']\n",
      "\n",
      "Joining domestic wells distance...\n",
      "Domestic wells distance joined\n",
      "New fields added: ['dist2_DomW']\n",
      "\n",
      "Adding Matrix criteria fields...\n",
      "   Added: SLOPE_PCT\n",
      "   Added: SOIL_PERC\n",
      "   Added: SOIL_KSAT\n",
      "   Added: AVAIL_AREA\n",
      "   Added: ZONING\n",
      "   Added: SEPTIC_OK\n",
      "   Added: ATU_OK\n",
      "   Added: SEEPAGE_OK\n",
      "   Added: PROC_LOG\n",
      "Successfully added 9 Matrix fields\n",
      "All fields in table: ['FID', 'Shape', 'X', 'Y', 'Island', 'TMK', 'Uid', 'dist2_MunW', 'dist2_DomW', 'SLOPE_PCT', 'SOIL_PERC', 'SOIL_KSAT', 'AVAIL_AREA', 'ZONING', 'SEPTIC_OK', 'ATU_OK', 'SEEPAGE_OK', 'PROC_LOG']\n",
      "\n",
      "Initializing processing log...\n",
      "   Processed 20,000 records...\n",
      "   Processed 40,000 records...\n",
      "   Processed 60,000 records...\n",
      "   Processed 80,000 records...\n",
      "Processing log initialized for 82,141 records\n",
      "\n",
      "Sample data (first 3 records):\n",
      "  Record 1: TMK: 186006001 | Island: Oahu | dist2_MunW: 3179.83311891 | dist2_DomW: 2913.17095276\n",
      "  Record 2: TMK: 186006001 | Island: Oahu | dist2_MunW: 3202.96147552 | dist2_DomW: 2913.17095276\n",
      "  Record 3: TMK: 186006001 | Island: Oahu | dist2_MunW: 3167.39673719 | dist2_DomW: 2913.17095276\n",
      "\n",
      "MASTER ATTRIBUTE TABLE COMPLETE!\n",
      "   Records: 82,141\n",
      "   Total fields: 18\n",
      "   Ready for soil data processing\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEP 4: CREATE MASTER ATTRIBUTE TABLE (FIXED)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define our verified file paths\n",
    "municipal_wells_path = r\"C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\data\\gis_downloads\\wells\\statewide\\CPs_Distance_to_Municipal_Wells.shp\"\n",
    "domestic_wells_path = r\"C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\data\\gis_downloads\\wells\\statewide\\CPs_Distance_to_Domestic_Wells.shp\" \n",
    "soils_path = r\"C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\data\\gis_downloads\\soils\\Statewide\\HIstate_nrcs_join2\\HIstate_nrcs_join2.shp\"\n",
    "\n",
    "print(\"Creating master attribute table from municipal wells foundation...\")\n",
    "\n",
    "# Create master attribute table\n",
    "master_table_path = os.path.join(foundation_dir, \"TMK_Master_Attributes.shp\")\n",
    "\n",
    "# Copy municipal wells as foundation (82,141 TMK records)\n",
    "arcpy.management.CopyFeatures(municipal_wells_path, master_table_path)\n",
    "\n",
    "record_count = int(arcpy.management.GetCount(master_table_path)[0])\n",
    "print(f\"Foundation created: {record_count:,} TMK records\")\n",
    "print(f\"Location: {master_table_path}\")\n",
    "\n",
    "# Examine current fields\n",
    "current_fields = [f.name for f in arcpy.ListFields(master_table_path)]\n",
    "print(f\"Current fields: {current_fields}\")\n",
    "\n",
    "# Add domestic wells distance\n",
    "print(f\"\\nJoining domestic wells distance...\")\n",
    "\n",
    "# Join domestic wells data\n",
    "arcpy.management.JoinField(\n",
    "    in_data=master_table_path,\n",
    "    in_field=\"TMK\",  \n",
    "    join_table=domestic_wells_path,\n",
    "    join_field=\"TMK\",\n",
    "    fields=[\"dist2_DomW\"]\n",
    ")\n",
    "\n",
    "print(f\"Domestic wells distance joined\")\n",
    "\n",
    "# Verify the join worked\n",
    "updated_fields = [f.name for f in arcpy.ListFields(master_table_path)]\n",
    "new_fields = [f for f in updated_fields if f not in current_fields]\n",
    "print(f\"New fields added: {new_fields}\")\n",
    "\n",
    "# Add Matrix fields with shorter names (shapefiles have 10-char limit)\n",
    "print(f\"\\nAdding Matrix criteria fields...\")\n",
    "\n",
    "matrix_fields = [\n",
    "    (\"SLOPE_PCT\", \"DOUBLE\", \"Site slope percentage\"),\n",
    "    (\"SOIL_PERC\", \"DOUBLE\", \"Soil percolation rate\"),  \n",
    "    (\"SOIL_KSAT\", \"DOUBLE\", \"Soil hydraulic conductivity\"),\n",
    "    (\"AVAIL_AREA\", \"DOUBLE\", \"Available land area\"),\n",
    "    (\"ZONING\", \"TEXT\", \"Zoning classification\"),\n",
    "    (\"SEPTIC_OK\", \"SHORT\", \"Conventional septic suitable\"),\n",
    "    (\"ATU_OK\", \"SHORT\", \"ATU system suitable\"), \n",
    "    (\"SEEPAGE_OK\", \"SHORT\", \"Seepage pit suitable\"),\n",
    "    (\"PROC_LOG\", \"TEXT\", \"Processing steps log\")  # Shortened name\n",
    "]\n",
    "\n",
    "successful_fields = []\n",
    "for field_name, field_type, field_alias in matrix_fields:\n",
    "    try:\n",
    "        if field_type == \"TEXT\":\n",
    "            arcpy.management.AddField(master_table_path, field_name, field_type, \n",
    "                                   field_alias=field_alias, field_length=50)\n",
    "        else:\n",
    "            arcpy.management.AddField(master_table_path, field_name, field_type, \n",
    "                                   field_alias=field_alias)\n",
    "        print(f\"   Added: {field_name}\")\n",
    "        successful_fields.append(field_name)\n",
    "    except Exception as e:\n",
    "        print(f\"   Warning {field_name}: {str(e)}\")\n",
    "\n",
    "print(f\"Successfully added {len(successful_fields)} Matrix fields\")\n",
    "\n",
    "# Check what fields actually exist now\n",
    "final_fields = [f.name for f in arcpy.ListFields(master_table_path)]\n",
    "print(f\"All fields in table: {final_fields}\")\n",
    "\n",
    "# Initialize processing log only if PROC_LOG field exists\n",
    "if \"PROC_LOG\" in final_fields:\n",
    "    print(f\"\\nInitializing processing log...\")\n",
    "    timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M')\n",
    "    \n",
    "    with arcpy.da.UpdateCursor(master_table_path, [\"PROC_LOG\"]) as cursor:\n",
    "        count = 0\n",
    "        for row in cursor:\n",
    "            row[0] = f\"Created: {timestamp}\"\n",
    "            cursor.updateRow(row)\n",
    "            count += 1\n",
    "            if count % 20000 == 0:\n",
    "                print(f\"   Processed {count:,} records...\")\n",
    "    \n",
    "    print(f\"Processing log initialized for {count:,} records\")\n",
    "else:\n",
    "    print(f\"PROC_LOG field not available - skipping log initialization\")\n",
    "\n",
    "# Sample data check\n",
    "print(f\"\\nSample data (first 3 records):\")\n",
    "sample_fields = [\"TMK\", \"Island\", \"dist2_MunW\", \"dist2_DomW\"]\n",
    "sample_fields = [f for f in sample_fields if f in final_fields]\n",
    "\n",
    "with arcpy.da.SearchCursor(master_table_path, sample_fields) as cursor:\n",
    "    for i, row in enumerate(cursor):\n",
    "        if i < 3:\n",
    "            record_data = []\n",
    "            for j, value in enumerate(row):\n",
    "                record_data.append(f\"{sample_fields[j]}: {value}\")\n",
    "            print(f\"  Record {i+1}: {' | '.join(record_data)}\")\n",
    "        if i >= 2:\n",
    "            break\n",
    "\n",
    "print(f\"\\nMASTER ATTRIBUTE TABLE COMPLETE!\")\n",
    "print(f\"   Records: {record_count:,}\")\n",
    "print(f\"   Total fields: {len(final_fields)}\")\n",
    "print(f\"   Ready for soil data processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SOIL CLASSIFICATION METHODOLOGY REVIEW AND DOCUMENTATION\n",
      "======================================================================\n",
      "EXAMINING ACTUAL NRCS SOIL DATA:\n",
      "----------------------------------------\n",
      "Total fields in NRCS data: 22\n",
      "\n",
      "Hydraulic conductivity fields found:\n",
      "  ksat_h: String, Length: 254\n",
      "  ksat_l: String, Length: 254\n",
      "  ksat_r: String, Length: 254\n",
      "\n",
      "Sample KSAT values (first 20 records):\n",
      "  Record 1: ksat_h: 16.46 | ksat_l: 3.5283333333333338 | ksat_r: 9.866666666666667\n",
      "  Record 2: ksat_h: 15.88 | ksat_l: 1.5533333333333335 | ksat_r: 8.666666666666666\n",
      "  Record 3: ksat_h: NoData | ksat_l: NoData | ksat_r: NoData\n",
      "  Record 4: ksat_h: 77.57 | ksat_l: 9.17 | ksat_r: 39.5\n",
      "  Record 5: ksat_h: 4.946666666666667 | ksat_l: 1.3466666666666667 | ksat_r: 3.1466666666666665\n",
      "  Record 6: ksat_h: 18.5575 | ksat_l: 2.115 | ksat_r: 9.875\n",
      "  Record 7: ksat_h: 32.1 | ksat_l: 3.227619047619048 | ksat_r: 17.576190476190476\n",
      "  Record 8: ksat_h: 77.57 | ksat_l: 9.17 | ksat_r: 39.5\n",
      "  Record 9: ksat_h: NoData | ksat_l: NoData | ksat_r: NoData\n",
      "  Record 10: ksat_h: 141.0 | ksat_l: 42.0 | ksat_r: 92.0\n",
      "  Record 11: ksat_h: 18.67 | ksat_l: 2.115 | ksat_r: 9.875\n",
      "  Record 12: ksat_h: 18.67 | ksat_l: 2.115 | ksat_r: 9.875\n",
      "  Record 13: ksat_h: 14.0 | ksat_l: 4.23 | ksat_r: 9.0\n",
      "  Record 14: ksat_h: 70.44466666666666 | ksat_l: 10.158 | ksat_r: 40.0\n",
      "  Record 15: ksat_h: 4.946666666666667 | ksat_l: 1.3466666666666667 | ksat_r: 3.1466666666666665\n",
      "  Record 16: ksat_h: 8.138 | ksat_l: 2.5380000000000003 | ksat_r: 5.1\n",
      "  Record 17: ksat_h: 16.46 | ksat_l: 3.5283333333333338 | ksat_r: 9.866666666666667\n",
      "  Record 18: ksat_h: 77.57 | ksat_l: 9.17 | ksat_r: 39.5\n",
      "  Record 19: ksat_h: 10.055000000000001 | ksat_l: 1.055 | ksat_r: 5.516666666666667\n",
      "  Record 20: ksat_h: 77.57 | ksat_l: 9.17 | ksat_r: 39.5\n",
      "\n",
      "Other potentially relevant fields:\n",
      "  slopegradw: String\n",
      "\n",
      "======================================================================\n",
      "METHODOLOGY ISSUES TO ADDRESS:\n",
      "======================================================================\n",
      "1. KSAT to Percolation Rate Conversion:\n",
      "   - KSAT units need verification (micrometers/sec vs other)\n",
      "   - Conversion formula needs literature backing\n",
      "   - HAR 11-62 requires specific percolation rate ranges\n",
      "   - Current conversion is approximate, not scientifically rigorous\n",
      "Ôªø\n",
      "2. Missing Critical Data:\n",
      "   - No direct percolation test data\n",
      "   - Drainage class information unclear\n",
      "   - Depth to restrictive layer not identified\n",
      "Ôªø\n",
      "3. Regulatory Compliance:\n",
      "   - HAR 11-62 requires site-specific percolation testing\n",
      "   - Soil survey data is reconnaissance level, not design level\n",
      "   - Matrix analysis should note limitations\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SOIL CLASSIFICATION METHODOLOGY REVIEW AND DOCUMENTATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# First, let's examine what we actually have in the NRCS data\n",
    "soils_path = r\"C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\data\\gis_downloads\\soils\\Statewide\\HIstate_nrcs_join2\\HIstate_nrcs_join2.shp\"\n",
    "\n",
    "print(\"EXAMINING ACTUAL NRCS SOIL DATA:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Get all fields and their properties\n",
    "soil_fields = arcpy.ListFields(soils_path)\n",
    "print(f\"Total fields in NRCS data: {len(soil_fields)}\")\n",
    "\n",
    "# Focus on hydraulic conductivity fields\n",
    "ksat_fields = [f for f in soil_fields if 'ksat' in f.name.lower()]\n",
    "print(f\"\\nHydraulic conductivity fields found:\")\n",
    "for field in ksat_fields:\n",
    "    print(f\"  {field.name}: {field.type}, Length: {field.length}\")\n",
    "\n",
    "# Sample actual KSAT values to understand the data distribution\n",
    "print(f\"\\nSample KSAT values (first 20 records):\")\n",
    "ksat_field_names = [f.name for f in ksat_fields]\n",
    "if ksat_field_names:\n",
    "    with arcpy.da.SearchCursor(soils_path, ksat_field_names) as cursor:\n",
    "        for i, row in enumerate(cursor):\n",
    "            if i < 20:\n",
    "                values = [f\"{ksat_field_names[j]}: {row[j]}\" for j in range(len(row))]\n",
    "                print(f\"  Record {i+1}: {' | '.join(values)}\")\n",
    "            if i >= 19:\n",
    "                break\n",
    "else:\n",
    "    print(\"  No KSAT fields found\")\n",
    "\n",
    "# Check for other relevant fields\n",
    "relevant_patterns = ['slope', 'drain', 'perc', 'hydro', 'restrict']\n",
    "relevant_fields = []\n",
    "for field in soil_fields:\n",
    "    if any(pattern in field.name.lower() for pattern in relevant_patterns):\n",
    "        relevant_fields.append(field)\n",
    "\n",
    "print(f\"\\nOther potentially relevant fields:\")\n",
    "for field in relevant_fields[:10]:  # Show first 10\n",
    "    print(f\"  {field.name}: {field.type}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(\"METHODOLOGY ISSUES TO ADDRESS:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"1. KSAT to Percolation Rate Conversion:\")\n",
    "print(\"   - KSAT units need verification (micrometers/sec vs other)\")\n",
    "print(\"   - Conversion formula needs literature backing\")\n",
    "print(\"   - HAR 11-62 requires specific percolation rate ranges\")\n",
    "print(\"   - Current conversion is approximate, not scientifically rigorous\")\n",
    "print(\"\")\n",
    "print(\"2. Missing Critical Data:\")\n",
    "print(\"   - No direct percolation test data\")\n",
    "print(\"   - Drainage class information unclear\") \n",
    "print(\"   - Depth to restrictive layer not identified\")\n",
    "print(\"\")\n",
    "print(\"3. Regulatory Compliance:\")\n",
    "print(\"   - HAR 11-62 requires site-specific percolation testing\")\n",
    "print(\"   - Soil survey data is reconnaissance level, not design level\")\n",
    "print(\"   - Matrix analysis should note limitations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STEP 6: UPDATE EXISTING TECHNOLOGY MATRIX\n",
      "==================================================\n",
      "Found existing Matrix file: C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\Matrix\\Technology,Treatment,Disposal,Treat.xls\n",
      "Matrix loaded successfully: 24 technologies, 42 criteria\n",
      "\n",
      "Current Matrix structure:\n",
      "Columns: ['Technology', 'Treatment', 'Disposal', 'Treatment & Disposal', 'DOH Approved', 'DOH Conditional Approval', 'DOH Experimental/Limited', 'Depth <3 ft', 'Depth 3-6 ft', 'Depth >6 ft', 'Lot <10k sf', 'Lot 10k-21k sf', 'Lot >21k sf', 'Slope <8%', 'Slope 8-12%', 'Slope >12%', 'SMA Within 50 ft', 'SMA Beyond 50 ft', 'SMA No', 'Soil <1 min/inch', 'Soil 1-10 min/inch', 'Soil 10-60 min/inch', 'Soil >60 min/inch', 'Soil N/A', 'N Removal High', 'N Removal Medium', 'N Removal Low/None', 'N Removal N/A', 'Stream <50 ft Yes', 'Stream <50 ft No', 'Stream <50 ft Conditional', 'Well 100 ft', 'Well 1000 ft', 'Well Variable', 'Well No Setback', 'Climate Xeric', 'Climate Mesic', 'Climate Hydric/Intense Rainfall', 'Flood Zone Yes', 'Flood Zone No', 'Service Contract Required', 'Case-by-Case Review']\n",
      "\n",
      "Technologies in Matrix:\n",
      "   1. Standard Septic Tank\n",
      "   2. Aerobic Treatment Unit (NSF 40)\n",
      "   3. Advanced Nitrogen-Reducing ATU (NSF 245)\n",
      "   4. Recirculating Media Filter\n",
      "   5. Packed Bed Media Filter\n",
      "   6. Nitrogen-Reducing Biofilter\n",
      "   7. Cluster System (Sm. Scale Shared Treatment)\n",
      "   8. Decentralized Wastewater Plant\n",
      "   9. Constructed Wetland\n",
      "  10. Composting or Incinerating Toilet\n",
      "  11. Standard Absorption Trenches\n",
      "  12. Deep Absorption Trenches\n",
      "  13. Absorption Beds\n",
      "  14. Traffic Rated (Compact) Drainfield\n",
      "  15. Seepage Pits\n",
      "  16. Subsurface Drip Irrigation\n",
      "  17. Injection Wells\n",
      "  18. Elevated Mound System\n",
      "  19. Soil Replacement System\n",
      "  20. Evapotranspiration System\n",
      "  21. Gravelless Systems\n",
      "  22. Bioreactor Garden\n",
      "  23. Incinerator Central & Individual\n",
      "  24. Graywater Diversion\n",
      "\n",
      "Existing soil-related columns: ['Soil <1 min/inch', 'Soil 1-10 min/inch', 'Soil 10-60 min/inch', 'Soil >60 min/inch', 'Soil N/A']\n",
      "Created documentation folder: C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\Matrix\\documentation\n",
      "\n",
      "Updating Matrix with screening-level approach...\n",
      "  Added disclaimer column: SCREENING_TOOL_ONLY\n",
      "  Added disclaimer column: SITE_TESTING_REQD\n",
      "  Added disclaimer column: DOH_PERMIT_REQD\n",
      "Updating existing soil columns to screening classifications...\n",
      "  Updating column: Soil <1 min/inch\n",
      "  Updating column: Soil 1-10 min/inch\n",
      "  Updating column: Soil 10-60 min/inch\n",
      "  Updating column: Soil >60 min/inch\n",
      "  Updating column: Soil N/A\n",
      "  Added screening column: High_Infiltration_Potential\n",
      "  Added screening column: Moderate_Infiltration_Potential\n",
      "  Added screening column: Low_Infiltration_Potential\n",
      "  Added screening column: Data_Insufficient\n",
      "Updated Matrix saved: C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\Matrix\\Technology_Matrix_Screening_Updated.xlsx\n",
      "Change log created: C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\Matrix\\documentation\\Matrix_Update_Change_Log.txt\n",
      "\n",
      "==================================================\n",
      "MATRIX UPDATE COMPLETE\n",
      "==================================================\n",
      "Updated Matrix emphasizes screening purpose with:\n",
      "- Disclaimer columns for all technologies\n",
      "- Screening-level soil classifications\n",
      "- Conservative default requirements\n",
      "- Extensive documentation of limitations\n",
      "- Professional engineering requirements\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEP 6: UPDATE EXISTING TECHNOLOGY MATRIX\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Path to existing Matrix file\n",
    "matrix_path = r\"C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\Matrix\\Technology,Treatment,Disposal,Treat.xls\"\n",
    "\n",
    "print(f\"Found existing Matrix file: {matrix_path}\")\n",
    "\n",
    "# Read the existing Matrix\n",
    "try:\n",
    "    # Try reading the Excel file\n",
    "    df_matrix = pd.read_excel(matrix_path)\n",
    "    print(f\"Matrix loaded successfully: {len(df_matrix)} technologies, {len(df_matrix.columns)} criteria\")\n",
    "    \n",
    "    # Display basic structure\n",
    "    print(f\"\\nCurrent Matrix structure:\")\n",
    "    print(f\"Columns: {list(df_matrix.columns)}\")\n",
    "    \n",
    "    if 'Technology' in df_matrix.columns:\n",
    "        print(f\"\\nTechnologies in Matrix:\")\n",
    "        for i, tech in enumerate(df_matrix['Technology'], 1):\n",
    "            print(f\"  {i:2d}. {tech}\")\n",
    "    \n",
    "    # Look for existing soil-related columns\n",
    "    soil_columns = [col for col in df_matrix.columns if any(keyword in col.lower() for keyword in ['soil', 'perc', 'infiltra', 'ksat'])]\n",
    "    print(f\"\\nExisting soil-related columns: {soil_columns}\")\n",
    "    \n",
    "    # Check current matrix documentation folder\n",
    "    matrix_docs_path = r\"C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\Matrix\\documentation\"\n",
    "    if not os.path.exists(matrix_docs_path):\n",
    "        os.makedirs(matrix_docs_path)\n",
    "        print(f\"Created documentation folder: {matrix_docs_path}\")\n",
    "    \n",
    "    # Create updated Matrix with screening disclaimers\n",
    "    print(f\"\\nUpdating Matrix with screening-level approach...\")\n",
    "    \n",
    "    # Add disclaimer columns if they don't exist\n",
    "    disclaimer_columns = {\n",
    "        'SCREENING_TOOL_ONLY': 'This Matrix provides preliminary screening only - professional engineering required',\n",
    "        'SITE_TESTING_REQD': 'Site-specific testing and evaluation required for all installations',\n",
    "        'DOH_PERMIT_REQD': 'DOH Individual Wastewater System permit required'\n",
    "    }\n",
    "    \n",
    "    df_updated = df_matrix.copy()\n",
    "    \n",
    "    # Add disclaimer columns\n",
    "    for col_name, col_value in disclaimer_columns.items():\n",
    "        if col_name not in df_updated.columns:\n",
    "            df_updated[col_name] = col_value\n",
    "            print(f\"  Added disclaimer column: {col_name}\")\n",
    "    \n",
    "    # Update soil-related columns with screening classifications\n",
    "    # Replace any existing soil columns with new screening approach\n",
    "    if soil_columns:\n",
    "        print(f\"Updating existing soil columns to screening classifications...\")\n",
    "        for col in soil_columns:\n",
    "            print(f\"  Updating column: {col}\")\n",
    "        \n",
    "        # For now, create new screening-based columns\n",
    "        soil_screening_columns = {\n",
    "            'High_Infiltration_Potential': 'KSAT >25 Œºm/s - Screening indicates favorable conditions',\n",
    "            'Moderate_Infiltration_Potential': 'KSAT 5-25 Œºm/s - Site testing critical',\n",
    "            'Low_Infiltration_Potential': 'KSAT <5 Œºm/s - Consider alternatives', \n",
    "            'Data_Insufficient': 'No soil survey data - Site investigation required'\n",
    "        }\n",
    "        \n",
    "        for col_name, description in soil_screening_columns.items():\n",
    "            df_updated[col_name] = 0  # Default to not suitable, requires case-by-case review\n",
    "            print(f\"  Added screening column: {col_name}\")\n",
    "    \n",
    "    # Save updated Matrix\n",
    "    updated_matrix_path = os.path.join(os.path.dirname(matrix_path), \"Technology_Matrix_Screening_Updated.xlsx\")\n",
    "    df_updated.to_excel(updated_matrix_path, index=False)\n",
    "    print(f\"Updated Matrix saved: {updated_matrix_path}\")\n",
    "    \n",
    "    # Create change log\n",
    "    change_log = f\"\"\"\n",
    "TECHNOLOGY MATRIX UPDATE LOG\n",
    "===========================\n",
    "Date: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Original File: {matrix_path}\n",
    "Updated File: {updated_matrix_path}\n",
    "\n",
    "CHANGES MADE:\n",
    "============\n",
    "\n",
    "1. ADDED DISCLAIMER COLUMNS:\n",
    "   - SCREENING_TOOL_ONLY: Emphasizes preliminary screening purpose\n",
    "   - SITE_TESTING_REQD: Requires site-specific evaluation\n",
    "   - DOH_PERMIT_REQD: Mandates regulatory compliance\n",
    "\n",
    "2. UPDATED SOIL CLASSIFICATION APPROACH:\n",
    "   - Replaced site-specific soil requirements with screening-level classifications\n",
    "   - Added four screening categories based on NRCS KSAT data\n",
    "   - Conservative approach requiring professional follow-up\n",
    "\n",
    "3. SCREENING-LEVEL CLASSIFICATIONS ADDED:\n",
    "   - High_Infiltration_Potential: KSAT >25 Œºm/s from soil survey data\n",
    "   - Moderate_Infiltration_Potential: KSAT 5-25 Œºm/s from soil survey data  \n",
    "   - Low_Infiltration_Potential: KSAT <5 Œºm/s from soil survey data\n",
    "   - Data_Insufficient: No reliable soil survey data available\n",
    "\n",
    "4. CONSERVATIVE DEFAULT VALUES:\n",
    "   - All screening columns default to 0 (not suitable)\n",
    "   - Requires case-by-case professional evaluation\n",
    "   - Emphasizes need for site-specific testing\n",
    "\n",
    "RATIONALE:\n",
    "=========\n",
    "- Matrix now serves as screening tool for planning purposes only\n",
    "- Removes inappropriate site-specific design elements\n",
    "- Adds extensive disclaimers about limitations\n",
    "- Aligns with professional engineering requirements\n",
    "- Meets academic standards for uncertainty acknowledgment\n",
    "\n",
    "NEXT STEPS:\n",
    "==========\n",
    "1. Review updated Matrix with subject matter experts\n",
    "2. Populate screening columns with appropriate technology compatibility\n",
    "3. Test screening approach with pilot parcels\n",
    "4. Document validation and verification procedures\n",
    "5. Integrate with master attribute table workflow\n",
    "\"\"\"\n",
    "    \n",
    "    change_log_path = os.path.join(matrix_docs_path, \"Matrix_Update_Change_Log.txt\")\n",
    "    with open(change_log_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(change_log)\n",
    "    \n",
    "    print(f\"Change log created: {change_log_path}\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 50)\n",
    "    print(\"MATRIX UPDATE COMPLETE\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Updated Matrix emphasizes screening purpose with:\")\n",
    "    print(\"- Disclaimer columns for all technologies\")\n",
    "    print(\"- Screening-level soil classifications\")  \n",
    "    print(\"- Conservative default requirements\")\n",
    "    print(\"- Extensive documentation of limitations\")\n",
    "    print(\"- Professional engineering requirements\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error reading Matrix file: {str(e)}\")\n",
    "    print(\"Please verify the file exists and is accessible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STEP 7: CLEAN UP NUMERIC FIELDS - REMOVE DECIMALS\n",
      "==================================================\n",
      "Cleaning numeric fields in: TMK_Master_Attributes.shp\n",
      "  Will clean: X (Double)\n",
      "  Will clean: Y (Double)\n",
      "  Will clean: dist2_MunW (Double)\n",
      "  Will clean: dist2_DomW (Double)\n",
      "  Will clean: SLOPE_PCT (Double)\n",
      "  Will clean: SOIL_PERC (Double)\n",
      "  Will clean: SOIL_KSAT (Double)\n",
      "  Will clean: AVAIL_AREA (Double)\n",
      "\n",
      "Found 8 numeric fields to clean\n",
      "Rounding to whole numbers...\n",
      "    Processed 10,000 of 82,141 records...\n",
      "    Processed 20,000 of 82,141 records...\n",
      "    Processed 30,000 of 82,141 records...\n",
      "    Processed 40,000 of 82,141 records...\n",
      "    Processed 50,000 of 82,141 records...\n",
      "    Processed 60,000 of 82,141 records...\n",
      "    Processed 70,000 of 82,141 records...\n",
      "    Processed 80,000 of 82,141 records...\n",
      "Completed cleaning 82,141 records\n",
      "\n",
      "Sample of cleaned data (first 3 records):\n",
      "  Record 1: TMK: 186006001 | dist2_MunW: 3180.0 | dist2_DomW: 2913.0 | X: -158.0 | Y: 21.0\n",
      "  Record 2: TMK: 186006001 | dist2_MunW: 3203.0 | dist2_DomW: 2913.0 | X: -158.0 | Y: 21.0\n",
      "  Record 3: TMK: 186006001 | dist2_MunW: 3167.0 | dist2_DomW: 2913.0 | X: -158.0 | Y: 21.0\n",
      "\n",
      "==================================================\n",
      "NUMERIC FIELD CLEANUP COMPLETE\n",
      "==================================================\n",
      "All distance and numeric fields now show as whole numbers\n",
      "Data is cleaner and more professional for 82,000+ records\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEP 7: CLEAN UP NUMERIC FIELDS - REMOVE DECIMALS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Path to master attribute table\n",
    "master_table_path = os.path.join(foundation_dir, \"TMK_Master_Attributes.shp\")\n",
    "\n",
    "print(f\"Cleaning numeric fields in: {os.path.basename(master_table_path)}\")\n",
    "\n",
    "# Get all numeric fields that need cleaning\n",
    "fields = arcpy.ListFields(master_table_path)\n",
    "numeric_fields_to_clean = []\n",
    "\n",
    "for field in fields:\n",
    "    if field.type in ['Double', 'Single'] and field.name not in ['Shape_Length', 'Shape_Area']:\n",
    "        numeric_fields_to_clean.append(field.name)\n",
    "        print(f\"  Will clean: {field.name} ({field.type})\")\n",
    "\n",
    "print(f\"\\nFound {len(numeric_fields_to_clean)} numeric fields to clean\")\n",
    "\n",
    "if numeric_fields_to_clean:\n",
    "    print(\"Rounding to whole numbers...\")\n",
    "    \n",
    "    # Process in chunks for better performance\n",
    "    chunk_size = 10000\n",
    "    total_records = int(arcpy.management.GetCount(master_table_path)[0])\n",
    "    processed = 0\n",
    "    \n",
    "    with arcpy.da.UpdateCursor(master_table_path, numeric_fields_to_clean) as cursor:\n",
    "        for row in cursor:\n",
    "            # Round each numeric value to whole number\n",
    "            cleaned_row = []\n",
    "            for value in row:\n",
    "                if value is not None:\n",
    "                    cleaned_row.append(round(float(value)))\n",
    "                else:\n",
    "                    cleaned_row.append(value)\n",
    "            \n",
    "            cursor.updateRow(cleaned_row)\n",
    "            processed += 1\n",
    "            \n",
    "            # Progress indicator\n",
    "            if processed % chunk_size == 0:\n",
    "                print(f\"    Processed {processed:,} of {total_records:,} records...\")\n",
    "    \n",
    "    print(f\"Completed cleaning {processed:,} records\")\n",
    "    \n",
    "    # Verify the changes\n",
    "    print(\"\\nSample of cleaned data (first 3 records):\")\n",
    "    sample_fields = ['TMK', 'dist2_MunW', 'dist2_DomW'] + numeric_fields_to_clean[:2]\n",
    "    sample_fields = [f for f in sample_fields if f in [field.name for field in arcpy.ListFields(master_table_path)]]\n",
    "    \n",
    "    with arcpy.da.SearchCursor(master_table_path, sample_fields) as cursor:\n",
    "        for i, row in enumerate(cursor):\n",
    "            if i < 3:\n",
    "                record_data = []\n",
    "                for j, value in enumerate(row):\n",
    "                    record_data.append(f\"{sample_fields[j]}: {value}\")\n",
    "                print(f\"  Record {i+1}: {' | '.join(record_data)}\")\n",
    "            if i >= 2:\n",
    "                break\n",
    "\n",
    "else:\n",
    "    print(\"No numeric fields found to clean\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 50)\n",
    "print(\"NUMERIC FIELD CLEANUP COMPLETE\")\n",
    "print(\"=\" * 50)\n",
    "print(\"All distance and numeric fields now show as whole numbers\")\n",
    "print(\"Data is cleaner and more professional for 82,000+ records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CLEANUP BAD OUTPUTS AND EXAMINE DATA STRUCTURES\n",
      "======================================================================\n",
      "Cleaning up problematic output files...\n",
      "  Deleted: Cesspool_Parcel_Polygons.shp\n",
      "  Deleted: Maui_Cesspool_Polygons.shp\n",
      "  Deleted: temp_parcels.shp\n",
      "  Deleted: cesspool_tmks.dbf\n",
      "  Deleted: Cesspool_Parcels_Statewide.shp\n",
      "  Deleted: Maui_Cesspool_Parcels.shp\n",
      "‚úì Cleanup complete\n",
      "\n",
      "EXAMINING DATA STRUCTURE COMPATIBILITY\n",
      "==================================================\n",
      "CESSPOOL POINTS DATA (TMK_Master_Attributes.shp):\n",
      "  Records: 82,141\n",
      "  TMK field details:\n",
      "    Name: TMK\n",
      "    Type: Integer\n",
      "    Length: 10\n",
      "  Sample TMK values:\n",
      "    186006001 (Oahu)\n",
      "    186006001 (Oahu)\n",
      "    186006001 (Oahu)\n",
      "    186006001 (Oahu)\n",
      "    186006001 (Oahu)\n",
      "    186006001 (Oahu)\n",
      "    186006001 (Oahu)\n",
      "    186006001 (Oahu)\n",
      "\n",
      "TMK PARCEL POLYGONS DATA:\n",
      "  Found at: C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\data\\tmk_state.shp\n",
      "  Error accessing parcel data: ERROR 000229: Cannot open C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\data\\tmk_state.shp\n",
      "Failed to execute (GetCount).\n",
      "\n",
      "\n",
      "TMK FORMAT COMPARISON:\n",
      "==============================\n"
     ]
    },
    {
     "ename": "<class 'NameError'>",
     "evalue": "name 'tmk_related_fields' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 147\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    146\u001b[0m parcel_tmks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tmk_related_fields:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m arcpy\u001b[38;5;241m.\u001b[39mda\u001b[38;5;241m.\u001b[39mSearchCursor(tmk_parcels_path, [primary_tmk_field]) \u001b[38;5;28;01mas\u001b[39;00m cursor:\n\u001b[0;32m    149\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(cursor):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tmk_related_fields' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CLEANUP BAD OUTPUTS AND EXAMINE DATA STRUCTURES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Clean up the problematic files we created\n",
    "cleanup_files = [\n",
    "    os.path.join(foundation_dir, \"Cesspool_Parcel_Polygons.shp\"),\n",
    "    os.path.join(foundation_dir, \"Maui_Cesspool_Polygons.shp\"),\n",
    "    os.path.join(foundation_dir, \"temp_parcels.shp\"),\n",
    "    os.path.join(foundation_dir, \"cesspool_tmks.dbf\"),\n",
    "    os.path.join(foundation_dir, \"Cesspool_Parcels_Statewide.shp\"),\n",
    "    os.path.join(foundation_dir, \"Maui_Cesspool_Points.shp\"),\n",
    "    os.path.join(foundation_dir, \"Maui_Cesspool_Parcels.shp\")\n",
    "]\n",
    "\n",
    "print(\"Cleaning up problematic output files...\")\n",
    "for file_path in cleanup_files:\n",
    "    try:\n",
    "        if arcpy.Exists(file_path):\n",
    "            arcpy.management.Delete(file_path)\n",
    "            print(f\"  Deleted: {os.path.basename(file_path)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Could not delete {os.path.basename(file_path)}: {str(e)}\")\n",
    "\n",
    "# Also remove any related files (.dbf, .shx, .prj, etc.)\n",
    "for cleanup_file in cleanup_files:\n",
    "    if cleanup_file.endswith('.shp'):\n",
    "        base_name = cleanup_file[:-4]\n",
    "        for ext in ['.dbf', '.shx', '.prj', '.cpg', '.sbn', '.sbx', '.xml']:\n",
    "            try:\n",
    "                related_file = base_name + ext\n",
    "                if os.path.exists(related_file):\n",
    "                    os.remove(related_file)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "print(\"‚úì Cleanup complete\")\n",
    "\n",
    "print(f\"\\nEXAMINING DATA STRUCTURE COMPATIBILITY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Examine cesspool points (master attribute table)\n",
    "master_table_path = os.path.join(foundation_dir, \"TMK_Master_Attributes.shp\")\n",
    "print(\"CESSPOOL POINTS DATA (TMK_Master_Attributes.shp):\")\n",
    "\n",
    "if arcpy.Exists(master_table_path):\n",
    "    cesspool_count = int(arcpy.management.GetCount(master_table_path)[0])\n",
    "    cesspool_fields = [(f.name, f.type, f.length) for f in arcpy.ListFields(master_table_path)]\n",
    "    \n",
    "    print(f\"  Records: {cesspool_count:,}\")\n",
    "    print(f\"  TMK field details:\")\n",
    "    \n",
    "    tmk_field_info = next((f for f in cesspool_fields if f[0] == 'TMK'), None)\n",
    "    if tmk_field_info:\n",
    "        print(f\"    Name: {tmk_field_info[0]}\")\n",
    "        print(f\"    Type: {tmk_field_info[1]}\")\n",
    "        print(f\"    Length: {tmk_field_info[2]}\")\n",
    "    \n",
    "    # Sample TMK values\n",
    "    print(\"  Sample TMK values:\")\n",
    "    with arcpy.da.SearchCursor(master_table_path, ['TMK', 'Island']) as cursor:\n",
    "        for i, row in enumerate(cursor):\n",
    "            if i < 8:\n",
    "                print(f\"    {row[0]} ({row[1]})\")\n",
    "            if i >= 7:\n",
    "                break\n",
    "\n",
    "# Examine parcel polygons\n",
    "tmk_parcels_path = None\n",
    "possible_paths = [\n",
    "    os.path.join(workspace, \"data\", \"tmk_state.shp\"),\n",
    "    os.path.join(workspace, \"data\", \"tmk_state.shp\", \"tmk_state.shp\")\n",
    "]\n",
    "\n",
    "for path in possible_paths:\n",
    "    if arcpy.Exists(path):\n",
    "        tmk_parcels_path = path\n",
    "        break\n",
    "\n",
    "print(f\"\\nTMK PARCEL POLYGONS DATA:\")\n",
    "\n",
    "if tmk_parcels_path:\n",
    "    print(f\"  Found at: {tmk_parcels_path}\")\n",
    "    \n",
    "    try:\n",
    "        parcel_count = int(arcpy.management.GetCount(tmk_parcels_path)[0])\n",
    "        parcel_desc = arcpy.Describe(tmk_parcels_path)\n",
    "        parcel_fields = [(f.name, f.type, f.length) for f in arcpy.ListFields(tmk_parcels_path)]\n",
    "        \n",
    "        print(f\"  Records: {parcel_count:,}\")\n",
    "        print(f\"  Geometry: {parcel_desc.shapeType}\")\n",
    "        \n",
    "        # Find TMK-related fields\n",
    "        tmk_related_fields = [f for f in parcel_fields if 'tmk' in f[0].lower()]\n",
    "        print(f\"  TMK-related fields:\")\n",
    "        for field_info in tmk_related_fields:\n",
    "            print(f\"    {field_info[0]} ({field_info[1]}, length: {field_info[2]})\")\n",
    "        \n",
    "        # Sample values from the first TMK field\n",
    "        if tmk_related_fields:\n",
    "            primary_tmk_field = tmk_related_fields[0][0]\n",
    "            print(f\"  Sample {primary_tmk_field} values:\")\n",
    "            \n",
    "            with arcpy.da.SearchCursor(tmk_parcels_path, [primary_tmk_field]) as cursor:\n",
    "                for i, row in enumerate(cursor):\n",
    "                    if i < 8:\n",
    "                        print(f\"    {row[0]} (type: {type(row[0])})\")\n",
    "                    if i >= 7:\n",
    "                        break\n",
    "        \n",
    "        # Check for Maui parcels specifically\n",
    "        if tmk_related_fields:\n",
    "            maui_count = 0\n",
    "            with arcpy.da.SearchCursor(tmk_parcels_path, [primary_tmk_field]) as cursor:\n",
    "                for row in cursor:\n",
    "                    if str(row[0]).startswith('2'):\n",
    "                        maui_count += 1\n",
    "                        if maui_count >= 20000:  # Stop counting at reasonable number\n",
    "                            break\n",
    "            \n",
    "            print(f\"  Estimated Maui parcels (TMK starting with '2'): {maui_count:,}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  Error accessing parcel data: {str(e)}\")\n",
    "        \n",
    "else:\n",
    "    print(\"  TMK parcel polygons NOT FOUND\")\n",
    "    print(\"  Searched in:\")\n",
    "    for path in possible_paths:\n",
    "        print(f\"    {path}\")\n",
    "\n",
    "# Compare TMK formats between datasets\n",
    "print(f\"\\nTMK FORMAT COMPARISON:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "if arcpy.Exists(master_table_path) and tmk_parcels_path and arcpy.Exists(tmk_parcels_path):\n",
    "    # Get a few TMK values from each dataset\n",
    "    cesspool_tmks = []\n",
    "    with arcpy.da.SearchCursor(master_table_path, ['TMK']) as cursor:\n",
    "        for i, row in enumerate(cursor):\n",
    "            if i < 10:\n",
    "                cesspool_tmks.append(str(row[0]))\n",
    "            if i >= 9:\n",
    "                break\n",
    "    \n",
    "    parcel_tmks = []\n",
    "    if tmk_related_fields:\n",
    "        with arcpy.da.SearchCursor(tmk_parcels_path, [primary_tmk_field]) as cursor:\n",
    "            for i, row in enumerate(cursor):\n",
    "                if i < 10:\n",
    "                    parcel_tmks.append(str(row[0]))\n",
    "                if i >= 9:\n",
    "                    break\n",
    "    \n",
    "    print(\"Cesspool TMK format:\")\n",
    "    for tmk in cesspool_tmks[:5]:\n",
    "        print(f\"  '{tmk}' (length: {len(tmk)})\")\n",
    "    \n",
    "    print(\"Parcel TMK format:\")\n",
    "    for tmk in parcel_tmks[:5]:\n",
    "        print(f\"  '{tmk}' (length: {len(tmk)})\")\n",
    "    \n",
    "    # Look for potential matches\n",
    "    matches = set(cesspool_tmks) & set(parcel_tmks)\n",
    "    print(f\"\\nDirect TMK matches found: {len(matches)}\")\n",
    "    if matches:\n",
    "        print(\"Sample matches:\")\n",
    "        for match in list(matches)[:3]:\n",
    "            print(f\"  {match}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(\"DATA STRUCTURE ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Status: Bad outputs cleaned up\")\n",
    "print(\"Next: Develop corrected parcel extraction approach based on TMK format analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FIX DATA ACCESS AND CONTINUE TMK ANALYSIS\n",
      "======================================================================\n",
      "CESSPOOL DATA CONFIRMED:\n",
      "  TMK Field: Integer type, 10 length\n",
      "  Sample format: 186006001 (9 digits)\n",
      "  Records: 82,141\n",
      "\n",
      "FINDING TMK PARCEL POLYGONS...\n",
      "Checking geodatabase: C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\ParcelAnalysis.gdb\n",
      "  Found: Parcels_Hawaii - Polygon, 135,471 records\n",
      "  Found: tmk_state - Polygon, 384,262 records\n",
      "  Found: Parcels_Honolulu - Polygon, 171,905 records\n",
      "  Found: Parcels_Kauai - Polygon, 25,121 records\n",
      "  Found: Parcels_Maui - Polygon, 51,765 records\n",
      "\n",
      "Searching data directory recursively...\n",
      "  Found: tmk_state.shp - Polygon, 384,262 records\n",
      "\n",
      "USING PARCEL DATASET:\n",
      "  Name: tmk_state\n",
      "  Path: C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\ParcelAnalysis.gdb\\tmk_state\n",
      "  Type: Polygon\n",
      "  Records: 384,262\n",
      "\n",
      "TMK FIELDS IN PARCEL DATA:\n",
      "  TMK: Integer (length: 4)\n",
      "  TMK_txt: String (length: 9)\n",
      "  cty_tmk: String (length: 8)\n",
      "\n",
      "Sample TMK values:\n",
      "  389014053 (type: <class 'int'>)\n",
      "  389014054 (type: <class 'int'>)\n",
      "  389014055 (type: <class 'int'>)\n",
      "  389014056 (type: <class 'int'>)\n",
      "  388009112 (type: <class 'int'>)\n",
      "  388009113 (type: <class 'int'>)\n",
      "  388009114 (type: <class 'int'>)\n",
      "  388009115 (type: <class 'int'>)\n",
      "\n",
      "TMK FORMAT COMPARISON:\n",
      "==============================\n",
      "Cesspool TMKs: Integer (186006001)\n",
      "Parcel TMKs: Integer (TMK)\n",
      "Direct matches in sample: 0/100\n",
      "No direct matches - need to investigate TMK format differences\n",
      "Cesspool sample: [412011010, 413007107, 432008067]\n",
      "Parcel sample: [392064035, 389014053, 389014054]\n",
      "\n",
      "======================================================================\n",
      "TMK DATA ANALYSIS COMPLETE\n",
      "Next step will depend on TMK format compatibility results\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FIX DATA ACCESS AND CONTINUE TMK ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# The cesspool data is good - TMK as Integer, 82,141 records\n",
    "print(\"CESSPOOL DATA CONFIRMED:\")\n",
    "print(\"  TMK Field: Integer type, 10 length\")\n",
    "print(\"  Sample format: 186006001 (9 digits)\")\n",
    "print(\"  Records: 82,141\")\n",
    "\n",
    "print(f\"\\nFINDING TMK PARCEL POLYGONS...\")\n",
    "\n",
    "# Search more thoroughly for the parcel polygons\n",
    "parcel_candidates = []\n",
    "\n",
    "# Check in geodatabase first\n",
    "gdb_path = os.path.join(workspace, \"ParcelAnalysis.gdb\")\n",
    "if arcpy.Exists(gdb_path):\n",
    "    print(f\"Checking geodatabase: {gdb_path}\")\n",
    "    arcpy.env.workspace = gdb_path\n",
    "    feature_classes = arcpy.ListFeatureClasses()\n",
    "    \n",
    "    for fc in feature_classes:\n",
    "        if 'tmk' in fc.lower() or 'parcel' in fc.lower():\n",
    "            fc_path = os.path.join(gdb_path, fc)\n",
    "            try:\n",
    "                desc = arcpy.Describe(fc_path)\n",
    "                count = int(arcpy.management.GetCount(fc_path)[0])\n",
    "                parcel_candidates.append({\n",
    "                    'name': fc,\n",
    "                    'path': fc_path,\n",
    "                    'type': desc.shapeType,\n",
    "                    'count': count\n",
    "                })\n",
    "                print(f\"  Found: {fc} - {desc.shapeType}, {count:,} records\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Could not access {fc}: {str(e)}\")\n",
    "\n",
    "# Check data directory more thoroughly\n",
    "print(f\"\\nSearching data directory recursively...\")\n",
    "data_dir = os.path.join(workspace, \"data\")\n",
    "\n",
    "for root, dirs, files in os.walk(data_dir):\n",
    "    for file in files:\n",
    "        if (('tmk' in file.lower() or 'parcel' in file.lower()) and \n",
    "            file.endswith('.shp')):\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                desc = arcpy.Describe(file_path)\n",
    "                if desc.shapeType == 'Polygon':\n",
    "                    count = int(arcpy.management.GetCount(file_path)[0])\n",
    "                    parcel_candidates.append({\n",
    "                        'name': file,\n",
    "                        'path': file_path,\n",
    "                        'type': desc.shapeType,\n",
    "                        'count': count\n",
    "                    })\n",
    "                    print(f\"  Found: {file} - {desc.shapeType}, {count:,} records\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Could not access {file}: {str(e)}\")\n",
    "\n",
    "# Select the best parcel dataset\n",
    "if parcel_candidates:\n",
    "    # Choose the one with most records (likely most comprehensive)\n",
    "    best_parcel = max(parcel_candidates, key=lambda x: x['count'])\n",
    "    tmk_parcels_path = best_parcel['path']\n",
    "    \n",
    "    print(f\"\\nUSING PARCEL DATASET:\")\n",
    "    print(f\"  Name: {best_parcel['name']}\")\n",
    "    print(f\"  Path: {tmk_parcels_path}\")\n",
    "    print(f\"  Type: {best_parcel['type']}\")\n",
    "    print(f\"  Records: {best_parcel['count']:,}\")\n",
    "    \n",
    "    # Now examine the TMK field structure\n",
    "    try:\n",
    "        parcel_fields = [(f.name, f.type, f.length) for f in arcpy.ListFields(tmk_parcels_path)]\n",
    "        tmk_related_fields = [f for f in parcel_fields if 'tmk' in f[0].lower()]\n",
    "        \n",
    "        print(f\"\\nTMK FIELDS IN PARCEL DATA:\")\n",
    "        for field_info in tmk_related_fields:\n",
    "            print(f\"  {field_info[0]}: {field_info[1]} (length: {field_info[2]})\")\n",
    "        \n",
    "        # Sample the TMK values\n",
    "        if tmk_related_fields:\n",
    "            primary_tmk_field = tmk_related_fields[0][0]\n",
    "            print(f\"\\nSample {primary_tmk_field} values:\")\n",
    "            \n",
    "            with arcpy.da.SearchCursor(tmk_parcels_path, [primary_tmk_field]) as cursor:\n",
    "                for i, row in enumerate(cursor):\n",
    "                    if i < 8:\n",
    "                        print(f\"  {row[0]} (type: {type(row[0])})\")\n",
    "                    if i >= 7:\n",
    "                        break\n",
    "            \n",
    "            print(f\"\\nTMK FORMAT COMPARISON:\")\n",
    "            print(\"=\" * 30)\n",
    "            print(\"Cesspool TMKs: Integer (186006001)\")\n",
    "            print(f\"Parcel TMKs: {tmk_related_fields[0][1]} ({primary_tmk_field})\")\n",
    "            \n",
    "            # Test for direct matches\n",
    "            cesspool_sample = set()\n",
    "            with arcpy.da.SearchCursor(master_table_path, ['TMK']) as cursor:\n",
    "                for i, row in enumerate(cursor):\n",
    "                    if i < 100:  # Sample first 100\n",
    "                        cesspool_sample.add(row[0])\n",
    "                    if i >= 99:\n",
    "                        break\n",
    "            \n",
    "            parcel_sample = set()\n",
    "            with arcpy.da.SearchCursor(tmk_parcels_path, [primary_tmk_field]) as cursor:\n",
    "                for i, row in enumerate(cursor):\n",
    "                    if i < 100:  # Sample first 100\n",
    "                        parcel_sample.add(row[0])\n",
    "                    if i >= 99:\n",
    "                        break\n",
    "            \n",
    "            matches = cesspool_sample & parcel_sample\n",
    "            print(f\"Direct matches in sample: {len(matches)}/100\")\n",
    "            \n",
    "            if matches:\n",
    "                print(\"Sample matches found - TMK formats are compatible!\")\n",
    "                print(f\"Ready to extract parcel polygons for {len(cesspool_sample)} cesspools\")\n",
    "            else:\n",
    "                print(\"No direct matches - need to investigate TMK format differences\")\n",
    "                print(\"Cesspool sample:\", list(cesspool_sample)[:3])\n",
    "                print(\"Parcel sample:\", list(parcel_sample)[:3])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error examining parcel TMK fields: {str(e)}\")\n",
    "\n",
    "else:\n",
    "    print(\"No suitable parcel polygon datasets found\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(\"TMK DATA ANALYSIS COMPLETE\")\n",
    "print(\"Next step will depend on TMK format compatibility results\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DIAGNOSE TMK MISMATCH AND FIND SOLUTION\n",
      "======================================================================\n",
      "INVESTIGATING TMK MISMATCH...\n",
      "Collecting larger TMK samples for pattern analysis...\n",
      "Cesspool TMKs by island:\n",
      "  Oahu: [186006001, 186006001, 186006001, 186006001, 186006001]\n",
      "  Kauai: [412010022, 412010039, 412010053, 412010110, 412010138]\n",
      "  Maui: [221019029, 221019033, 221019034, 221019035, 221019036]\n",
      "  BI: [311002003, 311002005, 311002010, 311002011, 311002012]\n",
      "\n",
      "Parcel TMK sample (first 10): [389014053, 389014054, 389014055, 389014056, 388009112, 388009113, 388009114, 388009115, 388009116, 388009117]\n",
      "\n",
      "TMK PATTERN ANALYSIS:\n",
      "==============================\n",
      "Direct matches found: 0 out of 970 cesspool TMKs\n",
      "NO DIRECT MATCHES - investigating alternative approaches...\n",
      "\n",
      "Checking for systematic TMK differences...\n",
      "Cesspool TMK ranges by island:\n",
      "  Oahu: 186006001 - 186006001 (starts with 1)\n",
      "  Kauai: 412010022 - 412010138 (starts with 4)\n",
      "  Maui: 221019029 - 221019036 (starts with 2)\n",
      "  BI: 311002003 - 311002012 (starts with 3)\n",
      "Parcel TMK patterns by first digit:\n",
      "  3xxx: [389014053, 389014054, 389014055]\n",
      "\n",
      "ALTERNATIVE: SPATIAL RELATIONSHIP APPROACH\n",
      "==================================================\n",
      "Since TMK direct matching may not work, use spatial containment:\n",
      "1. Use 'Select Layer by Location' with CONTAINS relationship\n",
      "2. Select parcels that contain cesspool points (centroids)\n",
      "3. This should work regardless of TMK numbering differences\n",
      "\n",
      "Testing spatial approach with small sample...\n",
      "Created test sample: 100 cesspool points\n",
      "Spatial selection result: 70 parcels contain 100 cesspool points\n",
      "‚úì SPATIAL APPROACH WORKS!\n",
      "Recommendation: Use spatial relationship instead of TMK matching\n",
      "\n",
      "======================================================================\n",
      "TMK DIAGNOSIS COMPLETE\n",
      "======================================================================\n",
      "Next: Implement working parcel extraction method\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DIAGNOSE TMK MISMATCH AND FIND SOLUTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Use the identified best datasets\n",
    "tmk_parcels_path = r\"C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\ParcelAnalysis.gdb\\tmk_state\"\n",
    "cesspool_points_path = os.path.join(foundation_dir, \"TMK_Master_Attributes.shp\")\n",
    "\n",
    "print(\"INVESTIGATING TMK MISMATCH...\")\n",
    "\n",
    "# Get larger samples for pattern analysis\n",
    "print(\"Collecting larger TMK samples for pattern analysis...\")\n",
    "\n",
    "cesspool_tmks = []\n",
    "island_tmks = {}\n",
    "\n",
    "with arcpy.da.SearchCursor(cesspool_points_path, ['TMK', 'Island']) as cursor:\n",
    "    for row in cursor:\n",
    "        tmk = row[0]\n",
    "        island = row[1] if row[1] else 'Unknown'\n",
    "        cesspool_tmks.append(tmk)\n",
    "        \n",
    "        if island not in island_tmks:\n",
    "            island_tmks[island] = []\n",
    "        if len(island_tmks[island]) < 5:\n",
    "            island_tmks[island].append(tmk)\n",
    "\n",
    "print(\"Cesspool TMKs by island:\")\n",
    "for island, tmks in island_tmks.items():\n",
    "    print(f\"  {island}: {tmks}\")\n",
    "\n",
    "# Get parcel TMK samples\n",
    "parcel_tmks = []\n",
    "with arcpy.da.SearchCursor(tmk_parcels_path, ['TMK']) as cursor:\n",
    "    for i, row in enumerate(cursor):\n",
    "        if i < 500:  # Larger sample\n",
    "            parcel_tmks.append(row[0])\n",
    "        if i >= 499:\n",
    "            break\n",
    "\n",
    "print(f\"\\nParcel TMK sample (first 10): {parcel_tmks[:10]}\")\n",
    "\n",
    "# Analyze TMK patterns\n",
    "print(f\"\\nTMK PATTERN ANALYSIS:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Check if cesspool TMKs are a subset of parcel TMKs\n",
    "cesspool_set = set(cesspool_tmks[:1000])  # Sample first 1000\n",
    "parcel_set = set(parcel_tmks)\n",
    "\n",
    "matches = cesspool_set & parcel_set\n",
    "print(f\"Direct matches found: {len(matches)} out of {len(cesspool_set)} cesspool TMKs\")\n",
    "\n",
    "if matches:\n",
    "    print(\"SOLUTION: Direct TMK matching will work\")\n",
    "    print(f\"Sample matches: {list(matches)[:5]}\")\n",
    "else:\n",
    "    print(\"NO DIRECT MATCHES - investigating alternative approaches...\")\n",
    "    \n",
    "    # Check if there's a systematic pattern difference\n",
    "    print(\"\\nChecking for systematic TMK differences...\")\n",
    "    \n",
    "    # Look at TMK ranges by island\n",
    "    cesspool_ranges = {}\n",
    "    for island, tmks in island_tmks.items():\n",
    "        if tmks:\n",
    "            cesspool_ranges[island] = {\n",
    "                'min': min(tmks),\n",
    "                'max': max(tmks),\n",
    "                'first_digit': str(tmks[0])[0]\n",
    "            }\n",
    "    \n",
    "    print(\"Cesspool TMK ranges by island:\")\n",
    "    for island, info in cesspool_ranges.items():\n",
    "        print(f\"  {island}: {info['min']} - {info['max']} (starts with {info['first_digit']})\")\n",
    "    \n",
    "    # Check parcel TMK ranges  \n",
    "    parcel_first_digits = {}\n",
    "    for tmk in parcel_tmks[:100]:\n",
    "        first_digit = str(tmk)[0]\n",
    "        if first_digit not in parcel_first_digits:\n",
    "            parcel_first_digits[first_digit] = []\n",
    "        if len(parcel_first_digits[first_digit]) < 3:\n",
    "            parcel_first_digits[first_digit].append(tmk)\n",
    "    \n",
    "    print(\"Parcel TMK patterns by first digit:\")\n",
    "    for digit, examples in parcel_first_digits.items():\n",
    "        print(f\"  {digit}xxx: {examples}\")\n",
    "\n",
    "# Alternative approach: Spatial relationship\n",
    "print(f\"\\nALTERNATIVE: SPATIAL RELATIONSHIP APPROACH\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Since TMK direct matching may not work, use spatial containment:\")\n",
    "print(\"1. Use 'Select Layer by Location' with CONTAINS relationship\")\n",
    "print(\"2. Select parcels that contain cesspool points (centroids)\")\n",
    "print(\"3. This should work regardless of TMK numbering differences\")\n",
    "\n",
    "# Test spatial approach on small sample\n",
    "print(\"\\nTesting spatial approach with small sample...\")\n",
    "\n",
    "try:\n",
    "    # Create a small sample for testing\n",
    "    test_sample_path = os.path.join(foundation_dir, \"Test_Cesspool_Sample.shp\")\n",
    "    \n",
    "    # Select first 100 cesspool points for testing\n",
    "    arcpy.management.MakeFeatureLayer(cesspool_points_path, \"cesspool_layer\")\n",
    "    arcpy.management.SelectLayerByAttribute(\"cesspool_layer\", \"NEW_SELECTION\", \"FID < 100\")\n",
    "    arcpy.management.CopyFeatures(\"cesspool_layer\", test_sample_path)\n",
    "    \n",
    "    test_count = int(arcpy.management.GetCount(test_sample_path)[0])\n",
    "    print(f\"Created test sample: {test_count} cesspool points\")\n",
    "    \n",
    "    # Test spatial selection\n",
    "    arcpy.management.MakeFeatureLayer(tmk_parcels_path, \"parcel_layer\")\n",
    "    \n",
    "    # Select parcels that contain the test cesspool points\n",
    "    arcpy.management.SelectLayerByLocation(\n",
    "        in_layer=\"parcel_layer\",\n",
    "        overlap_type=\"CONTAINS\",\n",
    "        select_features=test_sample_path,\n",
    "        selection_type=\"NEW_SELECTION\"\n",
    "    )\n",
    "    \n",
    "    selected_count = int(arcpy.management.GetCount(\"parcel_layer\")[0])\n",
    "    print(f\"Spatial selection result: {selected_count} parcels contain {test_count} cesspool points\")\n",
    "    \n",
    "    if selected_count > 0:\n",
    "        print(\"‚úì SPATIAL APPROACH WORKS!\")\n",
    "        print(\"Recommendation: Use spatial relationship instead of TMK matching\")\n",
    "    else:\n",
    "        print(\"‚ö† Spatial approach also failed - need to investigate further\")\n",
    "    \n",
    "    # Clean up test files\n",
    "    arcpy.management.Delete(\"cesspool_layer\")\n",
    "    arcpy.management.Delete(\"parcel_layer\") \n",
    "    arcpy.management.Delete(test_sample_path)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error testing spatial approach: {str(e)}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(\"TMK DIAGNOSIS COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Next: Implement working parcel extraction method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXTRACT CESSPOOL PARCEL POLYGONS - SPATIAL APPROACH\n",
      "======================================================================\n",
      "Using spatial relationship approach...\n",
      "Parcel polygons: C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\ParcelAnalysis.gdb\\tmk_state (384,262 parcels)\n",
      "Cesspool points: C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\outputs\\foundation\\TMK_Master_Attributes.shp (82,141 points)\n",
      "Selecting parcels that contain cesspool points...\n",
      "This may take a few minutes for 82,000+ cesspools...\n",
      "Selected 68,565 parcel polygons containing cesspools\n",
      "Creating cesspool parcel polygon dataset...\n",
      "‚úì Created: 68,565 cesspool parcel polygons\n",
      "‚úì Type: Polygon\n",
      "‚úì Location: C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\outputs\\foundation\\Cesspool_Parcel_Polygons_Spatial.shp\n",
      "Available TMK fields in parcels: ['TMK', 'TMK_txt', 'cty_tmk']\n",
      "\n",
      "Sample cesspool parcel polygons:\n",
      "  Parcel TMK: 389014053, Area: 0.02 acres\n",
      "  Parcel TMK: 389014054, Area: 0.02 acres\n",
      "  Parcel TMK: 389014055, Area: 0.02 acres\n",
      "  Parcel TMK: 389014056, Area: 0.02 acres\n",
      "  Parcel TMK: 388009127, Area: 0.02 acres\n",
      "\n",
      "Associating parcel polygons with cesspool TMK data...\n",
      "‚úì Created final dataset: 68,565 parcels with cesspool attributes\n",
      "Preserved cesspool attributes: ['TMK', 'dist2_MunW', 'dist2_DomW']\n",
      "Total fields in final dataset: 37\n",
      "\n",
      "Sample integrated cesspool-parcel data:\n",
      "  TMK: 389014053 | dist2_MunW: 18718m | Shape_Area: 0.02 acres\n",
      "  TMK: 389014054 | dist2_MunW: 18735m | Shape_Area: 0.02 acres\n",
      "  TMK: 389014055 | dist2_MunW: 18762m | Shape_Area: 0.02 acres\n",
      "\n",
      "Creating Maui subset for area-weighted pilot...\n",
      "ERROR in spatial parcel extraction: ERROR 000358: Invalid expression TMK LIKE '2%'\n",
      "Failed to execute (Select).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXTRACT CESSPOOL PARCEL POLYGONS - SPATIAL APPROACH\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Use spatial relationship to get parcel polygons for cesspools\n",
    "tmk_parcels_path = r\"C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\ParcelAnalysis.gdb\\tmk_state\"\n",
    "cesspool_points_path = os.path.join(foundation_dir, \"TMK_Master_Attributes.shp\")\n",
    "cesspool_parcels_path = os.path.join(foundation_dir, \"Cesspool_Parcel_Polygons_Spatial.shp\")\n",
    "\n",
    "print(\"Using spatial relationship approach...\")\n",
    "print(f\"Parcel polygons: {tmk_parcels_path} ({384262:,} parcels)\")\n",
    "print(f\"Cesspool points: {cesspool_points_path} ({82141:,} points)\")\n",
    "\n",
    "try:\n",
    "    # Create feature layers for spatial operations\n",
    "    arcpy.management.MakeFeatureLayer(tmk_parcels_path, \"parcel_layer\")\n",
    "    arcpy.management.MakeFeatureLayer(cesspool_points_path, \"cesspool_layer\")\n",
    "    \n",
    "    print(\"Selecting parcels that contain cesspool points...\")\n",
    "    print(\"This may take a few minutes for 82,000+ cesspools...\")\n",
    "    \n",
    "    # Select parcels that contain cesspool points\n",
    "    arcpy.management.SelectLayerByLocation(\n",
    "        in_layer=\"parcel_layer\",\n",
    "        overlap_type=\"CONTAINS\",\n",
    "        select_features=\"cesspool_layer\",\n",
    "        selection_type=\"NEW_SELECTION\"\n",
    "    )\n",
    "    \n",
    "    # Get count of selected parcels\n",
    "    selected_count = int(arcpy.management.GetCount(\"parcel_layer\")[0])\n",
    "    print(f\"Selected {selected_count:,} parcel polygons containing cesspools\")\n",
    "    \n",
    "    if selected_count > 0:\n",
    "        # Copy selected parcels to new dataset\n",
    "        print(\"Creating cesspool parcel polygon dataset...\")\n",
    "        arcpy.management.CopyFeatures(\"parcel_layer\", cesspool_parcels_path)\n",
    "        \n",
    "        # Verify the result\n",
    "        result_count = int(arcpy.management.GetCount(cesspool_parcels_path)[0])\n",
    "        result_desc = arcpy.Describe(cesspool_parcels_path)\n",
    "        \n",
    "        print(f\"‚úì Created: {result_count:,} cesspool parcel polygons\")\n",
    "        print(f\"‚úì Type: {result_desc.shapeType}\")\n",
    "        print(f\"‚úì Location: {cesspool_parcels_path}\")\n",
    "        \n",
    "        # Check the parcel TMK fields we now have\n",
    "        parcel_fields = [f.name for f in arcpy.ListFields(cesspool_parcels_path)]\n",
    "        tmk_fields = [f for f in parcel_fields if 'tmk' in f.lower()]\n",
    "        print(f\"Available TMK fields in parcels: {tmk_fields}\")\n",
    "        \n",
    "        # Sample the results\n",
    "        print(\"\\nSample cesspool parcel polygons:\")\n",
    "        sample_fields = ['TMK', 'Shape_Area'] if 'TMK' in parcel_fields else [tmk_fields[0], 'Shape_Area']\n",
    "        \n",
    "        with arcpy.da.SearchCursor(cesspool_parcels_path, sample_fields) as cursor:\n",
    "            for i, row in enumerate(cursor):\n",
    "                if i < 5:\n",
    "                    area_acres = row[1] / 43560 if row[1] else 0  # Convert sq ft to acres\n",
    "                    print(f\"  Parcel TMK: {row[0]}, Area: {area_acres:.2f} acres\")\n",
    "                if i >= 4:\n",
    "                    break\n",
    "        \n",
    "        # Now we need to associate these parcel polygons back to cesspool data\n",
    "        print(f\"\\nAssociating parcel polygons with cesspool TMK data...\")\n",
    "        \n",
    "        # Spatial join to add cesspool attributes to parcel polygons\n",
    "        cesspool_parcels_with_data_path = os.path.join(foundation_dir, \"Cesspool_Parcels_With_Attributes.shp\")\n",
    "        \n",
    "        arcpy.analysis.SpatialJoin(\n",
    "            target_features=cesspool_parcels_path,        # Parcel polygons\n",
    "            join_features=cesspool_points_path,           # Cesspool points with TMK data\n",
    "            out_feature_class=cesspool_parcels_with_data_path,\n",
    "            join_operation=\"JOIN_ONE_TO_ONE\",\n",
    "            join_type=\"KEEP_ALL\",\n",
    "            match_option=\"CONTAINS\"                       # Parcel contains cesspool point\n",
    "        )\n",
    "        \n",
    "        final_count = int(arcpy.management.GetCount(cesspool_parcels_with_data_path)[0])\n",
    "        print(f\"‚úì Created final dataset: {final_count:,} parcels with cesspool attributes\")\n",
    "        \n",
    "        # Verify the joined data\n",
    "        final_fields = [f.name for f in arcpy.ListFields(cesspool_parcels_with_data_path)]\n",
    "        cesspool_fields = ['TMK', 'Island', 'dist2_MunW', 'dist2_DomW']\n",
    "        preserved_fields = [f for f in cesspool_fields if f in final_fields]\n",
    "        \n",
    "        print(f\"Preserved cesspool attributes: {preserved_fields}\")\n",
    "        print(f\"Total fields in final dataset: {len(final_fields)}\")\n",
    "        \n",
    "        # Sample the final integrated data\n",
    "        print(\"\\nSample integrated cesspool-parcel data:\")\n",
    "        display_fields = ['TMK', 'Island', 'dist2_MunW', 'Shape_Area']\n",
    "        display_fields = [f for f in display_fields if f in final_fields]\n",
    "        \n",
    "        with arcpy.da.SearchCursor(cesspool_parcels_with_data_path, display_fields) as cursor:\n",
    "            for i, row in enumerate(cursor):\n",
    "                if i < 3:\n",
    "                    display_data = []\n",
    "                    for j, value in enumerate(row):\n",
    "                        field_name = display_fields[j]\n",
    "                        if 'area' in field_name.lower() and value:\n",
    "                            display_value = f\"{value/43560:.2f} acres\"\n",
    "                        elif 'dist' in field_name.lower() and value:\n",
    "                            display_value = f\"{value:.0f}m\"\n",
    "                        else:\n",
    "                            display_value = str(value)\n",
    "                        display_data.append(f\"{field_name}: {display_value}\")\n",
    "                    print(f\"  {' | '.join(display_data)}\")\n",
    "                if i >= 2:\n",
    "                    break\n",
    "        \n",
    "        # Create Maui subset for pilot\n",
    "        print(f\"\\nCreating Maui subset for area-weighted pilot...\")\n",
    "        maui_cesspool_parcels_path = os.path.join(foundation_dir, \"Maui_Cesspool_Parcels_Final.shp\")\n",
    "        \n",
    "        # Use cesspool TMK for Maui selection (starts with 2)\n",
    "        maui_expression = \"TMK LIKE '2%'\" if 'TMK' in final_fields else f\"{preserved_fields[0]} LIKE '2%'\"\n",
    "        \n",
    "        arcpy.analysis.Select(cesspool_parcels_with_data_path, maui_cesspool_parcels_path, maui_expression)\n",
    "        \n",
    "        maui_count = int(arcpy.management.GetCount(maui_cesspool_parcels_path)[0])\n",
    "        print(f\"‚úì Maui pilot dataset: {maui_count:,} cesspool parcels\")\n",
    "        \n",
    "        print(f\"\\n\" + \"=\" * 70)\n",
    "        print(\"CESSPOOL PARCEL EXTRACTION SUCCESS!\")\n",
    "        print(\"=\" * 70)\n",
    "        print(\"Created datasets:\")\n",
    "        print(f\"  Statewide: {cesspool_parcels_with_data_path}\")\n",
    "        print(f\"  Maui pilot: {maui_cesspool_parcels_path}\")\n",
    "        print(\"Each parcel polygon now has:\")\n",
    "        print(\"  - Parcel geometry for area calculations\")\n",
    "        print(\"  - Cesspool TMK and attribute data\")  \n",
    "        print(\"  - Wells distance information\")\n",
    "        print(\"Ready for area-weighted spatial analysis!\")\n",
    "        \n",
    "    else:\n",
    "        print(\"ERROR: No parcels selected - spatial relationship failed\")\n",
    "        \n",
    "    # Clean up layers\n",
    "    arcpy.management.Delete(\"parcel_layer\")\n",
    "    arcpy.management.Delete(\"cesspool_layer\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR in spatial parcel extraction: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "CREATE MAUI SUBSET - FIXED SQL EXPRESSION\n",
      "==================================================\n",
      "Source dataset: C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\outputs\\foundation\\Cesspool_Parcels_With_Attributes.shp\n",
      "TMK field type: Integer\n",
      "Using numeric expression for Integer TMK: TMK >= 200000000 AND TMK < 300000000\n",
      "Creating Maui subset...\n",
      "Success! Maui subset created: 8,248 parcels\n",
      "Sample Maui cesspool parcels:\n",
      "  TMK: 211003003\n",
      "  TMK: 211003012\n",
      "  TMK: 211003030\n",
      "  TMK: 211003031\n",
      "  TMK: 211003041\n",
      "\n",
      "Maui pilot dataset ready: C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\outputs\\foundation\\Maui_Cesspool_Parcels_Pilot.shp\n",
      "Ready for area-weighted spatial analysis!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"CREATE MAUI SUBSET - FIXED SQL EXPRESSION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define paths\n",
    "foundation_dir = os.path.join(workspace, \"outputs\", \"foundation\")\n",
    "cesspool_parcels_path = os.path.join(foundation_dir, \"Cesspool_Parcels_With_Attributes.shp\")\n",
    "maui_subset_path = os.path.join(foundation_dir, \"Maui_Cesspool_Parcels_Pilot.shp\")\n",
    "\n",
    "print(f\"Source dataset: {cesspool_parcels_path}\")\n",
    "\n",
    "if arcpy.Exists(cesspool_parcels_path):\n",
    "    # Check the TMK field type first\n",
    "    fields = arcpy.ListFields(cesspool_parcels_path)\n",
    "    tmk_field_info = None\n",
    "    \n",
    "    for field in fields:\n",
    "        if field.name == 'TMK':\n",
    "            tmk_field_info = field\n",
    "            break\n",
    "    \n",
    "    if tmk_field_info:\n",
    "        print(f\"TMK field type: {tmk_field_info.type}\")\n",
    "        \n",
    "        # Since TMK is Integer type, we need to use numeric comparison\n",
    "        # Maui TMKs start with 2, so they're in range 200000000-299999999\n",
    "        if tmk_field_info.type in ['Integer', 'OID']:\n",
    "            maui_expression = \"TMK >= 200000000 AND TMK < 300000000\"\n",
    "            print(f\"Using numeric expression for Integer TMK: {maui_expression}\")\n",
    "        else:\n",
    "            maui_expression = \"TMK LIKE '2%'\"\n",
    "            print(f\"Using text expression: {maui_expression}\")\n",
    "        \n",
    "        print(\"Creating Maui subset...\")\n",
    "        \n",
    "        try:\n",
    "            arcpy.analysis.Select(cesspool_parcels_path, maui_subset_path, maui_expression)\n",
    "            \n",
    "            # Verify results\n",
    "            maui_count = int(arcpy.management.GetCount(maui_subset_path)[0])\n",
    "            print(f\"Success! Maui subset created: {maui_count:,} parcels\")\n",
    "            \n",
    "            # Sample the results\n",
    "            print(\"Sample Maui cesspool parcels:\")\n",
    "            sample_fields = ['TMK', 'Island'] if 'Island' in [f.name for f in fields] else ['TMK']\n",
    "            \n",
    "            with arcpy.da.SearchCursor(maui_subset_path, sample_fields) as cursor:\n",
    "                for i, row in enumerate(cursor):\n",
    "                    if i < 5:\n",
    "                        if len(sample_fields) > 1:\n",
    "                            print(f\"  TMK: {row[0]}, Island: {row[1]}\")\n",
    "                        else:\n",
    "                            print(f\"  TMK: {row[0]}\")\n",
    "                    if i >= 4:\n",
    "                        break\n",
    "            \n",
    "            print(f\"\\nMaui pilot dataset ready: {maui_subset_path}\")\n",
    "            print(f\"Ready for area-weighted spatial analysis!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating Maui subset: {str(e)}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"TMK field not found in dataset\")\n",
    "\n",
    "else:\n",
    "    print(f\"Source dataset not found: {cesspool_parcels_path}\")\n",
    "    print(\"Need to run the spatial parcel extraction first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EXAMINE BEDROOMS_OUT.CSV COVERAGE\n",
      "==================================================\n",
      "Reading: C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\data\\bedrooms_out.csv\n",
      "Total records: 70,322\n",
      "Columns: ['TMK', 'BED_ROOMS']\n",
      "\n",
      "TMK analysis:\n",
      "Sample TMK values: [2110010230000, 2110020090000, 2110030030000, 2110030060000, 2110030120000]\n",
      "TMK data type: int64\n",
      "\n",
      "Geographic coverage by TMK first digit:\n",
      "  2xxx: 70,322 records (Maui County)\n",
      "\n",
      "Bedroom count distribution:\n",
      "  0 bedrooms: 104 parcels\n",
      "  1 bedrooms: 13,336 parcels\n",
      "  2 bedrooms: 20,277 parcels\n",
      "  3 bedrooms: 24,339 parcels\n",
      "  4 bedrooms: 7,831 parcels\n",
      "  5 bedrooms: 2,661 parcels\n",
      "  6 bedrooms: 951 parcels\n",
      "  7 bedrooms: 362 parcels\n",
      "  8 bedrooms: 222 parcels\n",
      "  9 bedrooms: 115 parcels\n",
      "  ... and 8 other bedroom counts\n",
      "\n",
      "Data quality:\n",
      "  Zero bedrooms: 104 parcels\n",
      "  Null/missing: 0 parcels\n",
      "\n",
      "Sample records by island:\n",
      "\n",
      "Maui County:\n",
      "  TMK: 2110010230000, Bedrooms: 1\n",
      "  TMK: 2110020090000, Bedrooms: 3\n",
      "  TMK: 2110030030000, Bedrooms: 3\n",
      "\n",
      "‚ö† MAUI ONLY: Only Maui County data present\n",
      "\n",
      "READY TO JOIN BEDROOM DATA\n",
      "If coverage matches our cesspool data, we can proceed with TMK join\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"EXAMINE BEDROOMS_OUT.CSV COVERAGE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the bedrooms file\n",
    "bedrooms_file = os.path.join(data_dir, \"bedrooms_out.csv\")\n",
    "\n",
    "if os.path.exists(bedrooms_file):\n",
    "    print(f\"Reading: {bedrooms_file}\")\n",
    "    \n",
    "    try:\n",
    "        df_bedrooms = pd.read_csv(bedrooms_file)\n",
    "        print(f\"Total records: {len(df_bedrooms):,}\")\n",
    "        print(f\"Columns: {list(df_bedrooms.columns)}\")\n",
    "        \n",
    "        # Check TMK format and coverage\n",
    "        print(f\"\\nTMK analysis:\")\n",
    "        print(f\"Sample TMK values: {df_bedrooms['TMK'].head().tolist()}\")\n",
    "        print(f\"TMK data type: {df_bedrooms['TMK'].dtype}\")\n",
    "        \n",
    "        # Determine geographic coverage by TMK first digit\n",
    "        df_bedrooms['TMK_str'] = df_bedrooms['TMK'].astype(str)\n",
    "        df_bedrooms['first_digit'] = df_bedrooms['TMK_str'].str[0]\n",
    "        \n",
    "        coverage = df_bedrooms['first_digit'].value_counts().sort_index()\n",
    "        print(f\"\\nGeographic coverage by TMK first digit:\")\n",
    "        \n",
    "        island_mapping = {\n",
    "            '1': 'Big Island (Hawaii County)', \n",
    "            '2': 'Maui County',\n",
    "            '3': 'Honolulu County (Oahu)',\n",
    "            '4': 'Kauai County'\n",
    "        }\n",
    "        \n",
    "        for digit, count in coverage.items():\n",
    "            island = island_mapping.get(digit, f'Unknown ({digit})')\n",
    "            print(f\"  {digit}xxx: {count:,} records ({island})\")\n",
    "        \n",
    "        # Check bedroom count distribution\n",
    "        print(f\"\\nBedroom count distribution:\")\n",
    "        bedroom_dist = df_bedrooms['BED_ROOMS'].value_counts().sort_index()\n",
    "        for bedrooms, count in bedroom_dist.head(10).items():\n",
    "            print(f\"  {bedrooms} bedrooms: {count:,} parcels\")\n",
    "        \n",
    "        if len(bedroom_dist) > 10:\n",
    "            print(f\"  ... and {len(bedroom_dist) - 10} other bedroom counts\")\n",
    "        \n",
    "        # Check for missing/zero bedrooms\n",
    "        zero_bedrooms = (df_bedrooms['BED_ROOMS'] == 0).sum()\n",
    "        null_bedrooms = df_bedrooms['BED_ROOMS'].isna().sum()\n",
    "        print(f\"\\nData quality:\")\n",
    "        print(f\"  Zero bedrooms: {zero_bedrooms:,} parcels\")\n",
    "        print(f\"  Null/missing: {null_bedrooms:,} parcels\")\n",
    "        \n",
    "        # Sample records by island\n",
    "        print(f\"\\nSample records by island:\")\n",
    "        for digit in ['1', '2', '3', '4']:\n",
    "            island_data = df_bedrooms[df_bedrooms['first_digit'] == digit].head(3)\n",
    "            if len(island_data) > 0:\n",
    "                island_name = island_mapping[digit]\n",
    "                print(f\"\\n{island_name}:\")\n",
    "                for _, row in island_data.iterrows():\n",
    "                    print(f\"  TMK: {row['TMK']}, Bedrooms: {row['BED_ROOMS']}\")\n",
    "        \n",
    "        # Determine coverage scope\n",
    "        if len(coverage) >= 4:\n",
    "            print(f\"\\n‚úì STATEWIDE COVERAGE: All 4 counties represented\")\n",
    "        elif '2' in coverage:\n",
    "            if len(coverage) == 1:\n",
    "                print(f\"\\n‚ö† MAUI ONLY: Only Maui County data present\")\n",
    "            else:\n",
    "                print(f\"\\n‚ö† PARTIAL COVERAGE: {len(coverage)} counties present\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö† LIMITED COVERAGE: Maui not included\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading bedrooms file: {str(e)}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Bedrooms file not found: {bedrooms_file}\")\n",
    "\n",
    "print(f\"\\nREADY TO JOIN BEDROOM DATA\")\n",
    "print(\"If coverage matches our cesspool data, we can proceed with TMK join\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download from ArcGIS Online...\n",
      "Source: https://services1.arcgis.com/x4h61KaW16vFs7PM/arcgis/rest/services/Categories_HI_Bedrooms_Per_Ac/FeatureServer/0\n",
      "Destination: C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\ParcelAnalysis.gdb\\HI_Parcels_Bedrooms_Bathrooms_Units\n",
      "‚úÖ SUCCESS: Downloaded 384,209 records\n",
      "‚úÖ Saved to: HI_Parcels_Bedrooms_Bathrooms_Units\n",
      "‚úÖ Ready for Matrix analysis!\n"
     ]
    }
   ],
   "source": [
    "import arcpy\n",
    "import os\n",
    "\n",
    "# Set your geodatabase path\n",
    "gdb_path = r\"C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\ParcelAnalysis.gdb\"\n",
    "\n",
    "# Feature service URL\n",
    "service_url = \"https://services1.arcgis.com/x4h61KaW16vFs7PM/arcgis/rest/services/Categories_HI_Bedrooms_Per_Ac/FeatureServer/0\"\n",
    "\n",
    "# Output name in your geodatabase\n",
    "output_name = \"HI_Parcels_Bedrooms_Bathrooms_Units\"\n",
    "\n",
    "print(\"Starting download from ArcGIS Online...\")\n",
    "print(f\"Source: {service_url}\")\n",
    "print(f\"Destination: {os.path.join(gdb_path, output_name)}\")\n",
    "\n",
    "try:\n",
    "    # Copy features from service to local geodatabase\n",
    "    arcpy.management.CopyFeatures(\n",
    "        service_url, \n",
    "        os.path.join(gdb_path, output_name)\n",
    "    )\n",
    "    \n",
    "    # Get record count\n",
    "    result = arcpy.management.GetCount(os.path.join(gdb_path, output_name))\n",
    "    record_count = int(result[0])\n",
    "    \n",
    "    print(f\"‚úÖ SUCCESS: Downloaded {record_count:,} records\")\n",
    "    print(f\"‚úÖ Saved to: {output_name}\")\n",
    "    print(\"‚úÖ Ready for Matrix analysis!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {str(e)}\")\n",
    "    print(\"Try Method 2 below...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TMK COMPATIBILITY CHECK ===\n",
      "Comparing cesspool data vs bedroom data for TMK matching\n",
      "\n",
      "Cesspool data exists: False\n",
      "Bedroom data exists: True\n",
      "\n",
      "‚ùå One or both datasets not found - check import status\n",
      "\n",
      "=== NEXT STEPS ===\n",
      "If TMK formats match: Ready for spatial join\n",
      "If TMK formats differ: Need conversion script\n",
      "If no overlaps: Check county coverage alignment\n"
     ]
    }
   ],
   "source": [
    "import arcpy\n",
    "import os\n",
    "\n",
    "print(\"=== TMK COMPATIBILITY CHECK ===\")\n",
    "print(\"Comparing cesspool data vs bedroom data for TMK matching\")\n",
    "print()\n",
    "\n",
    "# Set paths to your two key datasets\n",
    "gdb_path = r\"C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\ParcelAnalysis.gdb\"\n",
    "cesspool_fc = os.path.join(gdb_path, \"Cesspool_Parcels_With_Attributes\")\n",
    "bedroom_fc = os.path.join(gdb_path, \"HI_Parcels_Bedrooms_Bathrooms_Units\")\n",
    "\n",
    "# Check if both datasets exist\n",
    "cesspool_exists = arcpy.Exists(cesspool_fc)\n",
    "bedroom_exists = arcpy.Exists(bedroom_fc)\n",
    "\n",
    "print(f\"Cesspool data exists: {cesspool_exists}\")\n",
    "print(f\"Bedroom data exists: {bedroom_exists}\")\n",
    "print()\n",
    "\n",
    "if cesspool_exists and bedroom_exists:\n",
    "    # Get record counts\n",
    "    cesspool_count = int(arcpy.management.GetCount(cesspool_fc)[0])\n",
    "    bedroom_count = int(arcpy.management.GetCount(bedroom_fc)[0])\n",
    "    \n",
    "    print(f\"Cesspool parcels: {cesspool_count:,}\")\n",
    "    print(f\"Bedroom parcels: {bedroom_count:,}\")\n",
    "    print()\n",
    "    \n",
    "    # Sample TMK formats from both datasets\n",
    "    print(\"--- CESSPOOL DATA TMK SAMPLES ---\")\n",
    "    with arcpy.da.SearchCursor(cesspool_fc, [\"TMK\"]) as cursor:\n",
    "        cesspool_tmks = []\n",
    "        for i, row in enumerate(cursor):\n",
    "            if i < 5:\n",
    "                cesspool_tmks.append(str(row[0]))\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    for i, tmk in enumerate(cesspool_tmks):\n",
    "        print(f\"  {i+1}. TMK: {tmk} (Length: {len(tmk)})\")\n",
    "    \n",
    "    print(\"\\n--- BEDROOM DATA TMK SAMPLES ---\")\n",
    "    with arcpy.da.SearchCursor(bedroom_fc, [\"TMK\", \"SUM_Bedrooms\"]) as cursor:\n",
    "        bedroom_tmks = []\n",
    "        bedroom_counts = []\n",
    "        for i, row in enumerate(cursor):\n",
    "            if i < 5:\n",
    "                bedroom_tmks.append(str(row[0]))\n",
    "                bedroom_counts.append(row[1])\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    for i, (tmk, bedrooms) in enumerate(zip(bedroom_tmks, bedroom_counts)):\n",
    "        print(f\"  {i+1}. TMK: {tmk} (Length: {len(tmk)}) | Bedrooms: {bedrooms}\")\n",
    "    \n",
    "    # Check TMK format compatibility\n",
    "    print(\"\\n--- TMK FORMAT COMPATIBILITY ---\")\n",
    "    cesspool_lengths = [len(tmk) for tmk in cesspool_tmks]\n",
    "    bedroom_lengths = [len(tmk) for tmk in bedroom_tmks]\n",
    "    \n",
    "    print(f\"Cesspool TMK lengths: {set(cesspool_lengths)}\")\n",
    "    print(f\"Bedroom TMK lengths: {set(bedroom_lengths)}\")\n",
    "    \n",
    "    if set(cesspool_lengths) == set(bedroom_lengths):\n",
    "        print(\"‚úÖ TMK formats MATCH - Direct join possible!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  TMK formats DIFFER - Conversion needed\")\n",
    "    \n",
    "    # Test for actual TMK overlap\n",
    "    print(\"\\n--- TMK OVERLAP TEST ---\")\n",
    "    cesspool_tmk_set = set(cesspool_tmks)\n",
    "    bedroom_tmk_set = set(bedroom_tmks)\n",
    "    overlap = cesspool_tmk_set.intersection(bedroom_tmk_set)\n",
    "    \n",
    "    if overlap:\n",
    "        print(f\"‚úÖ Found {len(overlap)} matching TMKs in samples\")\n",
    "        print(f\"Matching TMKs: {list(overlap)}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No matching TMKs found in samples - need broader check\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå One or both datasets not found - check import status\")\n",
    "\n",
    "print(\"\\n=== NEXT STEPS ===\")\n",
    "print(\"If TMK formats match: Ready for spatial join\")\n",
    "print(\"If TMK formats differ: Need conversion script\") \n",
    "print(\"If no overlaps: Check county coverage alignment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DIRECT API DOWNLOAD ===\n",
      "Querying REST API...\n",
      "‚úÖ API Response received\n",
      "Features returned: 2000\n",
      "Sample fields: ['OBJECTID', 'TMK', 'TMK_txt', 'county', 'island', 'cty_tmk', 'GISAcres', 'qpub_link', 'SUM_Bedrooms', 'SUM_Bathrooms', 'COUNT_Units', 'BR_per_AC', 'Cat_BR_per_AC', 'Shape__Area', 'Shape__Length']\n",
      "Sample TMK: 111001001\n",
      "Sample County: Honolulu\n",
      "Sample Bedrooms: 0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import arcpy\n",
    "import os\n",
    "\n",
    "print(\"=== DIRECT API DOWNLOAD ===\")\n",
    "\n",
    "# REST API query URL\n",
    "base_url = \"https://services1.arcgis.com/x4h61KaW16vFs7PM/arcgis/rest/services/Categories_HI_Bedrooms_Per_Ac/FeatureServer/0/query\"\n",
    "\n",
    "# Parameters for getting all data\n",
    "params = {\n",
    "    'where': '1=1',  # Get all records\n",
    "    'outFields': '*',  # All fields\n",
    "    'f': 'json',      # JSON format\n",
    "    'returnGeometry': 'true'\n",
    "}\n",
    "\n",
    "try:\n",
    "    print(\"Querying REST API...\")\n",
    "    response = requests.get(base_url, params=params, timeout=300)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        print(f\"‚úÖ API Response received\")\n",
    "        print(f\"Features returned: {len(data.get('features', []))}\")\n",
    "        \n",
    "        # Show sample feature\n",
    "        if data.get('features'):\n",
    "            sample = data['features'][0]['attributes']\n",
    "            print(f\"Sample fields: {list(sample.keys())}\")\n",
    "            print(f\"Sample TMK: {sample.get('TMK', 'N/A')}\")\n",
    "            print(f\"Sample County: {sample.get('county', 'N/A')}\")\n",
    "            print(f\"Sample Bedrooms: {sample.get('SUM_Bedrooms', 'N/A')}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"‚ùå API Error: {response.status_code}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Request failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DOWNLOADING ALL BEDROOM DATA ===\n",
      "Total records available: 384,209\n",
      "Downloading records 1 to 2000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 2001 to 4000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 4001 to 6000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 6001 to 8000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 8001 to 10000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 10001 to 12000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 12001 to 14000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 14001 to 16000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 16001 to 18000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 18001 to 20000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 20001 to 22000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 22001 to 24000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 24001 to 26000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 26001 to 28000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 28001 to 30000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 30001 to 32000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 32001 to 34000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 34001 to 36000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 36001 to 38000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 38001 to 40000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 40001 to 42000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 42001 to 44000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 44001 to 46000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 46001 to 48000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 48001 to 50000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 50001 to 52000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 52001 to 54000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 54001 to 56000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 56001 to 58000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 58001 to 60000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 60001 to 62000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 62001 to 64000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 64001 to 66000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 66001 to 68000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 68001 to 70000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 70001 to 72000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 72001 to 74000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 74001 to 76000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 76001 to 78000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 78001 to 80000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 80001 to 82000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 82001 to 84000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 84001 to 86000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 86001 to 88000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 88001 to 90000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 90001 to 92000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 92001 to 94000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 94001 to 96000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 96001 to 98000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 98001 to 100000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 100001 to 102000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 102001 to 104000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 104001 to 106000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 106001 to 108000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 108001 to 110000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 110001 to 112000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 112001 to 114000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 114001 to 116000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 116001 to 118000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 118001 to 120000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 120001 to 122000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 122001 to 124000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 124001 to 126000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 126001 to 128000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 128001 to 130000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 130001 to 132000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 132001 to 134000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 134001 to 136000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 136001 to 138000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 138001 to 140000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 140001 to 142000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 142001 to 144000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 144001 to 146000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 146001 to 148000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 148001 to 150000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 150001 to 152000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 152001 to 154000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 154001 to 156000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 156001 to 158000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 158001 to 160000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 160001 to 162000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 162001 to 164000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 164001 to 166000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 166001 to 168000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 168001 to 170000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 170001 to 172000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 172001 to 174000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 174001 to 176000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 176001 to 178000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 178001 to 180000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 180001 to 182000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 182001 to 184000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 184001 to 186000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 186001 to 188000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 188001 to 190000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 190001 to 192000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 192001 to 194000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 194001 to 196000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 196001 to 198000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 198001 to 200000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 200001 to 202000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 202001 to 204000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 204001 to 206000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 206001 to 208000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 208001 to 210000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 210001 to 212000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 212001 to 214000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 214001 to 216000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 216001 to 218000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 218001 to 220000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 220001 to 222000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 222001 to 224000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 224001 to 226000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 226001 to 228000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 228001 to 230000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 230001 to 232000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 232001 to 234000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 234001 to 236000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 236001 to 238000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 238001 to 240000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 240001 to 242000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 242001 to 244000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 244001 to 246000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 246001 to 248000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 248001 to 250000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 250001 to 252000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 252001 to 254000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 254001 to 256000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 256001 to 258000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 258001 to 260000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 260001 to 262000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 262001 to 264000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 264001 to 266000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 266001 to 268000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 268001 to 270000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 270001 to 272000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 272001 to 274000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 274001 to 276000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 276001 to 278000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 278001 to 280000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 280001 to 282000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 282001 to 284000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 284001 to 286000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 286001 to 288000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 288001 to 290000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 290001 to 292000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 292001 to 294000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 294001 to 296000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 296001 to 298000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 298001 to 300000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 300001 to 302000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 302001 to 304000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 304001 to 306000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 306001 to 308000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 308001 to 310000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 310001 to 312000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 312001 to 314000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 314001 to 316000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 316001 to 318000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 318001 to 320000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 320001 to 322000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 322001 to 324000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 324001 to 326000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 326001 to 328000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 328001 to 330000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 330001 to 332000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 332001 to 334000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 334001 to 336000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 336001 to 338000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 338001 to 340000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 340001 to 342000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 342001 to 344000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 344001 to 346000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 346001 to 348000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 348001 to 350000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 350001 to 352000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 352001 to 354000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 354001 to 356000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 356001 to 358000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 358001 to 360000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 360001 to 362000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 362001 to 364000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 364001 to 366000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 366001 to 368000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 368001 to 370000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 370001 to 372000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 372001 to 374000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 374001 to 376000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 376001 to 378000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 378001 to 380000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 380001 to 382000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 382001 to 384000...\n",
      "  ‚úÖ Got 2000 features\n",
      "Downloading records 384001 to 384209...\n",
      "  ‚úÖ Got 209 features\n",
      "\n",
      "‚úÖ Total features downloaded: 384,209\n",
      "\n",
      "Counties found: ['Honolulu']\n",
      "Sample TMKs: [111001001, 111002001, 111002002, 111002004, 111002006, 111002008, 111002012, 111002014, 111002015, 111002016]\n",
      "Bedroom distribution (sample): {}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import arcpy\n",
    "import os\n",
    "\n",
    "print(\"=== DOWNLOADING ALL BEDROOM DATA ===\")\n",
    "\n",
    "base_url = \"https://services1.arcgis.com/x4h61KaW16vFs7PM/arcgis/rest/services/Categories_HI_Bedrooms_Per_Ac/FeatureServer/0/query\"\n",
    "\n",
    "# First, get the total count\n",
    "count_params = {\n",
    "    'where': '1=1',\n",
    "    'returnCountOnly': 'true',\n",
    "    'f': 'json'\n",
    "}\n",
    "\n",
    "response = requests.get(base_url, params=count_params)\n",
    "total_count = response.json().get('count', 0)\n",
    "print(f\"Total records available: {total_count:,}\")\n",
    "\n",
    "# Now download in chunks\n",
    "all_features = []\n",
    "offset = 0\n",
    "chunk_size = 2000\n",
    "\n",
    "while offset < total_count:\n",
    "    print(f\"Downloading records {offset + 1} to {min(offset + chunk_size, total_count)}...\")\n",
    "    \n",
    "    params = {\n",
    "        'where': '1=1',\n",
    "        'outFields': '*',\n",
    "        'f': 'json',\n",
    "        'returnGeometry': 'true',\n",
    "        'resultOffset': offset,\n",
    "        'resultRecordCount': chunk_size\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(base_url, params=params, timeout=300)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            features = data.get('features', [])\n",
    "            all_features.extend(features)\n",
    "            print(f\"  ‚úÖ Got {len(features)} features\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå Error: {response.status_code}\")\n",
    "            break\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Request failed: {str(e)}\")\n",
    "        break\n",
    "    \n",
    "    offset += chunk_size\n",
    "\n",
    "print(f\"\\n‚úÖ Total features downloaded: {len(all_features):,}\")\n",
    "\n",
    "# Analyze the complete dataset\n",
    "if all_features:\n",
    "    counties = set()\n",
    "    bedroom_counts = {}\n",
    "    tmk_samples = []\n",
    "    \n",
    "    for feature in all_features[:10]:  # Sample first 10\n",
    "        attrs = feature['attributes']\n",
    "        counties.add(attrs.get('county'))\n",
    "        tmk_samples.append(attrs.get('TMK'))\n",
    "        \n",
    "        bedrooms = attrs.get('SUM_Bedrooms', 0)\n",
    "        if bedrooms > 0:\n",
    "            bedroom_counts[bedrooms] = bedroom_counts.get(bedrooms, 0) + 1\n",
    "    \n",
    "    print(f\"\\nCounties found: {sorted(counties)}\")\n",
    "    print(f\"Sample TMKs: {tmk_samples}\")\n",
    "    print(\"Bedroom distribution (sample):\", dict(list(bedroom_counts.items())[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CREATING GEODATABASE FEATURE CLASS ===\n",
      "Deleted existing feature class\n",
      "‚úÖ Created feature class: HI_Parcels_Bedrooms_Bathrooms_Units\n",
      "‚úÖ Added all required fields\n",
      "\n",
      "=== POPULATING FEATURE CLASS ===\n",
      "This may take several minutes for 384k records...\n",
      "  Inserted 10,000 records...\n",
      "  Inserted 20,000 records...\n",
      "  Inserted 30,000 records...\n",
      "  Inserted 40,000 records...\n",
      "  Inserted 50,000 records...\n",
      "  Inserted 60,000 records...\n",
      "  Inserted 70,000 records...\n",
      "  Inserted 80,000 records...\n",
      "  Inserted 90,000 records...\n",
      "  Inserted 100,000 records...\n",
      "  Inserted 110,000 records...\n",
      "  Inserted 120,000 records...\n",
      "  Inserted 130,000 records...\n",
      "  Inserted 140,000 records...\n",
      "  Inserted 150,000 records...\n",
      "  Inserted 160,000 records...\n",
      "  Inserted 170,000 records...\n",
      "  Inserted 180,000 records...\n",
      "  Inserted 190,000 records...\n",
      "  Error inserting record 191439: 'geometry'\n",
      "  Inserted 200,000 records...\n",
      "  Inserted 210,000 records...\n",
      "  Inserted 220,000 records...\n",
      "  Inserted 230,000 records...\n",
      "  Inserted 240,000 records...\n",
      "  Inserted 250,000 records...\n",
      "  Inserted 260,000 records...\n",
      "  Inserted 270,000 records...\n",
      "  Inserted 280,000 records...\n",
      "  Inserted 290,000 records...\n",
      "  Inserted 300,000 records...\n",
      "  Inserted 310,000 records...\n",
      "  Inserted 320,000 records...\n",
      "  Inserted 330,000 records...\n",
      "  Inserted 340,000 records...\n",
      "  Inserted 350,000 records...\n",
      "  Inserted 360,000 records...\n",
      "  Inserted 370,000 records...\n",
      "  Inserted 380,000 records...\n",
      "‚úÖ Successfully populated 384,208 records\n",
      "‚úÖ Final record count in geodatabase: 384,208\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== CREATING GEODATABASE FEATURE CLASS ===\")\n",
    "\n",
    "gdb_path = r\"C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\ParcelAnalysis.gdb\"\n",
    "fc_name = \"HI_Parcels_Bedrooms_Bathrooms_Units\"\n",
    "fc_path = os.path.join(gdb_path, fc_name)\n",
    "\n",
    "# Create the feature class with proper schema\n",
    "try:\n",
    "    # Delete if exists\n",
    "    if arcpy.Exists(fc_path):\n",
    "        arcpy.management.Delete(fc_path)\n",
    "        print(\"Deleted existing feature class\")\n",
    "    \n",
    "    # Create new feature class\n",
    "    arcpy.management.CreateFeatureclass(\n",
    "        gdb_path, \n",
    "        fc_name, \n",
    "        \"POLYGON\", \n",
    "        spatial_reference=arcpy.SpatialReference(4326)  # WGS84\n",
    "    )\n",
    "    \n",
    "    # Add fields based on the API response\n",
    "    fields_to_add = [\n",
    "        (\"TMK\", \"TEXT\", \"\", \"\", 20),\n",
    "        (\"county\", \"TEXT\", \"\", \"\", 20),\n",
    "        (\"island\", \"TEXT\", \"\", \"\", 10),\n",
    "        (\"GISAcres\", \"DOUBLE\", \"\", \"\", \"\"),\n",
    "        (\"SUM_Bedrooms\", \"LONG\", \"\", \"\", \"\"),\n",
    "        (\"SUM_Bathrooms\", \"LONG\", \"\", \"\", \"\"),\n",
    "        (\"COUNT_Units\", \"LONG\", \"\", \"\", \"\"),\n",
    "        (\"BR_per_AC\", \"DOUBLE\", \"\", \"\", \"\")\n",
    "    ]\n",
    "    \n",
    "    for field_name, field_type, field_precision, field_scale, field_length in fields_to_add:\n",
    "        arcpy.management.AddField(fc_path, field_name, field_type, field_precision, field_scale, field_length)\n",
    "    \n",
    "    print(f\"‚úÖ Created feature class: {fc_name}\")\n",
    "    print(\"‚úÖ Added all required fields\")\n",
    "    \n",
    "    # Now populate with the downloaded data\n",
    "    print(\"\\n=== POPULATING FEATURE CLASS ===\")\n",
    "    print(\"This may take several minutes for 384k records...\")\n",
    "    \n",
    "    with arcpy.da.InsertCursor(fc_path, \n",
    "        [\"SHAPE@\", \"TMK\", \"county\", \"island\", \"GISAcres\", \"SUM_Bedrooms\", \"SUM_Bathrooms\", \"COUNT_Units\", \"BR_per_AC\"]) as cursor:\n",
    "        \n",
    "        count = 0\n",
    "        for feature in all_features:\n",
    "            try:\n",
    "                attrs = feature['attributes']\n",
    "                geom = feature['geometry']\n",
    "                \n",
    "                # Create polygon from geometry\n",
    "                if geom and geom.get('rings'):\n",
    "                    polygon = arcpy.Polygon(\n",
    "                        arcpy.Array([arcpy.Point(x, y) for x, y in geom['rings'][0]]),\n",
    "                        arcpy.SpatialReference(4326)\n",
    "                    )\n",
    "                    \n",
    "                    # Insert row\n",
    "                    cursor.insertRow([\n",
    "                        polygon,\n",
    "                        str(attrs.get('TMK', '')),\n",
    "                        attrs.get('county', ''),\n",
    "                        attrs.get('island', ''),\n",
    "                        attrs.get('GISAcres', 0),\n",
    "                        attrs.get('SUM_Bedrooms', 0),\n",
    "                        attrs.get('SUM_Bathrooms', 0),\n",
    "                        attrs.get('COUNT_Units', 0),\n",
    "                        attrs.get('BR_per_AC', 0)\n",
    "                    ])\n",
    "                    \n",
    "                    count += 1\n",
    "                    if count % 10000 == 0:\n",
    "                        print(f\"  Inserted {count:,} records...\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"  Error inserting record {count}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"‚úÖ Successfully populated {count:,} records\")\n",
    "    \n",
    "    # Verify the result\n",
    "    final_count = int(arcpy.management.GetCount(fc_path)[0])\n",
    "    print(f\"‚úÖ Final record count in geodatabase: {final_count:,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CHECKING FOUNDATION FOLDER ===\n",
      "Foundation folder: C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\Outputs\\foundation\n",
      "Folder exists: True\n",
      "\n",
      "Contents of foundation folder:\n",
      " 1. Cesspool_Parcels_With_Attributes.cpg (0.0 MB)\n",
      " 2. Cesspool_Parcels_With_Attributes.dbf (85.6 MB)\n",
      " 3. Cesspool_Parcels_With_Attributes.prj (0.0 MB)\n",
      " 4. Cesspool_Parcels_With_Attributes.sbn (0.6 MB)\n",
      " 5. Cesspool_Parcels_With_Attributes.sbx (0.0 MB)\n",
      " 6. Cesspool_Parcels_With_Attributes.shp (16.1 MB)\n",
      " 7. Cesspool_Parcels_With_Attributes.shp.LEGIONOFBLOOM.14060.640.sr.lock (0.0 MB)\n",
      " 8. Cesspool_Parcels_With_Attributes.shp.LEGIONOFBLOOM.14096.640.sr.lock (0.0 MB)\n",
      " 9. Cesspool_Parcels_With_Attributes.shp.LEGIONOFBLOOM.20020.640.sr.lock (0.0 MB)\n",
      "10. Cesspool_Parcels_With_Attributes.shp.LEGIONOFBLOOM.22728.640.sr.lock (0.0 MB)\n",
      "11. Cesspool_Parcels_With_Attributes.shp.xml (0.0 MB)\n",
      "12. Cesspool_Parcels_With_Attributes.shx (0.5 MB)\n",
      "13. Cesspool_Parcel_Polygons_Spatial.cpg (0.0 MB)\n",
      "14. Cesspool_Parcel_Polygons_Spatial.dbf (16.5 MB)\n",
      "15. Cesspool_Parcel_Polygons_Spatial.prj (0.0 MB)\n",
      "16. Cesspool_Parcel_Polygons_Spatial.sbn (0.6 MB)\n",
      "17. Cesspool_Parcel_Polygons_Spatial.sbx (0.0 MB)\n",
      "18. Cesspool_Parcel_Polygons_Spatial.shp (16.1 MB)\n",
      "19. Cesspool_Parcel_Polygons_Spatial.shp.xml (0.0 MB)\n",
      "20. Cesspool_Parcel_Polygons_Spatial.shx (0.5 MB)\n",
      "21. Maui_Cesspool_Parcels_Final.cpg (0.0 MB)\n",
      "22. Maui_Cesspool_Parcels_Final.dbf (0.0 MB)\n",
      "23. Maui_Cesspool_Parcels_Final.prj (0.0 MB)\n",
      "24. Maui_Cesspool_Parcels_Final.shp (0.0 MB)\n",
      "25. Maui_Cesspool_Parcels_Final.shp.xml (0.0 MB)\n",
      "26. Maui_Cesspool_Parcels_Final.shx (0.0 MB)\n",
      "27. Maui_Cesspool_Parcels_Pilot.cpg (0.0 MB)\n",
      "28. Maui_Cesspool_Parcels_Pilot.dbf (10.3 MB)\n",
      "29. Maui_Cesspool_Parcels_Pilot.prj (0.0 MB)\n",
      "30. Maui_Cesspool_Parcels_Pilot.sbn (0.1 MB)\n",
      "31. Maui_Cesspool_Parcels_Pilot.sbx (0.0 MB)\n",
      "32. Maui_Cesspool_Parcels_Pilot.shp (2.5 MB)\n",
      "33. Maui_Cesspool_Parcels_Pilot.shp.LEGIONOFBLOOM.14060.640.sr.lock (0.0 MB)\n",
      "34. Maui_Cesspool_Parcels_Pilot.shp.LEGIONOFBLOOM.14096.640.sr.lock (0.0 MB)\n",
      "35. Maui_Cesspool_Parcels_Pilot.shp.LEGIONOFBLOOM.20020.640.sr.lock (0.0 MB)\n",
      "36. Maui_Cesspool_Parcels_Pilot.shp.LEGIONOFBLOOM.22728.640.sr.lock (0.0 MB)\n",
      "37. Maui_Cesspool_Parcels_Pilot.shp.xml (0.0 MB)\n",
      "38. Maui_Cesspool_Parcels_Pilot.shx (0.1 MB)\n",
      "39. TMK_Master_Attributes.cpg (0.0 MB)\n",
      "40. TMK_Master_Attributes.dbf (61.6 MB)\n",
      "41. TMK_Master_Attributes.prj (0.0 MB)\n",
      "42. TMK_Master_Attributes.sbn (0.7 MB)\n",
      "43. TMK_Master_Attributes.sbx (0.0 MB)\n",
      "44. TMK_Master_Attributes.shp (2.2 MB)\n",
      "45. TMK_Master_Attributes.shp.LEGIONOFBLOOM.14060.640.sr.lock (0.0 MB)\n",
      "46. TMK_Master_Attributes.shp.LEGIONOFBLOOM.14096.640.sr.lock (0.0 MB)\n",
      "47. TMK_Master_Attributes.shp.LEGIONOFBLOOM.20020.640.sr.lock (0.0 MB)\n",
      "48. TMK_Master_Attributes.shp.LEGIONOFBLOOM.22728.640.sr.lock (0.0 MB)\n",
      "49. TMK_Master_Attributes.shp.xml (0.0 MB)\n",
      "50. TMK_Master_Attributes.shx (0.6 MB)\n",
      "\n",
      "--- SHAPEFILES FOUND ---\n",
      "‚úì Cesspool_Parcels_With_Attributes.shp (68,565 records)\n",
      "  Checking TMK format in Cesspool_Parcels_With_Attributes.shp...\n",
      "  Fields: FID, Shape, Join_Count, TARGET_FID, TMK, TMK_txt, county, division, island, zone...\n",
      "  TMK fields found: ['TMK', 'TMK_txt', 'cty_tmk', 'TMK_1']\n",
      "  Sample TMKs: ['389014053', '389014054', '389014055', '389014056', '388009127']\n",
      "  TMK length: 9 digits\n",
      "\n",
      "--- BEDROOM DATA TMK FORMAT (FOR COMPARISON) ---\n",
      "Bedroom data sample TMKs: ['111001001', '111002001', '111002002']\n",
      "Bedroom TMK length: 9 digits\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import arcpy\n",
    "\n",
    "print(\"=== CHECKING FOUNDATION FOLDER ===\")\n",
    "\n",
    "foundation_folder = r\"C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\Outputs\\foundation\"\n",
    "\n",
    "print(f\"Foundation folder: {foundation_folder}\")\n",
    "print(f\"Folder exists: {os.path.exists(foundation_folder)}\")\n",
    "\n",
    "if os.path.exists(foundation_folder):\n",
    "    print(\"\\nContents of foundation folder:\")\n",
    "    \n",
    "    files = os.listdir(foundation_folder)\n",
    "    for i, file in enumerate(files, 1):\n",
    "        file_path = os.path.join(foundation_folder, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            size_mb = os.path.getsize(file_path) / (1024*1024)\n",
    "            print(f\"{i:2}. {file} ({size_mb:.1f} MB)\")\n",
    "    \n",
    "    # Look for shapefiles or feature classes\n",
    "    shapefiles = [f for f in files if f.endswith('.shp')]\n",
    "    if shapefiles:\n",
    "        print(f\"\\n--- SHAPEFILES FOUND ---\")\n",
    "        for shp in shapefiles:\n",
    "            shp_path = os.path.join(foundation_folder, shp)\n",
    "            try:\n",
    "                count = int(arcpy.management.GetCount(shp_path)[0])\n",
    "                print(f\"‚úì {shp} ({count:,} records)\")\n",
    "                \n",
    "                # Sample the data\n",
    "                print(f\"  Checking TMK format in {shp}...\")\n",
    "                with arcpy.da.SearchCursor(shp_path, [\"*\"]) as cursor:\n",
    "                    fields = cursor.fields\n",
    "                    print(f\"  Fields: {', '.join(fields[:10])}{'...' if len(fields) > 10 else ''}\")\n",
    "                    \n",
    "                    # Look for TMK field and sample data\n",
    "                    tmk_fields = [f for f in fields if 'tmk' in f.lower()]\n",
    "                    if tmk_fields:\n",
    "                        print(f\"  TMK fields found: {tmk_fields}\")\n",
    "                        \n",
    "                        with arcpy.da.SearchCursor(shp_path, tmk_fields[:1]) as tmk_cursor:\n",
    "                            sample_tmks = []\n",
    "                            for i, row in enumerate(tmk_cursor):\n",
    "                                if i < 5:\n",
    "                                    sample_tmks.append(str(row[0]))\n",
    "                                else:\n",
    "                                    break\n",
    "                            \n",
    "                            print(f\"  Sample TMKs: {sample_tmks}\")\n",
    "                            if sample_tmks:\n",
    "                                print(f\"  TMK length: {len(sample_tmks[0])} digits\")\n",
    "                    break  # Just check first shapefile for now\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  Error reading {shp}: {str(e)}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"No shapefiles found in foundation folder\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Foundation folder not found\")\n",
    "\n",
    "# Compare with bedroom data TMK format\n",
    "print(f\"\\n--- BEDROOM DATA TMK FORMAT (FOR COMPARISON) ---\")\n",
    "gdb_path = r\"C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\ParcelAnalysis.gdb\"\n",
    "bedroom_fc = os.path.join(gdb_path, \"HI_Parcels_Bedrooms_Bathrooms_Units\")\n",
    "\n",
    "with arcpy.da.SearchCursor(bedroom_fc, [\"TMK\"]) as cursor:\n",
    "    bedroom_tmks = []\n",
    "    for i, row in enumerate(cursor):\n",
    "        if i < 3:\n",
    "            bedroom_tmks.append(str(row[0]))\n",
    "        else:\n",
    "            break\n",
    "\n",
    "print(f\"Bedroom data sample TMKs: {bedroom_tmks}\")\n",
    "print(f\"Bedroom TMK length: {len(bedroom_tmks[0])} digits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CREATING MASTER ANALYSIS TABLE ===\n",
      "Joining cesspool parcels with bedroom data...\n",
      "‚ùå Join failed: name 'cesspool_shp' is not defined\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n=== CREATING MASTER ANALYSIS TABLE ===\")\n",
    "print(\"Joining cesspool parcels with bedroom data...\")\n",
    "\n",
    "# Output path for joined data\n",
    "output_fc = os.path.join(gdb_path, \"Cesspool_Parcels_With_Bedrooms\")\n",
    "\n",
    "try:\n",
    "    # Perform spatial join based on TMK\n",
    "    arcpy.management.JoinField(\n",
    "        cesspool_shp,           # Target (cesspool data)\n",
    "        \"TMK\",                  # Target join field  \n",
    "        bedroom_fc,             # Join table (bedroom data)\n",
    "        \"TMK\",                  # Join field\n",
    "        [\"SUM_Bedrooms\", \"SUM_Bathrooms\", \"COUNT_Units\", \"GISAcres\", \"BR_per_AC\"]  # Fields to transfer\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Join completed!\")\n",
    "    \n",
    "    # Copy result to geodatabase\n",
    "    arcpy.management.CopyFeatures(cesspool_shp, output_fc)\n",
    "    \n",
    "    # Verify results\n",
    "    result_count = int(arcpy.management.GetCount(output_fc)[0])\n",
    "    print(f\"‚úÖ Master table created: {result_count:,} records\")\n",
    "    \n",
    "    # Check bedroom data integration\n",
    "    with arcpy.da.SearchCursor(output_fc, [\"TMK\", \"SUM_Bedrooms\", \"SUM_Bathrooms\"]) as cursor:\n",
    "        bedroom_matches = 0\n",
    "        for i, row in enumerate(cursor):\n",
    "            if row[1] is not None and row[1] > 0:\n",
    "                bedroom_matches += 1\n",
    "            if i >= 1000:\n",
    "                break\n",
    "        \n",
    "    print(f\"Sample check: {bedroom_matches}/1000 records have bedroom data\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Join failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CREATING MASTER ANALYSIS TABLE ===\n",
      "Cesspool data: C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\Outputs\\foundation\\Cesspool_Parcels_With_Attributes.shp\n",
      "Bedroom data:  C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\ParcelAnalysis.gdb\\HI_Parcels_Bedrooms_Bathrooms_Units\n",
      "Output:        C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\ParcelAnalysis.gdb\\Cesspool_Parcels_With_Bedrooms\n",
      "\n",
      "Step 1: Copying cesspool data to geodatabase...\n",
      "‚úÖ Copied 68,565 cesspool records\n",
      "Step 2: Joining with bedroom data...\n",
      "‚úÖ Join completed!\n",
      "Step 3: Creating final master table...\n",
      "‚úÖ Master table created: 68,565 records\n",
      "\n",
      "Step 4: Analyzing join results...\n",
      "\n",
      "--- JOIN ANALYSIS RESULTS ---\n",
      "Total cesspool parcels: 68,565\n",
      "With bedroom data: 0 (0.0%)\n",
      "Without bedroom data: 68,565 (100.0%)\n",
      "Counties represented: ['Hawaii', 'Honolulu', 'Kauai', 'Maui']\n",
      "\n",
      "‚ö†Ô∏è WARNING: No bedroom data joined - check TMK compatibility\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import arcpy\n",
    "\n",
    "print(\"=== CREATING MASTER ANALYSIS TABLE ===\")\n",
    "\n",
    "# Define paths\n",
    "foundation_folder = r\"C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\Outputs\\foundation\"\n",
    "cesspool_shp = os.path.join(foundation_folder, \"Cesspool_Parcels_With_Attributes.shp\")\n",
    "gdb_path = r\"C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\ParcelAnalysis.gdb\"\n",
    "bedroom_fc = os.path.join(gdb_path, \"HI_Parcels_Bedrooms_Bathrooms_Units\")\n",
    "output_fc = os.path.join(gdb_path, \"Cesspool_Parcels_With_Bedrooms\")\n",
    "\n",
    "print(f\"Cesspool data: {cesspool_shp}\")\n",
    "print(f\"Bedroom data:  {bedroom_fc}\")\n",
    "print(f\"Output:        {output_fc}\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    # First copy cesspool data to geodatabase\n",
    "    temp_cesspool = os.path.join(gdb_path, \"temp_cesspool_for_join\")\n",
    "    \n",
    "    print(\"Step 1: Copying cesspool data to geodatabase...\")\n",
    "    if arcpy.Exists(temp_cesspool):\n",
    "        arcpy.management.Delete(temp_cesspool)\n",
    "    \n",
    "    arcpy.management.CopyFeatures(cesspool_shp, temp_cesspool)\n",
    "    cesspool_count = int(arcpy.management.GetCount(temp_cesspool)[0])\n",
    "    print(f\"‚úÖ Copied {cesspool_count:,} cesspool records\")\n",
    "    \n",
    "    # Perform the join\n",
    "    print(\"Step 2: Joining with bedroom data...\")\n",
    "    arcpy.management.JoinField(\n",
    "        temp_cesspool,          # Target (cesspool data in GDB)\n",
    "        \"TMK\",                  # Target join field  \n",
    "        bedroom_fc,             # Join table (bedroom data)\n",
    "        \"TMK\",                  # Join field\n",
    "        [\"SUM_Bedrooms\", \"SUM_Bathrooms\", \"COUNT_Units\", \"GISAcres\", \"BR_per_AC\"]\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Join completed!\")\n",
    "    \n",
    "    # Copy to final output\n",
    "    print(\"Step 3: Creating final master table...\")\n",
    "    if arcpy.Exists(output_fc):\n",
    "        arcpy.management.Delete(output_fc)\n",
    "    \n",
    "    arcpy.management.CopyFeatures(temp_cesspool, output_fc)\n",
    "    \n",
    "    # Clean up temp data\n",
    "    arcpy.management.Delete(temp_cesspool)\n",
    "    \n",
    "    # Verify results\n",
    "    result_count = int(arcpy.management.GetCount(output_fc)[0])\n",
    "    print(f\"‚úÖ Master table created: {result_count:,} records\")\n",
    "    \n",
    "    # Analyze the join results\n",
    "    print(\"\\nStep 4: Analyzing join results...\")\n",
    "    with arcpy.da.SearchCursor(output_fc, [\"TMK\", \"SUM_Bedrooms\", \"SUM_Bathrooms\", \"county\"]) as cursor:\n",
    "        total_checked = 0\n",
    "        with_bedrooms = 0\n",
    "        without_bedrooms = 0\n",
    "        bedroom_distribution = {}\n",
    "        counties = set()\n",
    "        \n",
    "        for row in cursor:\n",
    "            total_checked += 1\n",
    "            counties.add(row[3])\n",
    "            \n",
    "            if row[1] is not None and row[1] > 0:\n",
    "                with_bedrooms += 1\n",
    "                bedrooms = int(row[1])\n",
    "                bedroom_distribution[bedrooms] = bedroom_distribution.get(bedrooms, 0) + 1\n",
    "            else:\n",
    "                without_bedrooms += 1\n",
    "    \n",
    "    print(f\"\\n--- JOIN ANALYSIS RESULTS ---\")\n",
    "    print(f\"Total cesspool parcels: {total_checked:,}\")\n",
    "    print(f\"With bedroom data: {with_bedrooms:,} ({with_bedrooms/total_checked*100:.1f}%)\")\n",
    "    print(f\"Without bedroom data: {without_bedrooms:,} ({without_bedrooms/total_checked*100:.1f}%)\")\n",
    "    print(f\"Counties represented: {sorted(counties)}\")\n",
    "    \n",
    "    if bedroom_distribution:\n",
    "        print(f\"\\nBedroom distribution (top 10):\")\n",
    "        sorted_bedrooms = sorted(bedroom_distribution.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        for bedrooms, count in sorted_bedrooms:\n",
    "            print(f\"  {bedrooms} bedrooms: {count:,} parcels\")\n",
    "    \n",
    "    if with_bedrooms > 0:\n",
    "        print(f\"\\n‚úÖ SUCCESS! Master table ready for Matrix analysis\")\n",
    "        print(f\"Dataset: {output_fc}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è WARNING: No bedroom data joined - check TMK compatibility\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {str(e)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DETAILED TMK FORMAT INVESTIGATION ===\n",
      "Collecting TMK samples from both datasets...\n",
      "--- CESSPOOL TMK SAMPLES BY COUNTY ---\n",
      "Hawaii: ['389014053', '389014054', '389014055', '389014056', '388009127']\n",
      "Maui: ['211003003', '211003012', '211003030', '211003031', '211003041']\n",
      "Kauai: ['412006014', '412006041', '412008999', '412010002', '412010003']\n",
      "Honolulu: ['111044013', '111065040', '111065041', '111065042', '111065046']\n",
      "\n",
      "--- BEDROOM TMK SAMPLES BY COUNTY ---\n",
      "Honolulu: ['111001001', '111002001', '111002002', '111002004', '111002006']\n",
      "Maui: ['211001001', '211001002', '211001003', '211001004', '211001005']\n",
      "Hawaii: ['311001001', '311001002', '311001003', '311001004', '311001006']\n",
      "Kauai: ['411001001', '411001002', '411001003', '412001001', '412001003']\n",
      "\n",
      "--- TMK PATTERN ANALYSIS ---\n",
      "\n",
      "Hawaii County:\n",
      "  Cesspool: 389014053\n",
      "  Bedroom:  311001001\n",
      "  Length match: ‚úÖ Both 9 digits\n",
      "  Prefix mismatch: ‚ùå 389 vs 311\n",
      "\n",
      "Honolulu County:\n",
      "  Cesspool: 111044013\n",
      "  Bedroom:  111001001\n",
      "  Length match: ‚úÖ Both 9 digits\n",
      "  Prefix match: ‚úÖ Both start with 111\n",
      "\n",
      "Kauai County:\n",
      "  Cesspool: 412006014\n",
      "  Bedroom:  411001001\n",
      "  Length match: ‚úÖ Both 9 digits\n",
      "  Prefix mismatch: ‚ùå 412 vs 411\n",
      "\n",
      "Maui County:\n",
      "  Cesspool: 211003003\n",
      "  Bedroom:  211001001\n",
      "  Length match: ‚úÖ Both 9 digits\n",
      "  Prefix match: ‚úÖ Both start with 211\n",
      "\n",
      "--- DIRECT TMK OVERLAP TEST ---\n",
      "Building TMK sets (this may take a moment)...\n",
      "Cesspool TMKs: 68,563 unique values\n",
      "Bedroom TMK sample: 49,997 values\n",
      "Direct matches: 843\n",
      "Sample matches: ['131038010', '117037036', '134021024', '125005009', '128020037', '129037079', '125002028', '125010003', '128027036', '131037002']\n"
     ]
    }
   ],
   "source": [
    "print(\"=== DETAILED TMK FORMAT INVESTIGATION ===\")\n",
    "\n",
    "# Paths\n",
    "foundation_folder = r\"C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\Outputs\\foundation\"\n",
    "cesspool_shp = os.path.join(foundation_folder, \"Cesspool_Parcels_With_Attributes.shp\")\n",
    "gdb_path = r\"C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\ParcelAnalysis.gdb\"\n",
    "bedroom_fc = os.path.join(gdb_path, \"HI_Parcels_Bedrooms_Bathrooms_Units\")\n",
    "\n",
    "print(\"Collecting TMK samples from both datasets...\")\n",
    "\n",
    "# Get cesspool TMKs by county\n",
    "cesspool_tmks_by_county = {}\n",
    "with arcpy.da.SearchCursor(cesspool_shp, [\"TMK\", \"county\"]) as cursor:\n",
    "    for i, row in enumerate(cursor):\n",
    "        county = row[1]\n",
    "        tmk = str(row[0])\n",
    "        if county not in cesspool_tmks_by_county:\n",
    "            cesspool_tmks_by_county[county] = []\n",
    "        if len(cesspool_tmks_by_county[county]) < 5:\n",
    "            cesspool_tmks_by_county[county].append(tmk)\n",
    "\n",
    "print(\"--- CESSPOOL TMK SAMPLES BY COUNTY ---\")\n",
    "for county, tmks in cesspool_tmks_by_county.items():\n",
    "    print(f\"{county}: {tmks}\")\n",
    "\n",
    "# Get bedroom TMKs by county  \n",
    "bedroom_tmks_by_county = {}\n",
    "with arcpy.da.SearchCursor(bedroom_fc, [\"TMK\", \"county\"]) as cursor:\n",
    "    for i, row in enumerate(cursor):\n",
    "        county = row[1]\n",
    "        tmk = str(row[0])\n",
    "        if county not in bedroom_tmks_by_county:\n",
    "            bedroom_tmks_by_county[county] = []\n",
    "        if len(bedroom_tmks_by_county[county]) < 5:\n",
    "            bedroom_tmks_by_county[county].append(tmk)\n",
    "\n",
    "print(\"\\n--- BEDROOM TMK SAMPLES BY COUNTY ---\")\n",
    "for county, tmks in bedroom_tmks_by_county.items():\n",
    "    print(f\"{county}: {tmks}\")\n",
    "\n",
    "# Compare patterns\n",
    "print(\"\\n--- TMK PATTERN ANALYSIS ---\")\n",
    "for county in ['Hawaii', 'Honolulu', 'Kauai', 'Maui']:\n",
    "    if county in cesspool_tmks_by_county and county in bedroom_tmks_by_county:\n",
    "        cp_sample = cesspool_tmks_by_county[county][0] if cesspool_tmks_by_county[county] else \"None\"\n",
    "        br_sample = bedroom_tmks_by_county[county][0] if bedroom_tmks_by_county[county] else \"None\"\n",
    "        \n",
    "        print(f\"\\n{county} County:\")\n",
    "        print(f\"  Cesspool: {cp_sample}\")\n",
    "        print(f\"  Bedroom:  {br_sample}\")\n",
    "        \n",
    "        if cp_sample != \"None\" and br_sample != \"None\":\n",
    "            # Compare digit patterns\n",
    "            if len(cp_sample) == len(br_sample):\n",
    "                print(f\"  Length match: ‚úÖ Both {len(cp_sample)} digits\")\n",
    "                \n",
    "                # Check if leading digits match (island codes)\n",
    "                cp_prefix = cp_sample[:3] if len(cp_sample) >= 3 else cp_sample[:2]\n",
    "                br_prefix = br_sample[:3] if len(br_sample) >= 3 else br_sample[:2]\n",
    "                \n",
    "                if cp_prefix == br_prefix:\n",
    "                    print(f\"  Prefix match: ‚úÖ Both start with {cp_prefix}\")\n",
    "                else:\n",
    "                    print(f\"  Prefix mismatch: ‚ùå {cp_prefix} vs {br_prefix}\")\n",
    "            else:\n",
    "                print(f\"  Length mismatch: ‚ùå {len(cp_sample)} vs {len(br_sample)} digits\")\n",
    "\n",
    "# Test actual overlaps with full datasets\n",
    "print(f\"\\n--- DIRECT TMK OVERLAP TEST ---\")\n",
    "print(\"Building TMK sets (this may take a moment)...\")\n",
    "\n",
    "cesspool_tmk_set = set()\n",
    "with arcpy.da.SearchCursor(cesspool_shp, [\"TMK\"]) as cursor:\n",
    "    for row in cursor:\n",
    "        cesspool_tmk_set.add(str(row[0]))\n",
    "\n",
    "print(f\"Cesspool TMKs: {len(cesspool_tmk_set):,} unique values\")\n",
    "\n",
    "# Sample bedroom TMKs (test with smaller set first)\n",
    "bedroom_tmk_sample = set()\n",
    "with arcpy.da.SearchCursor(bedroom_fc, [\"TMK\"]) as cursor:\n",
    "    for i, row in enumerate(cursor):\n",
    "        bedroom_tmk_sample.add(str(row[0]))\n",
    "        if i >= 50000:  # Test with 50k sample\n",
    "            break\n",
    "\n",
    "print(f\"Bedroom TMK sample: {len(bedroom_tmk_sample):,} values\")\n",
    "\n",
    "overlap = cesspool_tmk_set.intersection(bedroom_tmk_sample)\n",
    "print(f\"Direct matches: {len(overlap):,}\")\n",
    "\n",
    "if len(overlap) > 0:\n",
    "    print(f\"Sample matches: {list(overlap)[:10]}\")\n",
    "else:\n",
    "    print(\"‚ùå NO DIRECT MATCHES - TMK formats are incompatible\")\n",
    "    print(\"\\nPossible issues:\")\n",
    "    print(\"1. Different TMK numbering systems between datasets\")\n",
    "    print(\"2. Leading zeros missing/added\") \n",
    "    print(\"3. Different island/county coding systems\")\n",
    "    print(\"4. One dataset uses parcel numbers vs full TMKs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TMK CONVERSION AND RE-JOIN ===\n",
      "Creating TMK conversion mapping...\n",
      "Step 1: Adding converted TMK field to bedroom data...\n",
      "‚úÖ TMK conversion completed\n",
      "Step 2: Re-joining with converted TMKs...\n",
      "‚úÖ Re-join completed with converted TMKs!\n",
      "Step 3: Analyzing improved join results...\n",
      "\n",
      "--- IMPROVED JOIN RESULTS ---\n",
      "Total cesspool parcels: 68,565\n",
      "With bedroom data: 0 (0.0%)\n",
      "\n",
      "Results by county:\n",
      "  Hawaii: 0/42,037 (0.0%)\n",
      "  Maui: 0/8,248 (0.0%)\n",
      "  Kauai: 0/10,551 (0.0%)\n",
      "  Honolulu: 0/7,729 (0.0%)\n",
      "\n",
      "‚ö†Ô∏è Still low match rate - may need additional TMK investigation\n"
     ]
    }
   ],
   "source": [
    "print(\"=== TMK CONVERSION AND RE-JOIN ===\")\n",
    "\n",
    "gdb_path = r\"C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\ParcelAnalysis.gdb\"\n",
    "bedroom_fc = os.path.join(gdb_path, \"HI_Parcels_Bedrooms_Bathrooms_Units\")\n",
    "cesspool_master = os.path.join(gdb_path, \"Cesspool_Parcels_With_Bedrooms\")\n",
    "\n",
    "print(\"Creating TMK conversion mapping...\")\n",
    "\n",
    "# Create conversion lookup for problematic counties\n",
    "tmk_conversion = {\n",
    "    # Hawaii County: 389/388 -> 311\n",
    "    '389': '311',\n",
    "    '388': '311', \n",
    "    # Kauai County: 412 -> 411  \n",
    "    '412': '411'\n",
    "    # Honolulu (111) and Maui (211) don't need conversion\n",
    "}\n",
    "\n",
    "print(\"Step 1: Adding converted TMK field to bedroom data...\")\n",
    "\n",
    "# Add a new field for converted TMK\n",
    "converted_tmk_field = \"TMK_converted\"\n",
    "if converted_tmk_field not in [f.name for f in arcpy.ListFields(bedroom_fc)]:\n",
    "    arcpy.management.AddField(bedroom_fc, converted_tmk_field, \"TEXT\", \"\", \"\", 20)\n",
    "\n",
    "# Populate the converted TMK field\n",
    "with arcpy.da.UpdateCursor(bedroom_fc, [\"TMK\", \"county\", converted_tmk_field]) as cursor:\n",
    "    for row in cursor:\n",
    "        original_tmk = str(row[0])\n",
    "        county = row[1]\n",
    "        \n",
    "        # Convert TMK based on county and prefix\n",
    "        if len(original_tmk) >= 3:\n",
    "            prefix = original_tmk[:3]\n",
    "            \n",
    "            # Convert Hawaii County TMKs (311 -> 389)\n",
    "            if county == \"Hawaii\" and prefix == \"311\":\n",
    "                converted_tmk = \"389\" + original_tmk[3:]\n",
    "            # Convert Kauai County TMKs (411 -> 412) \n",
    "            elif county == \"Kauai\" and prefix == \"411\":\n",
    "                converted_tmk = \"412\" + original_tmk[3:]\n",
    "            else:\n",
    "                converted_tmk = original_tmk\n",
    "        else:\n",
    "            converted_tmk = original_tmk\n",
    "            \n",
    "        row[2] = converted_tmk\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "print(\"‚úÖ TMK conversion completed\")\n",
    "\n",
    "print(\"Step 2: Re-joining with converted TMKs...\")\n",
    "\n",
    "# Try the join again with converted TMKs\n",
    "try:\n",
    "    # Clear any existing bedroom fields from cesspool data\n",
    "    cesspool_fields = [f.name for f in arcpy.ListFields(cesspool_master)]\n",
    "    bedroom_fields_to_remove = [f for f in cesspool_fields if f in [\"SUM_Bedrooms\", \"SUM_Bathrooms\", \"COUNT_Units\", \"GISAcres\", \"BR_per_AC\"]]\n",
    "    \n",
    "    for field in bedroom_fields_to_remove:\n",
    "        if field in cesspool_fields:\n",
    "            try:\n",
    "                arcpy.management.DeleteField(cesspool_master, field)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Perform join with converted TMKs\n",
    "    arcpy.management.JoinField(\n",
    "        cesspool_master,        # Target\n",
    "        \"TMK\",                  # Target join field (cesspool TMKs)\n",
    "        bedroom_fc,             # Join table  \n",
    "        \"TMK_converted\",        # Join field (converted bedroom TMKs)\n",
    "        [\"SUM_Bedrooms\", \"SUM_Bathrooms\", \"COUNT_Units\", \"GISAcres\", \"BR_per_AC\"]\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Re-join completed with converted TMKs!\")\n",
    "    \n",
    "    # Analyze results\n",
    "    print(\"Step 3: Analyzing improved join results...\")\n",
    "    \n",
    "    with arcpy.da.SearchCursor(cesspool_master, [\"TMK\", \"SUM_Bedrooms\", \"county\"]) as cursor:\n",
    "        total = 0\n",
    "        with_bedrooms = 0\n",
    "        by_county = {}\n",
    "        \n",
    "        for row in cursor:\n",
    "            total += 1\n",
    "            county = row[2]\n",
    "            \n",
    "            if county not in by_county:\n",
    "                by_county[county] = {\"total\": 0, \"with_bedrooms\": 0}\n",
    "            by_county[county][\"total\"] += 1\n",
    "            \n",
    "            if row[1] is not None and row[1] > 0:\n",
    "                with_bedrooms += 1\n",
    "                by_county[county][\"with_bedrooms\"] += 1\n",
    "    \n",
    "    print(f\"\\n--- IMPROVED JOIN RESULTS ---\")\n",
    "    print(f\"Total cesspool parcels: {total:,}\")\n",
    "    print(f\"With bedroom data: {with_bedrooms:,} ({with_bedrooms/total*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nResults by county:\")\n",
    "    for county, stats in by_county.items():\n",
    "        match_rate = stats[\"with_bedrooms\"]/stats[\"total\"]*100 if stats[\"total\"] > 0 else 0\n",
    "        print(f\"  {county}: {stats['with_bedrooms']:,}/{stats['total']:,} ({match_rate:.1f}%)\")\n",
    "    \n",
    "    if with_bedrooms > (total * 0.3):  # If >30% match rate\n",
    "        print(f\"\\n‚úÖ EXCELLENT! Master table now ready for Matrix analysis\")\n",
    "        print(f\"Dataset: Cesspool_Parcels_With_Bedrooms\")\n",
    "        print(f\"Ready to run technology compatibility assessment!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è Still low match rate - may need additional TMK investigation\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Re-join failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEBUGGING TMK CONVERSION ===\n",
      "Checking what actually got converted...\n",
      "--- CONVERSION RESULTS BY COUNTY ---\n",
      "\n",
      "Honolulu County:\n",
      "  111001001 ‚Üí 111001001\n",
      "  111002001 ‚Üí 111002001\n",
      "  111002002 ‚Üí 111002002\n",
      "\n",
      "Maui County:\n",
      "  211001001 ‚Üí 211001001\n",
      "  211001002 ‚Üí 211001002\n",
      "  211001003 ‚Üí 211001003\n",
      "\n",
      "Hawaii County:\n",
      "  311001001 ‚Üí 389001001\n",
      "  311001002 ‚Üí 389001002\n",
      "  311001003 ‚Üí 389001003\n",
      "\n",
      "Kauai County:\n",
      "  411001001 ‚Üí 412001001\n",
      "  411001002 ‚Üí 412001002\n",
      "  411001003 ‚Üí 412001003\n",
      "\n",
      "=== CREATING DIRECT TMK LOOKUP TABLE ===\n",
      "--- TMK SET SIZES BY COUNTY ---\n",
      "\n",
      "Hawaii:\n",
      "  Cesspool TMKs: 42,035\n",
      "  Bedroom TMKs:  135,369\n",
      "  Direct overlap: 42,025\n",
      "  Sample matches: ['381006131', '381005024', '399007050', '399004016', '366007048']\n",
      "\n",
      "Maui:\n",
      "  Cesspool TMKs: 8,248\n",
      "  Bedroom TMKs:  51,875\n",
      "  Direct overlap: 8,248\n",
      "  Sample matches: ['227006102', '221013062', '224037054', '221014117', '225004084']\n",
      "\n",
      "Kauai:\n",
      "  Cesspool TMKs: 10,551\n",
      "  Bedroom TMKs:  25,068\n",
      "  Direct overlap: 10,550\n",
      "  Sample matches: ['428026054', '414004053', '418015035', '445001020', '427006121']\n",
      "\n",
      "Honolulu:\n",
      "  Cesspool TMKs: 7,729\n",
      "  Bedroom TMKs:  171,826\n",
      "  Direct overlap: 7,726\n",
      "  Sample matches: ['186007023', '134021024', '187019016', '159018013', '168013036']\n",
      "\n",
      "=== CHECKING OTHER TMK FIELDS ===\n",
      "Cesspool TMK fields: ['TMK', 'TMK_txt', 'cty_tmk', 'TMK_1']\n",
      "Checking alternative TMK field values...\n",
      "  Record 1: {'TMK': 389014053, 'TMK_txt': '389014053', 'cty_tmk': '89014053', 'TMK_1': 389014053}\n",
      "  Record 2: {'TMK': 389014054, 'TMK_txt': '389014054', 'cty_tmk': '89014054', 'TMK_1': 389014054}\n",
      "  Record 3: {'TMK': 389014055, 'TMK_txt': '389014055', 'cty_tmk': '89014055', 'TMK_1': 389014055}\n",
      "  Record 4: {'TMK': 389014056, 'TMK_txt': '389014056', 'cty_tmk': '89014056', 'TMK_1': 389014056}\n",
      "  Record 5: {'TMK': 388009127, 'TMK_txt': '388009127', 'cty_tmk': '88009127', 'TMK_1': 388009127}\n",
      "\n",
      "Checking if bedroom data has county-specific TMK fields...\n",
      "Available fields: ['TMK', 'TMK_converted']\n"
     ]
    }
   ],
   "source": [
    "print(\"=== DEBUGGING TMK CONVERSION ===\")\n",
    "\n",
    "gdb_path = r\"C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\ParcelAnalysis.gdb\"\n",
    "bedroom_fc = os.path.join(gdb_path, \"HI_Parcels_Bedrooms_Bathrooms_Units\")\n",
    "\n",
    "print(\"Checking what actually got converted...\")\n",
    "\n",
    "# Check the converted field values\n",
    "with arcpy.da.SearchCursor(bedroom_fc, [\"TMK\", \"county\", \"TMK_converted\"]) as cursor:\n",
    "    conversion_samples = {}\n",
    "    for i, row in enumerate(cursor):\n",
    "        county = row[1]\n",
    "        if county not in conversion_samples:\n",
    "            conversion_samples[county] = []\n",
    "        if len(conversion_samples[county]) < 3:\n",
    "            conversion_samples[county].append({\n",
    "                \"original\": str(row[0]),\n",
    "                \"converted\": str(row[2])\n",
    "            })\n",
    "\n",
    "print(\"--- CONVERSION RESULTS BY COUNTY ---\")\n",
    "for county, samples in conversion_samples.items():\n",
    "    print(f\"\\n{county} County:\")\n",
    "    for sample in samples:\n",
    "        print(f\"  {sample['original']} ‚Üí {sample['converted']}\")\n",
    "\n",
    "# Let's try a direct approach - create a lookup table instead\n",
    "print(\"\\n=== CREATING DIRECT TMK LOOKUP TABLE ===\")\n",
    "\n",
    "foundation_folder = r\"C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\Outputs\\foundation\"\n",
    "cesspool_shp = os.path.join(foundation_folder, \"Cesspool_Parcels_With_Attributes.shp\")\n",
    "\n",
    "# Get all cesspool TMKs by county\n",
    "cesspool_tmks_by_county = {}\n",
    "with arcpy.da.SearchCursor(cesspool_shp, [\"TMK\", \"county\"]) as cursor:\n",
    "    for row in cursor:\n",
    "        county = row[1]\n",
    "        tmk = str(row[0])\n",
    "        if county not in cesspool_tmks_by_county:\n",
    "            cesspool_tmks_by_county[county] = set()\n",
    "        cesspool_tmks_by_county[county].add(tmk)\n",
    "\n",
    "# Get bedroom TMKs by county  \n",
    "bedroom_tmks_by_county = {}\n",
    "with arcpy.da.SearchCursor(bedroom_fc, [\"TMK\", \"county\"]) as cursor:\n",
    "    for row in cursor:\n",
    "        county = row[1] \n",
    "        tmk = str(row[0])\n",
    "        if county not in bedroom_tmks_by_county:\n",
    "            bedroom_tmks_by_county[county] = set()\n",
    "        bedroom_tmks_by_county[county].add(tmk)\n",
    "\n",
    "print(\"--- TMK SET SIZES BY COUNTY ---\")\n",
    "for county in ['Hawaii', 'Maui', 'Kauai', 'Honolulu']:\n",
    "    cp_count = len(cesspool_tmks_by_county.get(county, set()))\n",
    "    br_count = len(bedroom_tmks_by_county.get(county, set()))\n",
    "    \n",
    "    if cp_count > 0 and br_count > 0:\n",
    "        direct_overlap = cesspool_tmks_by_county[county].intersection(bedroom_tmks_by_county[county])\n",
    "        print(f\"\\n{county}:\")\n",
    "        print(f\"  Cesspool TMKs: {cp_count:,}\")\n",
    "        print(f\"  Bedroom TMKs:  {br_count:,}\") \n",
    "        print(f\"  Direct overlap: {len(direct_overlap):,}\")\n",
    "        \n",
    "        if len(direct_overlap) > 0:\n",
    "            print(f\"  Sample matches: {list(direct_overlap)[:5]}\")\n",
    "\n",
    "# Try alternative approach - use the other TMK fields\n",
    "print(f\"\\n=== CHECKING OTHER TMK FIELDS ===\")\n",
    "\n",
    "# Check cesspool data for alternative TMK fields\n",
    "cesspool_fields = [f.name for f in arcpy.ListFields(cesspool_shp)]\n",
    "tmk_fields = [f for f in cesspool_fields if 'tmk' in f.lower()]\n",
    "print(f\"Cesspool TMK fields: {tmk_fields}\")\n",
    "\n",
    "if len(tmk_fields) > 1:\n",
    "    print(\"Checking alternative TMK field values...\")\n",
    "    with arcpy.da.SearchCursor(cesspool_shp, tmk_fields[:4]) as cursor:  # Check first 4 TMK fields\n",
    "        for i, row in enumerate(cursor):\n",
    "            if i < 5:  # Show first 5 records\n",
    "                print(f\"  Record {i+1}: {dict(zip(tmk_fields[:4], row))}\")\n",
    "\n",
    "# Check bedroom data alternative TMK formats\n",
    "print(f\"\\nChecking if bedroom data has county-specific TMK fields...\")\n",
    "bedroom_fields = [f.name for f in arcpy.ListFields(bedroom_fc)]\n",
    "print(f\"Available fields: {[f for f in bedroom_fields if 'tmk' in f.lower() or 'cty' in f.lower()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FIXING THE JOIN WITH CORRECT TMK FIELD ===\n",
      "The direct overlap analysis shows:\n",
      "  Hawaii: 42,025/42,035 matches (99.98%)\n",
      "  Maui: 8,248/8,248 matches (100%)\n",
      "  Kauai: 10,550/10,551 matches (99.99%)\n",
      "  Honolulu: 7,726/7,729 matches (99.96%)\n",
      "Total expected matches: ~68,549 out of 68,563\n",
      "\n",
      "Step 1: Clearing previous join results...\n",
      "  Removed field: SUM_Bedrooms\n",
      "  Removed field: SUM_Bathrooms\n",
      "  Removed field: COUNT_Units\n",
      "  Removed field: GISAcres\n",
      "  Removed field: BR_per_AC\n",
      "Step 2: Performing join with ORIGINAL TMK fields...\n",
      "‚úÖ Join completed with original TMK fields!\n",
      "Step 3: Analyzing final results...\n",
      "\n",
      "--- FINAL JOIN RESULTS ---\n",
      "Total cesspool parcels: 68,565\n",
      "With bedroom data >0: 0 (0.0%)\n",
      "With zero bedrooms: 0 (0.0%)\n",
      "No bedroom data: 68,565\n",
      "\n",
      "Results by county:\n",
      "  Hawaii: 0 positive + 0 zero = 0.0% joined (0.0% with bedrooms)\n",
      "  Maui: 0 positive + 0 zero = 0.0% joined (0.0% with bedrooms)\n",
      "  Kauai: 0 positive + 0 zero = 0.0% joined (0.0% with bedrooms)\n",
      "  Honolulu: 0 positive + 0 zero = 0.0% joined (0.0% with bedrooms)\n",
      "\n",
      "‚ö†Ô∏è Partial success: 0 joined (0.0%)\n"
     ]
    }
   ],
   "source": [
    "print(\"=== FIXING THE JOIN WITH CORRECT TMK FIELD ===\")\n",
    "\n",
    "gdb_path = r\"C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\ParcelAnalysis.gdb\"\n",
    "bedroom_fc = os.path.join(gdb_path, \"HI_Parcels_Bedrooms_Bathrooms_Units\")\n",
    "cesspool_master = os.path.join(gdb_path, \"Cesspool_Parcels_With_Bedrooms\")\n",
    "\n",
    "print(\"The direct overlap analysis shows:\")\n",
    "print(\"  Hawaii: 42,025/42,035 matches (99.98%)\")\n",
    "print(\"  Maui: 8,248/8,248 matches (100%)\")  \n",
    "print(\"  Kauai: 10,550/10,551 matches (99.99%)\")\n",
    "print(\"  Honolulu: 7,726/7,729 matches (99.96%)\")\n",
    "print(\"Total expected matches: ~68,549 out of 68,563\")\n",
    "print()\n",
    "\n",
    "print(\"Step 1: Clearing previous join results...\")\n",
    "\n",
    "# Remove bedroom fields that may exist from failed previous joins\n",
    "cesspool_fields = [f.name for f in arcpy.ListFields(cesspool_master)]\n",
    "bedroom_fields_to_remove = [\"SUM_Bedrooms\", \"SUM_Bathrooms\", \"COUNT_Units\", \"GISAcres\", \"BR_per_AC\"]\n",
    "\n",
    "for field in bedroom_fields_to_remove:\n",
    "    if field in cesspool_fields:\n",
    "        try:\n",
    "            arcpy.management.DeleteField(cesspool_master, field)\n",
    "            print(f\"  Removed field: {field}\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print(\"Step 2: Performing join with ORIGINAL TMK fields...\")\n",
    "\n",
    "try:\n",
    "    # Join using the original TMK field (not the converted one)\n",
    "    arcpy.management.JoinField(\n",
    "        cesspool_master,        # Target (cesspool data)\n",
    "        \"TMK\",                  # Target join field\n",
    "        bedroom_fc,             # Join table (bedroom data)\n",
    "        \"TMK\",                  # Join field - USE ORIGINAL, NOT CONVERTED!\n",
    "        [\"SUM_Bedrooms\", \"SUM_Bathrooms\", \"COUNT_Units\", \"GISAcres\", \"BR_per_AC\"]\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Join completed with original TMK fields!\")\n",
    "    \n",
    "    # Analyze the results\n",
    "    print(\"Step 3: Analyzing final results...\")\n",
    "    \n",
    "    with arcpy.da.SearchCursor(cesspool_master, [\"TMK\", \"SUM_Bedrooms\", \"SUM_Bathrooms\", \"county\"]) as cursor:\n",
    "        total = 0\n",
    "        with_bedrooms = 0\n",
    "        zero_bedrooms = 0\n",
    "        by_county = {}\n",
    "        bedroom_distribution = {}\n",
    "        \n",
    "        for row in cursor:\n",
    "            total += 1\n",
    "            county = row[3]\n",
    "            bedrooms = row[1]\n",
    "            \n",
    "            if county not in by_county:\n",
    "                by_county[county] = {\"total\": 0, \"with_bedrooms\": 0, \"zero_bedrooms\": 0}\n",
    "            by_county[county][\"total\"] += 1\n",
    "            \n",
    "            if bedrooms is not None:\n",
    "                if bedrooms > 0:\n",
    "                    with_bedrooms += 1\n",
    "                    by_county[county][\"with_bedrooms\"] += 1\n",
    "                    bedroom_distribution[bedrooms] = bedroom_distribution.get(bedrooms, 0) + 1\n",
    "                else:\n",
    "                    zero_bedrooms += 1\n",
    "                    by_county[county][\"zero_bedrooms\"] += 1\n",
    "    \n",
    "    print(f\"\\n--- FINAL JOIN RESULTS ---\")\n",
    "    print(f\"Total cesspool parcels: {total:,}\")\n",
    "    print(f\"With bedroom data >0: {with_bedrooms:,} ({with_bedrooms/total*100:.1f}%)\")\n",
    "    print(f\"With zero bedrooms: {zero_bedrooms:,} ({zero_bedrooms/total*100:.1f}%)\")\n",
    "    print(f\"No bedroom data: {total-with_bedrooms-zero_bedrooms:,}\")\n",
    "    \n",
    "    print(f\"\\nResults by county:\")\n",
    "    for county, stats in by_county.items():\n",
    "        match_rate = (stats[\"with_bedrooms\"] + stats[\"zero_bedrooms\"])/stats[\"total\"]*100 if stats[\"total\"] > 0 else 0\n",
    "        positive_rate = stats[\"with_bedrooms\"]/stats[\"total\"]*100 if stats[\"total\"] > 0 else 0\n",
    "        print(f\"  {county}: {stats['with_bedrooms']:,} positive + {stats['zero_bedrooms']:,} zero = {match_rate:.1f}% joined ({positive_rate:.1f}% with bedrooms)\")\n",
    "    \n",
    "    if bedroom_distribution:\n",
    "        print(f\"\\nBedroom distribution (top 10):\")\n",
    "        sorted_bedrooms = sorted(bedroom_distribution.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        for bedrooms, count in sorted_bedrooms:\n",
    "            print(f\"  {bedrooms} bedrooms: {count:,} parcels\")\n",
    "    \n",
    "    total_joined = with_bedrooms + zero_bedrooms\n",
    "    if total_joined > (total * 0.95):  # If >95% joined\n",
    "        print(f\"\\nüéâ EXCELLENT SUCCESS!\")\n",
    "        print(f\"Master table complete: {total_joined:,}/{total:,} parcels joined ({total_joined/total*100:.1f}%)\")\n",
    "        print(f\"Dataset ready: Cesspool_Parcels_With_Bedrooms\")\n",
    "        print(f\"‚úÖ READY FOR TECHNOLOGY MATRIX ANALYSIS!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è Partial success: {total_joined:,} joined ({total_joined/total*100:.1f}%)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Join failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INVESTIGATING TMK DATA TYPES ===\n",
      "--- FIELD TYPE ANALYSIS ---\n",
      "Cesspool TMK: Integer, Length: 4\n",
      "Cesspool TMK_txt: String, Length: 9\n",
      "Cesspool cty_tmk: String, Length: 8\n",
      "Cesspool TMK_1: Integer, Length: 4\n",
      "Bedroom TMK: String, Length: 20\n",
      "Bedroom TMK_converted: String, Length: 20\n",
      "\n",
      "--- TMK VALUE COMPARISON ---\n",
      "Cesspool TMK samples:\n",
      "  389014053 (type: <class 'int'>, length: 9)\n",
      "  389014054 (type: <class 'int'>, length: 9)\n",
      "  389014055 (type: <class 'int'>, length: 9)\n",
      "  389014056 (type: <class 'int'>, length: 9)\n",
      "  388009127 (type: <class 'int'>, length: 9)\n",
      "\n",
      "Bedroom TMK samples:\n",
      "  111001001 (type: <class 'str'>, length: 9)\n",
      "  111002001 (type: <class 'str'>, length: 9)\n",
      "  111002002 (type: <class 'str'>, length: 9)\n",
      "  111002004 (type: <class 'str'>, length: 9)\n",
      "  111002006 (type: <class 'str'>, length: 9)\n",
      "\n",
      "=== MANUAL JOIN APPROACH ===\n",
      "Creating manual join using Python dictionary...\n",
      "Step 1: Building bedroom data lookup...\n",
      "  Processed 50,000 bedroom records...\n",
      "  Processed 100,000 bedroom records...\n",
      "  Processed 150,000 bedroom records...\n",
      "  Processed 200,000 bedroom records...\n",
      "  Processed 250,000 bedroom records...\n",
      "  Processed 300,000 bedroom records...\n",
      "  Processed 350,000 bedroom records...\n",
      "‚úÖ Bedroom lookup created: 384,138 records\n",
      "Step 2: Adding bedroom fields to cesspool data...\n",
      "Step 3: Manually updating cesspool records with bedroom data...\n",
      "  Processed 10,000 records, 10,000 matched...\n",
      "  Processed 20,000 records, 20,000 matched...\n",
      "  Processed 30,000 records, 30,000 matched...\n",
      "  Processed 40,000 records, 40,000 matched...\n",
      "  Processed 50,000 records, 49,990 matched...\n",
      "  Processed 60,000 records, 59,989 matched...\n",
      "\n",
      "--- MANUAL JOIN RESULTS ---\n",
      "Total cesspool records: 68,565\n",
      "Successfully matched: 68,551\n",
      "Match rate: 100.0%\n",
      "\n",
      "üéâ MANUAL JOIN SUCCESS!\n",
      "‚úÖ Master table complete and ready for Matrix analysis!\n",
      "Dataset: Cesspool_Parcels_With_Bedrooms\n"
     ]
    }
   ],
   "source": [
    "print(\"=== INVESTIGATING TMK DATA TYPES ===\")\n",
    "\n",
    "gdb_path = r\"C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\ParcelAnalysis.gdb\"\n",
    "bedroom_fc = os.path.join(gdb_path, \"HI_Parcels_Bedrooms_Bathrooms_Units\")\n",
    "cesspool_master = os.path.join(gdb_path, \"Cesspool_Parcels_With_Bedrooms\")\n",
    "\n",
    "# Check field types\n",
    "print(\"--- FIELD TYPE ANALYSIS ---\")\n",
    "cesspool_fields = arcpy.ListFields(cesspool_master)\n",
    "for field in cesspool_fields:\n",
    "    if 'tmk' in field.name.lower():\n",
    "        print(f\"Cesspool {field.name}: {field.type}, Length: {field.length}\")\n",
    "\n",
    "bedroom_fields = arcpy.ListFields(bedroom_fc)\n",
    "for field in bedroom_fields:\n",
    "    if 'tmk' in field.name.lower():\n",
    "        print(f\"Bedroom {field.name}: {field.type}, Length: {field.length}\")\n",
    "\n",
    "# Check actual values to see if there are format differences\n",
    "print(\"\\n--- TMK VALUE COMPARISON ---\")\n",
    "print(\"Cesspool TMK samples:\")\n",
    "with arcpy.da.SearchCursor(cesspool_master, [\"TMK\"]) as cursor:\n",
    "    for i, row in enumerate(cursor):\n",
    "        if i < 5:\n",
    "            tmk_val = row[0]\n",
    "            print(f\"  {tmk_val} (type: {type(tmk_val)}, length: {len(str(tmk_val))})\")\n",
    "\n",
    "print(\"\\nBedroom TMK samples:\")\n",
    "with arcpy.da.SearchCursor(bedroom_fc, [\"TMK\"]) as cursor:\n",
    "    for i, row in enumerate(cursor):\n",
    "        if i < 5:\n",
    "            tmk_val = row[0]\n",
    "            print(f\"  {tmk_val} (type: {type(tmk_val)}, length: {len(str(tmk_val))})\")\n",
    "\n",
    "print(\"\\n=== MANUAL JOIN APPROACH ===\")\n",
    "print(\"Creating manual join using Python dictionary...\")\n",
    "\n",
    "# Build lookup dictionary from bedroom data\n",
    "print(\"Step 1: Building bedroom data lookup...\")\n",
    "bedroom_lookup = {}\n",
    "with arcpy.da.SearchCursor(bedroom_fc, [\"TMK\", \"SUM_Bedrooms\", \"SUM_Bathrooms\", \"COUNT_Units\", \"GISAcres\", \"BR_per_AC\"]) as cursor:\n",
    "    count = 0\n",
    "    for row in cursor:\n",
    "        tmk = str(row[0])  # Convert to string for consistent matching\n",
    "        bedroom_lookup[tmk] = {\n",
    "            \"SUM_Bedrooms\": row[1],\n",
    "            \"SUM_Bathrooms\": row[2], \n",
    "            \"COUNT_Units\": row[3],\n",
    "            \"GISAcres\": row[4],\n",
    "            \"BR_per_AC\": row[5]\n",
    "        }\n",
    "        count += 1\n",
    "        if count % 50000 == 0:\n",
    "            print(f\"  Processed {count:,} bedroom records...\")\n",
    "\n",
    "print(f\"‚úÖ Bedroom lookup created: {len(bedroom_lookup):,} records\")\n",
    "\n",
    "# Add bedroom fields to cesspool data if they don't exist\n",
    "print(\"Step 2: Adding bedroom fields to cesspool data...\")\n",
    "bedroom_fields_to_add = [\n",
    "    (\"SUM_Bedrooms\", \"LONG\"),\n",
    "    (\"SUM_Bathrooms\", \"LONG\"), \n",
    "    (\"COUNT_Units\", \"LONG\"),\n",
    "    (\"GISAcres\", \"DOUBLE\"),\n",
    "    (\"BR_per_AC\", \"DOUBLE\")\n",
    "]\n",
    "\n",
    "existing_fields = [f.name for f in arcpy.ListFields(cesspool_master)]\n",
    "for field_name, field_type in bedroom_fields_to_add:\n",
    "    if field_name not in existing_fields:\n",
    "        arcpy.management.AddField(cesspool_master, field_name, field_type)\n",
    "        print(f\"  Added field: {field_name}\")\n",
    "\n",
    "# Perform manual update\n",
    "print(\"Step 3: Manually updating cesspool records with bedroom data...\")\n",
    "matched_count = 0\n",
    "total_count = 0\n",
    "\n",
    "with arcpy.da.UpdateCursor(cesspool_master, [\"TMK\", \"SUM_Bedrooms\", \"SUM_Bathrooms\", \"COUNT_Units\", \"GISAcres\", \"BR_per_AC\"]) as cursor:\n",
    "    for row in cursor:\n",
    "        total_count += 1\n",
    "        tmk = str(row[0])  # Convert to string\n",
    "        \n",
    "        if tmk in bedroom_lookup:\n",
    "            # Update with bedroom data\n",
    "            bedroom_data = bedroom_lookup[tmk]\n",
    "            row[1] = bedroom_data[\"SUM_Bedrooms\"]\n",
    "            row[2] = bedroom_data[\"SUM_Bathrooms\"] \n",
    "            row[3] = bedroom_data[\"COUNT_Units\"]\n",
    "            row[4] = bedroom_data[\"GISAcres\"]\n",
    "            row[5] = bedroom_data[\"BR_per_AC\"]\n",
    "            cursor.updateRow(row)\n",
    "            matched_count += 1\n",
    "        \n",
    "        if total_count % 10000 == 0:\n",
    "            print(f\"  Processed {total_count:,} records, {matched_count:,} matched...\")\n",
    "\n",
    "print(f\"\\n--- MANUAL JOIN RESULTS ---\")\n",
    "print(f\"Total cesspool records: {total_count:,}\")\n",
    "print(f\"Successfully matched: {matched_count:,}\")\n",
    "print(f\"Match rate: {matched_count/total_count*100:.1f}%\")\n",
    "\n",
    "if matched_count > (total_count * 0.95):\n",
    "    print(f\"\\nüéâ MANUAL JOIN SUCCESS!\")\n",
    "    print(f\"‚úÖ Master table complete and ready for Matrix analysis!\")\n",
    "    print(f\"Dataset: Cesspool_Parcels_With_Bedrooms\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Lower than expected match rate - investigating further...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MASTER TABLE ANALYSIS ===\n",
      "Analyzing bedroom distribution in cesspool parcels...\n",
      "--- CESSPOOL PARCEL BEDROOM ANALYSIS ---\n",
      "Total cesspool parcels analyzed: 68,565\n",
      "\n",
      "Bedroom distribution:\n",
      "   0 bedrooms:   2,244 parcels (  3.3%)\n",
      "   1 bedrooms:   2,693 parcels (  3.9%)\n",
      "   2 bedrooms:   9,100 parcels ( 13.3%)\n",
      "   3 bedrooms:  33,575 parcels ( 49.0%)\n",
      "   4 bedrooms:  10,577 parcels ( 15.4%)\n",
      "   5 bedrooms:   4,957 parcels (  7.2%)\n",
      "   6 bedrooms:   2,624 parcels (  3.8%)\n",
      "   7 bedrooms:   1,013 parcels (  1.5%)\n",
      "   8 bedrooms:     545 parcels (  0.8%)\n",
      "   9 bedrooms:     300 parcels (  0.4%)\n",
      "  10 bedrooms:     183 parcels (  0.3%)\n",
      "  11 bedrooms:      96 parcels (  0.1%)\n",
      "  12 bedrooms:     117 parcels (  0.2%)\n",
      "  13 bedrooms:      55 parcels (  0.1%)\n",
      "  14 bedrooms:      38 parcels (  0.1%)\n",
      "\n",
      "County breakdown:\n",
      "  Hawaii  : 42,037 parcels, 95.5% with bedrooms, avg 9.52 acres\n",
      "  Maui    : 8,248 parcels, 97.5% with bedrooms, avg 6.56 acres\n",
      "  Kauai   : 10,551 parcels, 99.2% with bedrooms, avg 6.24 acres\n",
      "  Honolulu: 7,729 parcels, 99.1% with bedrooms, avg 2.31 acres\n",
      "\n",
      "--- WASTEWATER FLOW ESTIMATES ---\n",
      "Residential parcels (bedrooms >0): 66,321\n",
      "Total bedrooms: 256,608\n",
      "Average bedrooms per residential parcel: 3.87\n",
      "Total estimated wastewater flow: 51,321,600 gallons/day\n",
      "Average flow per residential parcel: 774 gallons/day\n",
      "\n",
      "üéØ READY FOR TECHNOLOGY MATRIX!\n",
      "Master table contains all data needed for:\n",
      "‚Ä¢ Bedroom-based wastewater flow calculations\n",
      "‚Ä¢ Lot size analysis (GISAcres field)\n",
      "‚Ä¢ County-specific technology assessment\n",
      "‚Ä¢ HAR 11-62 compliance evaluation\n"
     ]
    }
   ],
   "source": [
    "print(\"=== MASTER TABLE ANALYSIS ===\")\n",
    "\n",
    "gdb_path = r\"C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\ParcelAnalysis.gdb\"\n",
    "cesspool_master = os.path.join(gdb_path, \"Cesspool_Parcels_With_Bedrooms\")\n",
    "\n",
    "print(\"Analyzing bedroom distribution in cesspool parcels...\")\n",
    "\n",
    "with arcpy.da.SearchCursor(cesspool_master, [\"TMK\", \"SUM_Bedrooms\", \"SUM_Bathrooms\", \"GISAcres\", \"county\"]) as cursor:\n",
    "    bedroom_stats = {}\n",
    "    county_stats = {}\n",
    "    total_analyzed = 0\n",
    "    \n",
    "    for row in cursor:\n",
    "        total_analyzed += 1\n",
    "        county = row[4]\n",
    "        bedrooms = row[1] if row[1] is not None else 0\n",
    "        acres = row[3] if row[3] is not None else 0\n",
    "        \n",
    "        # County statistics\n",
    "        if county not in county_stats:\n",
    "            county_stats[county] = {\"total\": 0, \"with_bedrooms\": 0, \"avg_acres\": 0}\n",
    "        county_stats[county][\"total\"] += 1\n",
    "        if bedrooms > 0:\n",
    "            county_stats[county][\"with_bedrooms\"] += 1\n",
    "        county_stats[county][\"avg_acres\"] += acres\n",
    "        \n",
    "        # Bedroom distribution\n",
    "        bedroom_stats[bedrooms] = bedroom_stats.get(bedrooms, 0) + 1\n",
    "\n",
    "print(f\"--- CESSPOOL PARCEL BEDROOM ANALYSIS ---\")\n",
    "print(f\"Total cesspool parcels analyzed: {total_analyzed:,}\")\n",
    "\n",
    "print(f\"\\nBedroom distribution:\")\n",
    "sorted_bedrooms = sorted(bedroom_stats.items())[:15]  # Show first 15 categories\n",
    "for bedrooms, count in sorted_bedrooms:\n",
    "    percentage = count/total_analyzed*100\n",
    "    print(f\"  {bedrooms:2} bedrooms: {count:7,} parcels ({percentage:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nCounty breakdown:\")\n",
    "for county, stats in county_stats.items():\n",
    "    avg_acres = stats[\"avg_acres\"]/stats[\"total\"] if stats[\"total\"] > 0 else 0\n",
    "    bedroom_rate = stats[\"with_bedrooms\"]/stats[\"total\"]*100 if stats[\"total\"] > 0 else 0\n",
    "    print(f\"  {county:8}: {stats['total']:5,} parcels, {bedroom_rate:4.1f}% with bedrooms, avg {avg_acres:.2f} acres\")\n",
    "\n",
    "# Calculate wastewater flow estimates\n",
    "print(f\"\\n--- WASTEWATER FLOW ESTIMATES ---\")\n",
    "residential_parcels = sum(count for bedrooms, count in bedroom_stats.items() if bedrooms > 0)\n",
    "total_bedrooms = sum(bedrooms * count for bedrooms, count in bedroom_stats.items() if bedrooms > 0)\n",
    "avg_bedrooms = total_bedrooms / residential_parcels if residential_parcels > 0 else 0\n",
    "\n",
    "print(f\"Residential parcels (bedrooms >0): {residential_parcels:,}\")\n",
    "print(f\"Total bedrooms: {total_bedrooms:,}\")\n",
    "print(f\"Average bedrooms per residential parcel: {avg_bedrooms:.2f}\")\n",
    "\n",
    "# Wastewater flow calculation (200 gal/bedroom/day per HAR 11-62)\n",
    "total_flow_gpd = total_bedrooms * 200\n",
    "print(f\"Total estimated wastewater flow: {total_flow_gpd:,} gallons/day\")\n",
    "print(f\"Average flow per residential parcel: {total_flow_gpd/residential_parcels:.0f} gallons/day\")\n",
    "\n",
    "print(f\"\\nüéØ READY FOR TECHNOLOGY MATRIX!\")\n",
    "print(\"Master table contains all data needed for:\")\n",
    "print(\"‚Ä¢ Bedroom-based wastewater flow calculations\")\n",
    "print(\"‚Ä¢ Lot size analysis (GISAcres field)\")  \n",
    "print(\"‚Ä¢ County-specific technology assessment\")\n",
    "print(\"‚Ä¢ HAR 11-62 compliance evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CHECKING TMK_Master_Attributes STATUS ===\n",
      "Checking: C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\ParcelAnalysis.gdb\\TMK_Master_Attributes\n",
      "Exists in geodatabase: False\n",
      "‚ùå TMK_Master_Attributes not found in geodatabase\n",
      "\n",
      "Checking foundation folder...\n",
      "‚úÖ Found in foundation folder: C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\Outputs\\foundation\\TMK_Master_Attributes.shp\n",
      "Records: 82,141\n",
      "Should we import this to the geodatabase?\n",
      "\n",
      "=== NEXT STEPS ===\n",
      "1. Confirm TMK_Master_Attributes location\n",
      "2. Check if it has bedroom data\n",
      "3. If missing bedroom data, join from Cesspool_Parcels_With_Bedrooms\n",
      "4. Verify all Matrix analysis fields are present\n"
     ]
    }
   ],
   "source": [
    "print(\"=== CHECKING TMK_Master_Attributes STATUS ===\")\n",
    "\n",
    "gdb_path = r\"C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\ParcelAnalysis.gdb\"\n",
    "tmk_master = os.path.join(gdb_path, \"TMK_Master_Attributes\")\n",
    "\n",
    "print(f\"Checking: {tmk_master}\")\n",
    "print(f\"Exists in geodatabase: {arcpy.Exists(tmk_master)}\")\n",
    "\n",
    "if arcpy.Exists(tmk_master):\n",
    "    # Analyze TMK_Master_Attributes\n",
    "    count = int(arcpy.management.GetCount(tmk_master)[0])\n",
    "    print(f\"‚úÖ TMK_Master_Attributes found: {count:,} records\")\n",
    "    \n",
    "    # Check all fields\n",
    "    fields = [f.name for f in arcpy.ListFields(tmk_master)]\n",
    "    print(f\"\\nAll fields in TMK_Master_Attributes ({len(fields)} total):\")\n",
    "    for i, field in enumerate(fields, 1):\n",
    "        print(f\"{i:2}. {field}\")\n",
    "    \n",
    "    # Categorize fields\n",
    "    categories = {\n",
    "        \"TMK/ID\": [f for f in fields if any(x in f.lower() for x in ['tmk', 'objectid', 'fid'])],\n",
    "        \"Location\": [f for f in fields if any(x in f.lower() for x in ['county', 'island', 'zone'])],\n",
    "        \"Wells\": [f for f in fields if any(x in f.lower() for x in ['well', 'distance', 'municipal', 'domestic'])],\n",
    "        \"Bedrooms\": [f for f in fields if any(x in f.lower() for x in ['bedroom', 'bathroom', 'unit'])],\n",
    "        \"Spatial\": [f for f in fields if any(x in f.lower() for x in ['slope', 'soil', 'area', 'acres', 'gis'])],\n",
    "        \"Regulatory\": [f for f in fields if any(x in f.lower() for x in ['sma', 'flood', 'setback', 'regulatory'])]\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n--- FIELD CATEGORIES IN TMK_Master_Attributes ---\")\n",
    "    for category, cat_fields in categories.items():\n",
    "        if cat_fields:\n",
    "            print(f\"{category}: {', '.join(cat_fields)}\")\n",
    "        else:\n",
    "            print(f\"{category}: ‚ùå MISSING\")\n",
    "    \n",
    "    # Sample the data\n",
    "    print(f\"\\n--- SAMPLE DATA ---\")\n",
    "    with arcpy.da.SearchCursor(tmk_master, fields[:10]) as cursor:  # First 10 fields\n",
    "        field_names = fields[:10]\n",
    "        print(f\"Sample fields: {', '.join(field_names)}\")\n",
    "        for i, row in enumerate(cursor):\n",
    "            if i < 3:\n",
    "                print(f\"Record {i+1}: {dict(zip(field_names, row))}\")\n",
    "    \n",
    "    # Check if it needs bedroom data\n",
    "    has_bedrooms = any('bedroom' in f.lower() for f in fields)\n",
    "    if not has_bedrooms:\n",
    "        print(f\"\\n‚ö†Ô∏è TMK_Master_Attributes MISSING bedroom data\")\n",
    "        print(\"Need to join bedroom data from Cesspool_Parcels_With_Bedrooms\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ TMK_Master_Attributes already has bedroom data\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå TMK_Master_Attributes not found in geodatabase\")\n",
    "    print(\"\\nChecking foundation folder...\")\n",
    "    \n",
    "    foundation_folder = r\"C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\Outputs\\foundation\"\n",
    "    tmk_master_shp = os.path.join(foundation_folder, \"TMK_Master_Attributes.shp\")\n",
    "    \n",
    "    if os.path.exists(tmk_master_shp):\n",
    "        print(f\"‚úÖ Found in foundation folder: {tmk_master_shp}\")\n",
    "        \n",
    "        count = int(arcpy.management.GetCount(tmk_master_shp)[0])\n",
    "        print(f\"Records: {count:,}\")\n",
    "        \n",
    "        print(\"Should we import this to the geodatabase?\")\n",
    "    else:\n",
    "        print(\"‚ùå TMK_Master_Attributes not found anywhere\")\n",
    "\n",
    "print(f\"\\n=== NEXT STEPS ===\")\n",
    "print(\"1. Confirm TMK_Master_Attributes location\")\n",
    "print(\"2. Check if it has bedroom data\")\n",
    "print(\"3. If missing bedroom data, join from Cesspool_Parcels_With_Bedrooms\")\n",
    "print(\"4. Verify all Matrix analysis fields are present\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CLEANING UP AND ORGANIZING GEODATABASE ===\n",
      "Importing all foundation outputs and creating complete master table\n",
      "\n",
      "Step 1: Inventory foundation folder shapefiles...\n",
      "Foundation shapefiles found:\n",
      "  1. Cesspool_Parcels_With_Attributes (68,565 records)\n",
      "  2. Cesspool_Parcel_Polygons_Spatial (68,565 records)\n",
      "  3. Maui_Cesspool_Parcels_Final (0 records)\n",
      "  4. Maui_Cesspool_Parcels_Pilot (8,248 records)\n",
      "  5. TMK_Master_Attributes (82,141 records)\n",
      "\n",
      "Step 2: Import key foundation files to geodatabase...\n",
      "\n",
      "Importing TMK_Master_Attributes...\n",
      "  ‚úÖ Imported 82,141 records\n",
      "  Fields: 18 total\n",
      "\n",
      "Importing Cesspool_Parcels_With_Attributes...\n",
      "  ‚úÖ Imported 68,565 records\n",
      "  Fields: 38 total\n",
      "\n",
      "Importing Cesspool_Parcel_Polygons_Spatial...\n",
      "  ‚úÖ Imported 68,565 records\n",
      "  Fields: 20 total\n",
      "\n",
      "Step 3: Analyze imported datasets...\n",
      "\n",
      "--- TMK_Master_Attributes ANALYSIS ---\n",
      "Records: 82,141\n",
      "  TMK/ID: OBJECTID, TMK\n",
      "  Location: Island\n",
      "  Spatial: SLOPE_PCT, SOIL_PERC, SOIL_KSAT, AVAIL_AREA\n",
      "\n",
      "--- Cesspool_Parcels_With_Attributes ANALYSIS ---\n",
      "Records: 68,565\n",
      "  TMK/ID: OBJECTID, TARGET_FID, TMK, TMK_txt, cty_tmk...\n",
      "  Location: county, island, zone, Island_1\n",
      "  Spatial: GISAcres, SLOPE_PCT, SOIL_PERC, SOIL_KSAT, AVAIL_AREA...\n",
      "\n",
      "--- Cesspool_Parcel_Polygons_Spatial ANALYSIS ---\n",
      "Records: 68,565\n",
      "  TMK/ID: OBJECTID, TMK, TMK_txt, cty_tmk\n",
      "  Location: county, island, zone\n",
      "  Spatial: GISAcres, Shape_Area\n",
      "\n",
      "--- BEDROOM DATA STATUS ---\n",
      "Cesspool_Parcels_With_Bedrooms: 68,565 records\n",
      "Bedroom fields: GISAcres_1, SUM_Bedrooms, SUM_Bathrooms, COUNT_Units, GISAcres\n",
      "\n",
      "=== NEXT STEP: CREATE COMPLETE MASTER TABLE ===\n",
      "Options:\n",
      "A. Use TMK_Master_Attributes as base (if it has well distances)\n",
      "B. Use Cesspool_Parcels_With_Bedrooms as base (has bedroom data)\n",
      "C. Create new master by joining all components\n",
      "\n",
      "Which approach should we take?\n"
     ]
    }
   ],
   "source": [
    "print(\"=== CLEANING UP AND ORGANIZING GEODATABASE ===\")\n",
    "print(\"Importing all foundation outputs and creating complete master table\")\n",
    "print()\n",
    "\n",
    "import os\n",
    "import arcpy\n",
    "\n",
    "# Define paths\n",
    "gdb_path = r\"C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\ParcelAnalysis.gdb\"\n",
    "foundation_folder = r\"C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\Outputs\\foundation\"\n",
    "\n",
    "print(\"Step 1: Inventory foundation folder shapefiles...\")\n",
    "\n",
    "# List all shapefiles in foundation folder\n",
    "foundation_files = []\n",
    "if os.path.exists(foundation_folder):\n",
    "    for file in os.listdir(foundation_folder):\n",
    "        if file.endswith('.shp'):\n",
    "            shp_path = os.path.join(foundation_folder, file)\n",
    "            try:\n",
    "                count = int(arcpy.management.GetCount(shp_path)[0])\n",
    "                foundation_files.append({\n",
    "                    'name': file[:-4],  # Remove .shp extension\n",
    "                    'path': shp_path,\n",
    "                    'count': count\n",
    "                })\n",
    "            except:\n",
    "                print(f\"  ‚ö†Ô∏è Could not read {file}\")\n",
    "\n",
    "    print(\"Foundation shapefiles found:\")\n",
    "    for i, file_info in enumerate(foundation_files, 1):\n",
    "        print(f\"  {i}. {file_info['name']} ({file_info['count']:,} records)\")\n",
    "\n",
    "print(f\"\\nStep 2: Import key foundation files to geodatabase...\")\n",
    "\n",
    "# Priority imports - the most important files\n",
    "priority_imports = [\n",
    "    'TMK_Master_Attributes',\n",
    "    'Cesspool_Parcels_With_Attributes', \n",
    "    'Cesspool_Parcel_Polygons_Spatial'\n",
    "]\n",
    "\n",
    "imported_datasets = []\n",
    "for priority in priority_imports:\n",
    "    foundation_file = next((f for f in foundation_files if f['name'] == priority), None)\n",
    "    if foundation_file:\n",
    "        print(f\"\\nImporting {priority}...\")\n",
    "        \n",
    "        source_path = foundation_file['path']\n",
    "        target_path = os.path.join(gdb_path, priority)\n",
    "        \n",
    "        try:\n",
    "            # Remove if already exists\n",
    "            if arcpy.Exists(target_path):\n",
    "                arcpy.management.Delete(target_path)\n",
    "                print(f\"  Removed existing {priority}\")\n",
    "            \n",
    "            # Import to geodatabase\n",
    "            arcpy.management.CopyFeatures(source_path, target_path)\n",
    "            \n",
    "            # Verify\n",
    "            count = int(arcpy.management.GetCount(target_path)[0])\n",
    "            fields = [f.name for f in arcpy.ListFields(target_path)]\n",
    "            \n",
    "            print(f\"  ‚úÖ Imported {count:,} records\")\n",
    "            print(f\"  Fields: {len(fields)} total\")\n",
    "            \n",
    "            imported_datasets.append({\n",
    "                'name': priority,\n",
    "                'path': target_path,\n",
    "                'count': count,\n",
    "                'fields': fields\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Import failed: {str(e)}\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è {priority} not found in foundation folder\")\n",
    "\n",
    "print(f\"\\nStep 3: Analyze imported datasets...\")\n",
    "\n",
    "for dataset in imported_datasets:\n",
    "    print(f\"\\n--- {dataset['name']} ANALYSIS ---\")\n",
    "    print(f\"Records: {dataset['count']:,}\")\n",
    "    \n",
    "    # Categorize fields\n",
    "    fields = dataset['fields']\n",
    "    categories = {\n",
    "        \"TMK/ID\": [f for f in fields if any(x in f.lower() for x in ['tmk', 'objectid', 'fid'])],\n",
    "        \"Location\": [f for f in fields if any(x in f.lower() for x in ['county', 'island', 'zone'])],\n",
    "        \"Wells\": [f for f in fields if any(x in f.lower() for x in ['well', 'distance', 'municipal', 'domestic'])],\n",
    "        \"Bedrooms\": [f for f in fields if any(x in f.lower() for x in ['bedroom', 'bathroom', 'unit'])],\n",
    "        \"Spatial\": [f for f in fields if any(x in f.lower() for x in ['slope', 'soil', 'area', 'acres'])],\n",
    "        \"Matrix\": [f for f in fields if any(x in f.lower() for x in ['matrix', 'compatible', 'suitable'])]\n",
    "    }\n",
    "    \n",
    "    for category, cat_fields in categories.items():\n",
    "        if cat_fields:\n",
    "            print(f\"  {category}: {', '.join(cat_fields[:5])}{'...' if len(cat_fields) > 5 else ''}\")\n",
    "\n",
    "# Now check our bedroom data \n",
    "bedroom_data = os.path.join(gdb_path, \"Cesspool_Parcels_With_Bedrooms\")\n",
    "if arcpy.Exists(bedroom_data):\n",
    "    print(f\"\\n--- BEDROOM DATA STATUS ---\")\n",
    "    count = int(arcpy.management.GetCount(bedroom_data)[0])\n",
    "    print(f\"Cesspool_Parcels_With_Bedrooms: {count:,} records\")\n",
    "    \n",
    "    bedroom_fields = [f.name for f in arcpy.ListFields(bedroom_data)]\n",
    "    bedroom_specific = [f for f in bedroom_fields if any(x in f.lower() for x in ['bedroom', 'bathroom', 'unit', 'gis'])]\n",
    "    print(f\"Bedroom fields: {', '.join(bedroom_specific)}\")\n",
    "\n",
    "print(f\"\\n=== NEXT STEP: CREATE COMPLETE MASTER TABLE ===\")\n",
    "print(\"Options:\")\n",
    "print(\"A. Use TMK_Master_Attributes as base (if it has well distances)\")\n",
    "print(\"B. Use Cesspool_Parcels_With_Bedrooms as base (has bedroom data)\")\n",
    "print(\"C. Create new master by joining all components\")\n",
    "print(\"\\nWhich approach should we take?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CREATING MASTER TABLE WITH FULL SPATIAL COVERAGE ===\n",
      "Base: TMK_Master_Attributes (82,141 records)\n",
      "Join: Cesspool bedroom data (68,565 records)\n",
      "Result: Complete coverage with cesspool data where applicable\n",
      "\n",
      "Step 1: Using TMK_Master_Attributes as comprehensive base...\n",
      "Base table: 82,141 records\n",
      "Base fields: 18 total\n",
      "Base table data categories:\n",
      "  ‚ùå Wells: Missing\n",
      "  ‚úÖ Spatial: SLOPE_PCT, SOIL_PERC, SOIL_KSAT, AVAIL_AREA\n",
      "  ‚úÖ Location: Island\n",
      "\n",
      "Step 2: Creating master table from base...\n",
      "  ‚úÖ Master table created: 82,141 records\n",
      "\n",
      "Step 3: Adding bedroom data to all applicable parcels...\n",
      "  Adding bedroom fields: SUM_Bedrooms, SUM_Bathrooms, COUNT_Units, county\n",
      "  ‚úÖ Bedroom data joined to master table\n",
      "\n",
      "Step 4: Analyzing master table coverage...\n",
      "--- MASTER TABLE ANALYSIS ---\n",
      "Total records in master table: 82,141\n",
      "Cesspool parcels (matched): 79,155 (96.4%)\n",
      "Edge cases (no cesspool data): 2,986 (3.6%)\n",
      "Ôªø\n",
      "Of cesspool parcels:\n",
      "  With bedrooms >0: 78,469 (99.1%)\n",
      "  With zero bedrooms: 686 (0.9%)\n",
      "\n",
      "By county:\n",
      "  Honolulu: 8,065 parcels, 8,050 with bedrooms (99.8%)\n",
      "  Kauai: 14,132 parcels, 14,121 with bedrooms (99.9%)\n",
      "  Maui: 10,658 parcels, 10,619 with bedrooms (99.6%)\n",
      "  Hawaii: 46,300 parcels, 45,679 with bedrooms (98.7%)\n",
      "\n",
      "Final master table: 22 fields total\n",
      "\n",
      "--- COMPLETE MASTER TABLE CAPABILITIES ---\n",
      "‚úÖ TMK/ID (2): OBJECTID, TMK\n",
      "‚úÖ Location (2): Island, county\n",
      "‚ùå Wells (0): MISSING\n",
      "‚úÖ Bedrooms (3): SUM_Bedrooms, SUM_Bathrooms, COUNT_Units\n",
      "‚úÖ Spatial (4): SLOPE_PCT, SOIL_PERC, SOIL_KSAT...\n",
      "\n",
      "üéâ MASTER TABLE SUCCESS!\n",
      "‚úÖ 82,141 total parcels with full spatial coverage\n",
      "‚úÖ 79,155 cesspool parcels with bedroom data\n",
      "‚úÖ 2,986 edge case parcels for comprehensive analysis\n",
      "‚úÖ Ready for complete statewide technology matrix analysis!\n",
      "\n",
      "=== FINAL MASTER TABLE STATUS ===\n",
      "Dataset: Complete_Master_Analysis_Table\n",
      "Coverage: Statewide with cesspool focus\n",
      "Ready for Matrix: ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "print(\"=== CREATING MASTER TABLE WITH FULL SPATIAL COVERAGE ===\")\n",
    "print(\"Base: TMK_Master_Attributes (82,141 records)\")\n",
    "print(\"Join: Cesspool bedroom data (68,565 records)\")\n",
    "print(\"Result: Complete coverage with cesspool data where applicable\")\n",
    "print()\n",
    "\n",
    "gdb_path = r\"C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\ParcelAnalysis.gdb\"\n",
    "\n",
    "# Define data sources\n",
    "base_table = os.path.join(gdb_path, \"TMK_Master_Attributes\")  # 82k records - our base\n",
    "bedroom_source = os.path.join(gdb_path, \"Cesspool_Parcels_With_Bedrooms\")  # 68k records - join data\n",
    "output_master = os.path.join(gdb_path, \"Complete_Master_Analysis_Table\")\n",
    "\n",
    "print(\"Step 1: Using TMK_Master_Attributes as comprehensive base...\")\n",
    "\n",
    "# Check what we have in the base table\n",
    "base_fields = [f.name for f in arcpy.ListFields(base_table)]\n",
    "base_count = int(arcpy.management.GetCount(base_table)[0])\n",
    "\n",
    "print(f\"Base table: {base_count:,} records\")\n",
    "print(f\"Base fields: {len(base_fields)} total\")\n",
    "\n",
    "# Categorize base fields\n",
    "base_categories = {\n",
    "    \"Wells\": [f for f in base_fields if any(x in f.lower() for x in ['well', 'distance', 'municipal', 'domestic'])],\n",
    "    \"Spatial\": [f for f in base_fields if any(x in f.lower() for x in ['slope', 'soil', 'area', 'avail'])],\n",
    "    \"Location\": [f for f in base_fields if any(x in f.lower() for x in ['county', 'island', 'zone'])]\n",
    "}\n",
    "\n",
    "print(\"Base table data categories:\")\n",
    "for category, fields in base_categories.items():\n",
    "    if fields:\n",
    "        print(f\"  ‚úÖ {category}: {', '.join(fields[:4])}{'...' if len(fields) > 4 else ''}\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå {category}: Missing\")\n",
    "\n",
    "print(f\"\\nStep 2: Creating master table from base...\")\n",
    "\n",
    "try:\n",
    "    # Create master table from TMK_Master_Attributes\n",
    "    if arcpy.Exists(output_master):\n",
    "        arcpy.management.Delete(output_master)\n",
    "        print(\"  Removed existing master table\")\n",
    "    \n",
    "    arcpy.management.CopyFeatures(base_table, output_master)\n",
    "    master_count = int(arcpy.management.GetCount(output_master)[0])\n",
    "    print(f\"  ‚úÖ Master table created: {master_count:,} records\")\n",
    "    \n",
    "    print(f\"\\nStep 3: Adding bedroom data to all applicable parcels...\")\n",
    "    \n",
    "    # Join bedroom data where TMKs match\n",
    "    bedroom_fields_to_add = [\"SUM_Bedrooms\", \"SUM_Bathrooms\", \"COUNT_Units\", \"county\"]\n",
    "    \n",
    "    # Check which bedroom fields are available\n",
    "    bedroom_fields = [f.name for f in arcpy.ListFields(bedroom_source)]\n",
    "    available_fields = [f for f in bedroom_fields_to_add if f in bedroom_fields]\n",
    "    \n",
    "    print(f\"  Adding bedroom fields: {', '.join(available_fields)}\")\n",
    "    \n",
    "    arcpy.management.JoinField(\n",
    "        output_master,          # Target (82k records)\n",
    "        \"TMK\",                  # Target join field\n",
    "        bedroom_source,         # Source (68k records with bedroom data)\n",
    "        \"TMK\",                  # Source join field  \n",
    "        available_fields        # Fields to add\n",
    "    )\n",
    "    \n",
    "    print(f\"  ‚úÖ Bedroom data joined to master table\")\n",
    "    \n",
    "    print(f\"\\nStep 4: Analyzing master table coverage...\")\n",
    "    \n",
    "    # Analyze the join results\n",
    "    with arcpy.da.SearchCursor(output_master, [\"TMK\", \"SUM_Bedrooms\", \"county\"]) as cursor:\n",
    "        total_records = 0\n",
    "        with_bedrooms = 0\n",
    "        with_zero_bedrooms = 0\n",
    "        cesspool_parcels = 0  # Records that matched cesspool data\n",
    "        edge_cases = 0  # Records without cesspool data\n",
    "        \n",
    "        county_stats = {}\n",
    "        \n",
    "        for row in cursor:\n",
    "            total_records += 1\n",
    "            bedrooms = row[1]\n",
    "            county = row[2]\n",
    "            \n",
    "            if county is not None:  # Matched with cesspool data\n",
    "                cesspool_parcels += 1\n",
    "                if county not in county_stats:\n",
    "                    county_stats[county] = {\"total\": 0, \"with_bedrooms\": 0}\n",
    "                county_stats[county][\"total\"] += 1\n",
    "                \n",
    "                if bedrooms is not None:\n",
    "                    if bedrooms > 0:\n",
    "                        with_bedrooms += 1\n",
    "                        county_stats[county][\"with_bedrooms\"] += 1\n",
    "                    else:\n",
    "                        with_zero_bedrooms += 1\n",
    "            else:\n",
    "                edge_cases += 1\n",
    "    \n",
    "    print(f\"--- MASTER TABLE ANALYSIS ---\")\n",
    "    print(f\"Total records in master table: {total_records:,}\")\n",
    "    print(f\"Cesspool parcels (matched): {cesspool_parcels:,} ({cesspool_parcels/total_records*100:.1f}%)\")\n",
    "    print(f\"Edge cases (no cesspool data): {edge_cases:,} ({edge_cases/total_records*100:.1f}%)\")\n",
    "    print(f\"\")\n",
    "    print(f\"Of cesspool parcels:\")\n",
    "    print(f\"  With bedrooms >0: {with_bedrooms:,} ({with_bedrooms/cesspool_parcels*100:.1f}%)\")\n",
    "    print(f\"  With zero bedrooms: {with_zero_bedrooms:,} ({with_zero_bedrooms/cesspool_parcels*100:.1f}%)\")\n",
    "    \n",
    "    if county_stats:\n",
    "        print(f\"\\nBy county:\")\n",
    "        for county, stats in county_stats.items():\n",
    "            bedroom_rate = stats[\"with_bedrooms\"]/stats[\"total\"]*100 if stats[\"total\"] > 0 else 0\n",
    "            print(f\"  {county}: {stats['total']:,} parcels, {stats['with_bedrooms']:,} with bedrooms ({bedroom_rate:.1f}%)\")\n",
    "    \n",
    "    # Check final field inventory\n",
    "    final_fields = [f.name for f in arcpy.ListFields(output_master)]\n",
    "    print(f\"\\nFinal master table: {len(final_fields)} fields total\")\n",
    "    \n",
    "    # Final categorization\n",
    "    final_categories = {\n",
    "        \"TMK/ID\": [f for f in final_fields if any(x in f.lower() for x in ['tmk', 'objectid'])],\n",
    "        \"Location\": [f for f in final_fields if any(x in f.lower() for x in ['county', 'island', 'zone'])],\n",
    "        \"Wells\": [f for f in final_fields if any(x in f.lower() for x in ['well', 'distance', 'municipal', 'domestic'])],\n",
    "        \"Bedrooms\": [f for f in final_fields if any(x in f.lower() for x in ['bedroom', 'bathroom', 'unit'])],\n",
    "        \"Spatial\": [f for f in final_fields if any(x in f.lower() for x in ['slope', 'soil', 'area', 'avail'])],\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n--- COMPLETE MASTER TABLE CAPABILITIES ---\")\n",
    "    for category, cat_fields in final_categories.items():\n",
    "        status = \"‚úÖ\" if cat_fields else \"‚ùå\"\n",
    "        field_count = len(cat_fields)\n",
    "        sample_fields = ', '.join(cat_fields[:3]) + ('...' if len(cat_fields) > 3 else '')\n",
    "        print(f\"{status} {category} ({field_count}): {sample_fields if cat_fields else 'MISSING'}\")\n",
    "    \n",
    "    if cesspool_parcels >= 68000 and with_bedrooms > 20000:\n",
    "        print(f\"\\nüéâ MASTER TABLE SUCCESS!\")\n",
    "        print(f\"‚úÖ {total_records:,} total parcels with full spatial coverage\")\n",
    "        print(f\"‚úÖ {cesspool_parcels:,} cesspool parcels with bedroom data\")\n",
    "        print(f\"‚úÖ {edge_cases:,} edge case parcels for comprehensive analysis\")\n",
    "        print(f\"‚úÖ Ready for complete statewide technology matrix analysis!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è Lower coverage than expected - investigate data quality\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Master table creation failed: {str(e)}\")\n",
    "\n",
    "print(f\"\\n=== FINAL MASTER TABLE STATUS ===\")\n",
    "print(f\"Dataset: Complete_Master_Analysis_Table\")\n",
    "print(f\"Coverage: Statewide with cesspool focus\")\n",
    "print(f\"Ready for Matrix: {'‚úÖ' if 'cesspool_parcels' in locals() and cesspool_parcels >= 68000 else '‚ùå'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DIRECT WELLS DATA SEARCH AND FIX ===\n",
      "Searching geodatabase systematically for wells data\n",
      "\n",
      "Step 1: Complete geodatabase inventory...\n",
      "Feature classes (11):\n",
      "   1. Parcels_Hawaii (135,471 records) \n",
      "   2. tmk_state (384,262 records) \n",
      "   3. Parcels_Honolulu (171,905 records) \n",
      "   4. Parcels_Kauai (25,121 records) \n",
      "   5. Parcels_Maui (51,765 records) \n",
      "   6. HI_Parcels_Bedrooms_Bathrooms_Units (384,208 records) \n",
      "   7. Cesspool_Parcels_With_Bedrooms (68,565 records) \n",
      "   8. TMK_Master_Attributes (82,141 records) \n",
      "   9. Cesspool_Parcels_With_Attributes (68,565 records) \n",
      "  10. Cesspool_Parcel_Polygons_Spatial (68,565 records) \n",
      "  11. Complete_Master_Analysis_Table (82,141 records) \n",
      "\n",
      "Tables (7):\n",
      "   1. ParcelsData_Maui_ExportTable (51,765 records) \n",
      "   2. bedrooms_out_ExportTable (70,322 records) \n",
      "   3. ParcelsData_Maui_ExportTable1 (51,765 records) \n",
      "   4. ParcelsData_Maui_ExportTable2 (51,765 records) \n",
      "   5. ParcelsData_Maui_ExportTable3 (51,765 records) \n",
      "   6. ParcelsData_Maui_ExportTable4 (51,765 records) \n",
      "   7. bedrooms_out_ExportTable1 (70,322 records) \n",
      "\n",
      "Step 2: Wells data candidates found:\n",
      "‚ùå NO WELLS DATA FOUND ANYWHERE!\n",
      "Check if wells data exists in a different location\n",
      "\n",
      "=== WELLS STATUS ===\n",
      "Wells fields in master table: NONE\n",
      "Status: ‚ùå STILL MISSING\n"
     ]
    }
   ],
   "source": [
    "print(\"=== DIRECT WELLS DATA SEARCH AND FIX ===\")\n",
    "print(\"Searching geodatabase systematically for wells data\")\n",
    "print()\n",
    "\n",
    "gdb_path = r\"C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\ParcelAnalysis.gdb\"\n",
    "master_table = os.path.join(gdb_path, \"Complete_Master_Analysis_Table\")\n",
    "\n",
    "# Set workspace to geodatabase\n",
    "arcpy.env.workspace = gdb_path\n",
    "\n",
    "print(\"Step 1: Complete geodatabase inventory...\")\n",
    "\n",
    "# List EVERYTHING in the geodatabase\n",
    "all_feature_classes = arcpy.ListFeatureClasses()\n",
    "all_tables = arcpy.ListTables()\n",
    "all_datasets = arcpy.ListDatasets()\n",
    "\n",
    "print(f\"Feature classes ({len(all_feature_classes)}):\")\n",
    "wells_candidates = []\n",
    "for i, fc in enumerate(all_feature_classes, 1):\n",
    "    try:\n",
    "        count = int(arcpy.management.GetCount(fc)[0])\n",
    "        # Check for wells in name or fields\n",
    "        fields = [f.name for f in arcpy.ListFields(fc)]\n",
    "        has_wells = any('well' in fc.lower() for x in ['well']) or any('well' in f.lower() or 'distance' in f.lower() for f in fields)\n",
    "        \n",
    "        print(f\"  {i:2}. {fc} ({count:,} records) {'üîç WELLS?' if has_wells else ''}\")\n",
    "        \n",
    "        if has_wells or 'well' in fc.lower():\n",
    "            wells_candidates.append({\n",
    "                'name': fc,\n",
    "                'type': 'feature_class',\n",
    "                'count': count,\n",
    "                'fields': fields\n",
    "            })\n",
    "    except:\n",
    "        print(f\"  {i:2}. {fc} (error reading)\")\n",
    "\n",
    "if all_tables:\n",
    "    print(f\"\\nTables ({len(all_tables)}):\")\n",
    "    for i, table in enumerate(all_tables, 1):\n",
    "        try:\n",
    "            count = int(arcpy.management.GetCount(table)[0])\n",
    "            fields = [f.name for f in arcpy.ListFields(table)]\n",
    "            has_wells = 'well' in table.lower() or any('well' in f.lower() for f in fields)\n",
    "            \n",
    "            print(f\"  {i:2}. {table} ({count:,} records) {'üîç WELLS?' if has_wells else ''}\")\n",
    "            \n",
    "            if has_wells:\n",
    "                wells_candidates.append({\n",
    "                    'name': table,\n",
    "                    'type': 'table', \n",
    "                    'count': count,\n",
    "                    'fields': fields\n",
    "                })\n",
    "        except:\n",
    "            print(f\"  {i:2}. {table} (error reading)\")\n",
    "\n",
    "if all_datasets:\n",
    "    print(f\"\\nFeature datasets ({len(all_datasets)}):\")\n",
    "    for dataset in all_datasets:\n",
    "        print(f\"  üìÅ {dataset}\")\n",
    "        \n",
    "        # List contents of feature dataset\n",
    "        arcpy.env.workspace = os.path.join(gdb_path, dataset)\n",
    "        dataset_fcs = arcpy.ListFeatureClasses()\n",
    "        \n",
    "        for fc in dataset_fcs:\n",
    "            try:\n",
    "                count = int(arcpy.management.GetCount(fc)[0])\n",
    "                full_path = os.path.join(gdb_path, dataset, fc)\n",
    "                fields = [f.name for f in arcpy.ListFields(full_path)]\n",
    "                has_wells = 'well' in fc.lower() or any('well' in f.lower() for f in fields)\n",
    "                \n",
    "                print(f\"    ‚îî‚îÄ {fc} ({count:,} records) {'üîç WELLS?' if has_wells else ''}\")\n",
    "                \n",
    "                if has_wells:\n",
    "                    wells_candidates.append({\n",
    "                        'name': f\"{dataset}/{fc}\",\n",
    "                        'full_path': full_path,\n",
    "                        'type': 'feature_class_in_dataset',\n",
    "                        'count': count,\n",
    "                        'fields': fields\n",
    "                    })\n",
    "            except:\n",
    "                print(f\"    ‚îî‚îÄ {fc} (error reading)\")\n",
    "\n",
    "# Reset workspace\n",
    "arcpy.env.workspace = gdb_path\n",
    "\n",
    "print(f\"\\nStep 2: Wells data candidates found:\")\n",
    "if wells_candidates:\n",
    "    for i, candidate in enumerate(wells_candidates, 1):\n",
    "        well_fields = [f for f in candidate['fields'] if 'well' in f.lower() or 'distance' in f.lower()]\n",
    "        print(f\"  {i}. {candidate['name']} ({candidate['count']:,} records)\")\n",
    "        print(f\"     Wells fields: {', '.join(well_fields)}\")\n",
    "        \n",
    "    # Use the best candidate (most records with wells data)\n",
    "    best_wells = max(wells_candidates, key=lambda x: len([f for f in x['fields'] if 'well' in f.lower()]))\n",
    "    \n",
    "    print(f\"\\nStep 3: Using best wells source: {best_wells['name']}\")\n",
    "    \n",
    "    wells_source = best_wells.get('full_path') or os.path.join(gdb_path, best_wells['name'])\n",
    "    wells_fields = [f for f in best_wells['fields'] if 'well' in f.lower() or 'distance' in f.lower()]\n",
    "    \n",
    "    print(f\"Adding wells fields: {', '.join(wells_fields)}\")\n",
    "    \n",
    "    try:\n",
    "        arcpy.management.JoinField(\n",
    "            master_table,       # Target\n",
    "            \"TMK\",             # Target field\n",
    "            wells_source,      # Wells source\n",
    "            \"TMK\",             # Source field\n",
    "            wells_fields       # Fields to add\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ WELLS DATA ADDED TO MASTER TABLE!\")\n",
    "        \n",
    "        # Verify\n",
    "        master_fields = [f.name for f in arcpy.ListFields(master_table)]\n",
    "        added_wells = [f for f in master_fields if 'well' in f.lower()]\n",
    "        print(f\"Wells fields now in master: {', '.join(added_wells)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Wells join failed: {str(e)}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå NO WELLS DATA FOUND ANYWHERE!\")\n",
    "    print(\"Check if wells data exists in a different location\")\n",
    "\n",
    "print(f\"\\n=== WELLS STATUS ===\")\n",
    "master_fields = [f.name for f in arcpy.ListFields(master_table)]\n",
    "wells_in_master = [f for f in master_fields if 'well' in f.lower()]\n",
    "print(f\"Wells fields in master table: {wells_in_master if wells_in_master else 'NONE'}\")\n",
    "print(f\"Status: {'‚úÖ FIXED' if wells_in_master else '‚ùå STILL MISSING'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== IMPORTING WELLS DATA FROM CORRECT LOCATION ===\n",
      "Source: C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\data\\gis_downloads\\wells\\statewide\n",
      "\n",
      "Step 1: Inventory wells folder...\n",
      "Wells folder exists: True\n",
      "Wells files found:\n",
      "  1. CPs_Distance_to_Domestic_Wells (82,141 records)\n",
      "  2. CPs_Distance_to_Municipal_Wells (82,141 records)\n",
      "\n",
      "Step 2: Import wells data to geodatabase...\n",
      "  ‚úÖ Imported Wells_CPs_Distance_to_Domestic_Wells: 82,141 records\n",
      "  ‚úÖ Imported Wells_CPs_Distance_to_Municipal_Wells: 82,141 records\n",
      "\n",
      "Step 3: Join wells data to master table...\n",
      "Using wells source: Wells_CPs_Distance_to_Domestic_Wells\n",
      "Wells fields to join: TMK\n",
      "Join field: TMK\n",
      "‚úÖ WELLS DATA SUCCESSFULLY JOINED TO MASTER TABLE!\n",
      "Wells fields now in master table: \n",
      "\n",
      "=== FINAL WELLS STATUS ===\n",
      "‚ùå Wells data still missing from master table\n"
     ]
    }
   ],
   "source": [
    "print(\"=== IMPORTING WELLS DATA FROM CORRECT LOCATION ===\")\n",
    "print(\"Source: C:\\\\Users\\\\rober\\\\OneDrive\\\\Documents\\\\GIS_Projects\\\\ParcelAnalysis\\\\data\\\\gis_downloads\\\\wells\\\\statewide\")\n",
    "print()\n",
    "\n",
    "import os\n",
    "import arcpy\n",
    "\n",
    "# Define paths\n",
    "wells_folder = r\"C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\data\\gis_downloads\\wells\\statewide\"\n",
    "gdb_path = r\"C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\ParcelAnalysis.gdb\"\n",
    "master_table = os.path.join(gdb_path, \"Complete_Master_Analysis_Table\")\n",
    "\n",
    "print(\"Step 1: Inventory wells folder...\")\n",
    "print(f\"Wells folder exists: {os.path.exists(wells_folder)}\")\n",
    "\n",
    "if os.path.exists(wells_folder):\n",
    "    wells_files = []\n",
    "    for file in os.listdir(wells_folder):\n",
    "        file_path = os.path.join(wells_folder, file)\n",
    "        if file.endswith('.shp'):\n",
    "            try:\n",
    "                count = int(arcpy.management.GetCount(file_path)[0])\n",
    "                wells_files.append({\n",
    "                    'name': file[:-4],  # Remove .shp\n",
    "                    'path': file_path,\n",
    "                    'count': count\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"  Error reading {file}: {str(e)}\")\n",
    "    \n",
    "    if wells_files:\n",
    "        print(\"Wells files found:\")\n",
    "        for i, file_info in enumerate(wells_files, 1):\n",
    "            print(f\"  {i}. {file_info['name']} ({file_info['count']:,} records)\")\n",
    "            \n",
    "            # Check fields in each wells file\n",
    "            fields = [f.name for f in arcpy.ListFields(file_info['path'])]\n",
    "            wells_fields = [f for f in fields if any(keyword in f.lower() for keyword in ['well', 'distance', 'municipal', 'domestic'])]\n",
    "            if wells_fields:\n",
    "                print(f\"     Wells fields: {', '.join(wells_fields)}\")\n",
    "    \n",
    "        print(f\"\\nStep 2: Import wells data to geodatabase...\")\n",
    "        \n",
    "        # Import each wells file to geodatabase\n",
    "        imported_wells = []\n",
    "        for file_info in wells_files:\n",
    "            target_name = f\"Wells_{file_info['name']}\"\n",
    "            target_path = os.path.join(gdb_path, target_name)\n",
    "            \n",
    "            try:\n",
    "                # Remove if exists\n",
    "                if arcpy.Exists(target_path):\n",
    "                    arcpy.management.Delete(target_path)\n",
    "                \n",
    "                # Import\n",
    "                arcpy.management.CopyFeatures(file_info['path'], target_path)\n",
    "                count = int(arcpy.management.GetCount(target_path)[0])\n",
    "                \n",
    "                print(f\"  ‚úÖ Imported {target_name}: {count:,} records\")\n",
    "                \n",
    "                fields = [f.name for f in arcpy.ListFields(target_path)]\n",
    "                wells_fields = [f for f in fields if any(keyword in f.lower() for keyword in ['well', 'distance', 'municipal', 'domestic', 'tmk'])]\n",
    "                \n",
    "                imported_wells.append({\n",
    "                    'name': target_name,\n",
    "                    'path': target_path,\n",
    "                    'count': count,\n",
    "                    'fields': fields,\n",
    "                    'wells_fields': wells_fields\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Failed to import {file_info['name']}: {str(e)}\")\n",
    "        \n",
    "        if imported_wells:\n",
    "            print(f\"\\nStep 3: Join wells data to master table...\")\n",
    "            \n",
    "            # Use the wells dataset with the most records or best field coverage\n",
    "            best_wells = max(imported_wells, key=lambda x: len(x['wells_fields']))\n",
    "            \n",
    "            print(f\"Using wells source: {best_wells['name']}\")\n",
    "            print(f\"Wells fields to join: {', '.join(best_wells['wells_fields'])}\")\n",
    "            \n",
    "            # Check if wells data has TMK field for joining\n",
    "            tmk_fields = [f for f in best_wells['fields'] if 'tmk' in f.lower()]\n",
    "            if tmk_fields:\n",
    "                join_field = tmk_fields[0]\n",
    "                print(f\"Join field: {join_field}\")\n",
    "                \n",
    "                # Get only the distance/wells fields (not TMK)\n",
    "                distance_fields = [f for f in best_wells['wells_fields'] if 'tmk' not in f.lower()]\n",
    "                \n",
    "                try:\n",
    "                    arcpy.management.JoinField(\n",
    "                        master_table,           # Target (master table)\n",
    "                        \"TMK\",                  # Target join field\n",
    "                        best_wells['path'],     # Source (wells data)\n",
    "                        join_field,             # Source join field\n",
    "                        distance_fields         # Fields to add (distances only)\n",
    "                    )\n",
    "                    \n",
    "                    print(f\"‚úÖ WELLS DATA SUCCESSFULLY JOINED TO MASTER TABLE!\")\n",
    "                    \n",
    "                    # Verify the join\n",
    "                    master_fields = [f.name for f in arcpy.ListFields(master_table)]\n",
    "                    added_wells_fields = [f for f in master_fields if any(keyword in f.lower() for keyword in ['well', 'distance', 'municipal', 'domestic'])]\n",
    "                    \n",
    "                    print(f\"Wells fields now in master table: {', '.join(added_wells_fields)}\")\n",
    "                    \n",
    "                    # Test data coverage\n",
    "                    if added_wells_fields:\n",
    "                        with arcpy.da.SearchCursor(master_table, [\"TMK\"] + added_wells_fields[:2]) as cursor:\n",
    "                            sample_count = 0\n",
    "                            wells_data_count = 0\n",
    "                            \n",
    "                            for row in cursor:\n",
    "                                sample_count += 1\n",
    "                                if len(row) > 1 and row[1] is not None:\n",
    "                                    wells_data_count += 1\n",
    "                                if sample_count >= 10000:\n",
    "                                    break\n",
    "                        \n",
    "                        coverage = wells_data_count/sample_count*100 if sample_count > 0 else 0\n",
    "                        print(f\"Wells data coverage: {wells_data_count:,}/{sample_count:,} ({coverage:.1f}%)\")\n",
    "                        \n",
    "                        if coverage > 70:\n",
    "                            print(f\"\\nüéâ WELLS DATA INTEGRATION SUCCESS!\")\n",
    "                            print(f\"‚úÖ Master table now complete with wells distances\")\n",
    "                            print(f\"‚úÖ HAR 11-62 compliance ready (1000ft setbacks)\")\n",
    "                            print(f\"‚úÖ Technology Matrix fully supported!\")\n",
    "                        else:\n",
    "                            print(f\"\\n‚ö†Ô∏è Lower wells coverage than expected\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Wells join failed: {str(e)}\")\n",
    "                    \n",
    "            else:\n",
    "                print(f\"‚ùå No TMK field found in wells data for joining\")\n",
    "                print(f\"Available fields: {', '.join(best_wells['fields'])}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No shapefile wells data found in wells folder\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Wells folder not found!\")\n",
    "    print(\"Please verify the path is correct\")\n",
    "\n",
    "print(f\"\\n=== FINAL WELLS STATUS ===\")\n",
    "if arcpy.Exists(master_table):\n",
    "    master_fields = [f.name for f in arcpy.ListFields(master_table)]\n",
    "    wells_fields = [f for f in master_fields if any(keyword in f.lower() for keyword in ['well', 'distance', 'municipal', 'domestic'])]\n",
    "    \n",
    "    if wells_fields:\n",
    "        print(f\"‚úÖ Wells data in master table: {', '.join(wells_fields)}\")\n",
    "        print(f\"‚úÖ Master table complete and ready for Matrix analysis!\")\n",
    "    else:\n",
    "        print(f\"‚ùå Wells data still missing from master table\")\n",
    "else:\n",
    "    print(f\"‚ùå Master table not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DIAGNOSING WELLS DATA FIELD NAMES ===\n",
      "Checking what fields actually exist in the wells datasets\n",
      "\n",
      "Examining actual field names in wells datasets...\n",
      "\n",
      "Wells_CPs_Distance_to_Domestic_Wells:\n",
      "  All fields: OBJECTID (OID), Shape (Geometry), X (Double), Y (Double), Island (String), TMK (Integer), Uid (String), dist2_DomW (Double)\n",
      "  Numeric fields (likely distances): X, Y, dist2_DomW\n",
      "  Sample values:\n",
      "    TMK 186006001: {'X': -158.150145493, 'Y': 21.4425027002}\n",
      "    TMK 186006001: {'X': -158.150088955, 'Y': 21.4422976382}\n",
      "    TMK 186006001: {'X': -158.150329285, 'Y': 21.4426010061}\n",
      "\n",
      "Wells_CPs_Distance_to_Municipal_Wells:\n",
      "  All fields: OBJECTID (OID), Shape (Geometry), X (Double), Y (Double), Island (String), TMK (Integer), Uid (String), dist2_MunW (Double)\n",
      "  Numeric fields (likely distances): X, Y, dist2_MunW\n",
      "  Sample values:\n",
      "    TMK 186006001: {'X': -158.150145493, 'Y': 21.4425027002}\n",
      "    TMK 186006001: {'X': -158.150088955, 'Y': 21.4422976382}\n",
      "    TMK 186006001: {'X': -158.150329285, 'Y': 21.4426010061}\n",
      "\n",
      "Joining the actual distance fields...\n",
      "\n",
      "Joining from Wells_CPs_Distance_to_Domestic_Wells:\n",
      "  Distance fields: X, Y, dist2_DomW\n",
      "  ‚úÖ Successfully joined 3 distance fields\n",
      "\n",
      "Joining from Wells_CPs_Distance_to_Municipal_Wells:\n",
      "  Distance fields: X, Y, dist2_MunW\n",
      "  ‚úÖ Successfully joined 3 distance fields\n",
      "\n",
      "=== FINAL VERIFICATION ===\n",
      "Distance/wells fields now in master table: dist2_MunW, dist2_DomW, dist2_DomW_1, dist2_DomW_12, dist2_MunW_1\n",
      "‚úÖ WELLS DATA SUCCESSFULLY INTEGRATED!\n",
      "‚úÖ Master table ready for HAR 11-62 analysis (1000ft setbacks)\n",
      "‚úÖ Technology Matrix fully supported!\n"
     ]
    }
   ],
   "source": [
    "print(\"=== DIAGNOSING WELLS DATA FIELD NAMES ===\")\n",
    "print(\"Checking what fields actually exist in the wells datasets\")\n",
    "print()\n",
    "\n",
    "gdb_path = r\"C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\ParcelAnalysis.gdb\"\n",
    "master_table = os.path.join(gdb_path, \"Complete_Master_Analysis_Table\")\n",
    "\n",
    "# Check the imported wells datasets\n",
    "wells_datasets = [\n",
    "    \"Wells_CPs_Distance_to_Domestic_Wells\",\n",
    "    \"Wells_CPs_Distance_to_Municipal_Wells\"\n",
    "]\n",
    "\n",
    "print(\"Examining actual field names in wells datasets...\")\n",
    "\n",
    "for wells_name in wells_datasets:\n",
    "    wells_path = os.path.join(gdb_path, wells_name)\n",
    "    if arcpy.Exists(wells_path):\n",
    "        fields = [f.name for f in arcpy.ListFields(wells_path)]\n",
    "        field_info = []\n",
    "        \n",
    "        for field in fields:\n",
    "            field_obj = next(f for f in arcpy.ListFields(wells_path) if f.name == field)\n",
    "            field_info.append(f\"{field} ({field_obj.type})\")\n",
    "        \n",
    "        print(f\"\\n{wells_name}:\")\n",
    "        print(f\"  All fields: {', '.join(field_info)}\")\n",
    "        \n",
    "        # The distance values are probably numeric fields that aren't TMK\n",
    "        numeric_fields = [f.name for f in arcpy.ListFields(wells_path) \n",
    "                         if f.type in ['Double', 'Single', 'Integer', 'SmallInteger'] \n",
    "                         and f.name not in ['OBJECTID', 'TMK']]\n",
    "        \n",
    "        print(f\"  Numeric fields (likely distances): {', '.join(numeric_fields)}\")\n",
    "        \n",
    "        # Sample the data to see values\n",
    "        if numeric_fields:\n",
    "            print(f\"  Sample values:\")\n",
    "            with arcpy.da.SearchCursor(wells_path, [\"TMK\"] + numeric_fields[:2]) as cursor:\n",
    "                for i, row in enumerate(cursor):\n",
    "                    if i < 3:\n",
    "                        print(f\"    TMK {row[0]}: {dict(zip(numeric_fields[:2], row[1:]))}\")\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "# Now join the correct fields\n",
    "print(f\"\\nJoining the actual distance fields...\")\n",
    "\n",
    "for wells_name in wells_datasets:\n",
    "    wells_path = os.path.join(gdb_path, wells_name)\n",
    "    if arcpy.Exists(wells_path):\n",
    "        # Get numeric fields that aren't system fields\n",
    "        numeric_fields = [f.name for f in arcpy.ListFields(wells_path) \n",
    "                         if f.type in ['Double', 'Single', 'Integer', 'SmallInteger'] \n",
    "                         and f.name not in ['OBJECTID', 'TMK', 'Shape_Length', 'Shape_Area']]\n",
    "        \n",
    "        if numeric_fields:\n",
    "            print(f\"\\nJoining from {wells_name}:\")\n",
    "            print(f\"  Distance fields: {', '.join(numeric_fields)}\")\n",
    "            \n",
    "            try:\n",
    "                arcpy.management.JoinField(\n",
    "                    master_table,       # Target\n",
    "                    \"TMK\",             # Target field\n",
    "                    wells_path,        # Source\n",
    "                    \"TMK\",             # Source field\n",
    "                    numeric_fields     # Actual distance fields\n",
    "                )\n",
    "                \n",
    "                print(f\"  ‚úÖ Successfully joined {len(numeric_fields)} distance fields\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Join failed: {str(e)}\")\n",
    "\n",
    "# Final verification\n",
    "print(f\"\\n=== FINAL VERIFICATION ===\")\n",
    "\n",
    "master_fields = [f.name for f in arcpy.ListFields(master_table)]\n",
    "distance_fields = [f for f in master_fields if any(keyword in f.lower() \n",
    "                  for keyword in ['distance', 'dist', 'near', 'buffer', 'well', 'municipal', 'domestic'])]\n",
    "\n",
    "print(f\"Distance/wells fields now in master table: {', '.join(distance_fields)}\")\n",
    "\n",
    "if distance_fields:\n",
    "    print(f\"‚úÖ WELLS DATA SUCCESSFULLY INTEGRATED!\")\n",
    "    print(f\"‚úÖ Master table ready for HAR 11-62 analysis (1000ft setbacks)\")\n",
    "    print(f\"‚úÖ Technology Matrix fully supported!\")\n",
    "else:\n",
    "    print(f\"‚ùå Wells distance fields still not found\")\n",
    "    print(\"Need to examine the wells data structure more carefully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPREHENSIVE BUILDING FOOTPRINTS BY COUNTY ===\n",
      "Processing all counties with appropriate data sources\n",
      "\n",
      "Step 1: Verify source layers and master table...\n",
      "Available source layers:\n",
      "  Honolulu: Building Footprints CCH ‚ùå\n",
      "  Maui: Maui County Building Footprints circa 2023 ‚ùå\n",
      "  Hawaii: USA_Structures_B ‚ùå\n",
      "  Kauai: USA_Structures_B ‚ùå\n",
      "\n",
      "Master table: 82,141 parcels ‚úÖ\n",
      "\n",
      "Step 2: Processing each county...\n",
      "\n",
      "--- Processing Honolulu County ---\n",
      "‚ùå Source layer not available: Building Footprints CCH\n",
      "\n",
      "--- Processing Maui County ---\n",
      "‚ùå Source layer not available: Maui County Building Footprints circa 2023\n",
      "\n",
      "--- Processing Hawaii County ---\n",
      "‚ùå Source layer not available: USA_Structures_B\n",
      "\n",
      "--- Processing Kauai County ---\n",
      "‚ùå Source layer not available: USA_Structures_B\n",
      "\n",
      "Step 3: Creating building area summary for master table...\n",
      "\n",
      "=== BUILDING FOOTPRINTS PROCESSING COMPLETE ===\n",
      "County results:\n",
      "\n",
      "Master table updated:\n",
      "  Total buildings processed: 0\n",
      "  Field added: Area_Under_Structures\n",
      "\n",
      "Building coverage by county:\n",
      "  Honolulu: 0 parcels with buildings\n",
      "         0 sq ft total building area\n",
      "  Kauai: 0 parcels with buildings\n",
      "         0 sq ft total building area\n",
      "  None: 0 parcels with buildings\n",
      "         0 sq ft total building area\n",
      "  Maui: 0 parcels with buildings\n",
      "         0 sq ft total building area\n",
      "  Hawaii: 0 parcels with buildings\n",
      "         0 sq ft total building area\n",
      "\n",
      "‚úÖ Ready for disposal field analysis!\n",
      "‚úÖ Building footprints available by county\n",
      "‚úÖ Master table has Area_Under_Structures field\n"
     ]
    }
   ],
   "source": [
    "print(\"=== COMPREHENSIVE BUILDING FOOTPRINTS BY COUNTY ===\")\n",
    "print(\"Processing all counties with appropriate data sources\")\n",
    "print()\n",
    "\n",
    "import arcpy\n",
    "import os\n",
    "\n",
    "# Define paths and settings\n",
    "gdb_path = r\"C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\ParcelAnalysis.gdb\"\n",
    "master_table = os.path.join(gdb_path, \"Complete_Master_Analysis_Table\")\n",
    "\n",
    "# County-specific data sources and processing plan\n",
    "county_sources = {\n",
    "    \"Honolulu\": {\n",
    "        \"source_layer\": \"Building Footprints CCH\",\n",
    "        \"island\": \"Oahu\",\n",
    "        \"output_name\": \"Honolulu_Building_Footprints\"\n",
    "    },\n",
    "    \"Maui\": {\n",
    "        \"source_layer\": \"Maui County Building Footprints circa 2023\", \n",
    "        \"island\": \"Maui\",\n",
    "        \"output_name\": \"Maui_Building_Footprints\"\n",
    "    },\n",
    "    \"Hawaii\": {\n",
    "        \"source_layer\": \"USA_Structures_B\",\n",
    "        \"island\": \"Hawaii\", \n",
    "        \"output_name\": \"Hawaii_Building_Footprints\"\n",
    "    },\n",
    "    \"Kauai\": {\n",
    "        \"source_layer\": \"USA_Structures_B\",\n",
    "        \"island\": \"Kauai\",\n",
    "        \"output_name\": \"Kauai_Building_Footprints\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Step 1: Verify source layers and master table...\")\n",
    "\n",
    "# Get current map layers\n",
    "aprx = arcpy.mp.ArcGISProject(\"CURRENT\")\n",
    "map_obj = aprx.listMaps()[0]\n",
    "available_layers = {layer.name: layer for layer in map_obj.listLayers()}\n",
    "\n",
    "print(f\"Available source layers:\")\n",
    "for county, config in county_sources.items():\n",
    "    source_available = config[\"source_layer\"] in available_layers\n",
    "    print(f\"  {county}: {config['source_layer']} {'‚úÖ' if source_available else '‚ùå'}\")\n",
    "\n",
    "# Check master table\n",
    "if arcpy.Exists(master_table):\n",
    "    parcel_count = int(arcpy.management.GetCount(master_table)[0])\n",
    "    print(f\"\\nMaster table: {parcel_count:,} parcels ‚úÖ\")\n",
    "else:\n",
    "    print(f\"\\nMaster table: Missing ‚ùå\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\nStep 2: Processing each county...\")\n",
    "\n",
    "county_results = {}\n",
    "\n",
    "for county, config in county_sources.items():\n",
    "    print(f\"\\n--- Processing {county} County ---\")\n",
    "    \n",
    "    if config[\"source_layer\"] not in available_layers:\n",
    "        print(f\"‚ùå Source layer not available: {config['source_layer']}\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Create county-specific parcel subset\n",
    "        county_parcels = os.path.join(gdb_path, f\"temp_{county}_parcels\")\n",
    "        \n",
    "        # Select parcels for this county\n",
    "        where_clause = f\"county = '{county}'\"\n",
    "        arcpy.management.Select(master_table, county_parcels, where_clause)\n",
    "        \n",
    "        county_parcel_count = int(arcpy.management.GetCount(county_parcels)[0])\n",
    "        print(f\"  County parcels: {county_parcel_count:,}\")\n",
    "        \n",
    "        if county_parcel_count == 0:\n",
    "            print(f\"  ‚ö†Ô∏è No parcels found for {county}\")\n",
    "            continue\n",
    "        \n",
    "        # Get source layer\n",
    "        source_layer = available_layers[config[\"source_layer\"]]\n",
    "        \n",
    "        # For USA_Structures_B, pre-filter by island to improve performance\n",
    "        if config[\"source_layer\"] == \"USA_Structures_B\":\n",
    "            print(f\"  Pre-filtering USA structures for {config['island']} island...\")\n",
    "            \n",
    "            # Create island boundary from county parcels\n",
    "            island_boundary = os.path.join(gdb_path, f\"temp_{county}_boundary\")\n",
    "            arcpy.management.MinimumBoundingGeometry(\n",
    "                county_parcels,\n",
    "                island_boundary, \n",
    "                \"CONVEX_HULL\",\n",
    "                \"ALL\"\n",
    "            )\n",
    "            \n",
    "            # Select buildings within island boundary first\n",
    "            arcpy.management.SelectLayerByLocation(\n",
    "                source_layer,\n",
    "                \"INTERSECT\", \n",
    "                island_boundary,\n",
    "                selection_type=\"NEW_SELECTION\"\n",
    "            )\n",
    "            \n",
    "            island_building_count = int(arcpy.management.GetCount(source_layer)[0])\n",
    "            print(f\"  Buildings in {config['island']}: {island_building_count:,}\")\n",
    "            \n",
    "            # Clean up boundary\n",
    "            arcpy.management.Delete(island_boundary)\n",
    "        \n",
    "        # Select buildings that intersect (touch or overlap) county parcels\n",
    "        print(f\"  Selecting buildings intersecting {county} parcels...\")\n",
    "        \n",
    "        arcpy.management.SelectLayerByLocation(\n",
    "            source_layer,\n",
    "            \"INTERSECT\",\n",
    "            county_parcels,\n",
    "            selection_type=\"SUBSET_SELECTION\" if config[\"source_layer\"] == \"USA_Structures_B\" else \"NEW_SELECTION\"\n",
    "        )\n",
    "        \n",
    "        selected_count = int(arcpy.management.GetCount(source_layer)[0])\n",
    "        print(f\"  Selected buildings: {selected_count:,}\")\n",
    "        \n",
    "        if selected_count > 0:\n",
    "            # Export buildings\n",
    "            output_path = os.path.join(gdb_path, config[\"output_name\"])\n",
    "            \n",
    "            if arcpy.Exists(output_path):\n",
    "                arcpy.management.Delete(output_path)\n",
    "            \n",
    "            arcpy.management.CopyFeatures(source_layer, output_path)\n",
    "            \n",
    "            # Add TMK assignment field\n",
    "            arcpy.management.AddField(output_path, \"Assigned_TMK\", \"TEXT\", \"\", \"\", 20)\n",
    "            \n",
    "            print(f\"  Exported: {config['output_name']}\")\n",
    "            \n",
    "            # Assign each building to the parcel containing most of its area\n",
    "            print(f\"  Assigning buildings to TMKs...\")\n",
    "            \n",
    "            # Spatial join to find overlapping parcels for each building\n",
    "            join_table = os.path.join(gdb_path, f\"temp_{county}_building_parcel_join\")\n",
    "            \n",
    "            arcpy.analysis.SpatialJoin(\n",
    "                output_path,           # Target (buildings)\n",
    "                county_parcels,        # Join (parcels) \n",
    "                join_table,\n",
    "                \"JOIN_ONE_TO_MANY\",    # Multiple parcels per building\n",
    "                \"KEEP_ALL\",\n",
    "                match_option=\"INTERSECT\"\n",
    "            )\n",
    "            \n",
    "            # Calculate area of intersection for each building-parcel pair\n",
    "            arcpy.management.AddField(join_table, \"Overlap_Area\", \"DOUBLE\")\n",
    "            \n",
    "            # Use geometry to calculate actual overlap area\n",
    "            with arcpy.da.UpdateCursor(join_table, [\"SHAPE@\", \"Overlap_Area\"]) as cursor:\n",
    "                for row in cursor:\n",
    "                    if row[0]:\n",
    "                        row[1] = row[0].area\n",
    "                        cursor.updateRow(row)\n",
    "            \n",
    "            # Find TMK with maximum overlap for each building\n",
    "            building_tmk_dict = {}\n",
    "            with arcpy.da.SearchCursor(join_table, [\"TARGET_FID\", \"TMK\", \"Overlap_Area\"]) as cursor:\n",
    "                for row in cursor:\n",
    "                    building_fid = row[0]\n",
    "                    tmk = row[1] \n",
    "                    area = row[2]\n",
    "                    \n",
    "                    if building_fid not in building_tmk_dict or area > building_tmk_dict[building_fid][1]:\n",
    "                        building_tmk_dict[building_fid] = (tmk, area)\n",
    "            \n",
    "            # Update buildings with assigned TMK\n",
    "            with arcpy.da.UpdateCursor(output_path, [\"OBJECTID\", \"Assigned_TMK\"]) as cursor:\n",
    "                for row in cursor:\n",
    "                    building_fid = row[0]\n",
    "                    if building_fid in building_tmk_dict:\n",
    "                        row[1] = str(building_tmk_dict[building_fid][0])\n",
    "                        cursor.updateRow(row)\n",
    "            \n",
    "            # Calculate building areas and summarize by TMK\n",
    "            arcpy.management.AddField(output_path, \"Building_Area_SF\", \"DOUBLE\")\n",
    "            \n",
    "            with arcpy.da.UpdateCursor(output_path, [\"SHAPE@AREA\", \"Building_Area_SF\"]) as cursor:\n",
    "                for row in cursor:\n",
    "                    row[1] = row[0]  # Area in square feet\n",
    "                    cursor.updateRow(row)\n",
    "            \n",
    "            print(f\"  ‚úÖ TMK assignment complete\")\n",
    "            \n",
    "            # Store results\n",
    "            final_count = int(arcpy.management.GetCount(output_path)[0])\n",
    "            county_results[county] = {\n",
    "                \"output\": config[\"output_name\"],\n",
    "                \"parcels\": county_parcel_count,\n",
    "                \"buildings\": final_count\n",
    "            }\n",
    "            \n",
    "            # Clean up temporary files\n",
    "            arcpy.management.Delete(join_table)\n",
    "            \n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è No buildings found for {county}\")\n",
    "        \n",
    "        # Clear selection and clean up\n",
    "        arcpy.management.SelectLayerByAttribute(source_layer, \"CLEAR_SELECTION\")\n",
    "        arcpy.management.Delete(county_parcels)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error processing {county}: {str(e)}\")\n",
    "\n",
    "print(f\"\\nStep 3: Creating building area summary for master table...\")\n",
    "\n",
    "# Add building area field to master table\n",
    "if \"Area_Under_Structures\" not in [f.name for f in arcpy.ListFields(master_table)]:\n",
    "    arcpy.management.AddField(master_table, \"Area_Under_Structures\", \"DOUBLE\")\n",
    "\n",
    "# Initialize all values to 0\n",
    "with arcpy.da.UpdateCursor(master_table, [\"Area_Under_Structures\"]) as cursor:\n",
    "    for row in cursor:\n",
    "        row[0] = 0.0\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "# Sum building areas by TMK for each county\n",
    "total_buildings_processed = 0\n",
    "\n",
    "for county, result in county_results.items():\n",
    "    if result:\n",
    "        building_layer = os.path.join(gdb_path, result[\"output\"])\n",
    "        \n",
    "        print(f\"  Processing {county} building areas...\")\n",
    "        \n",
    "        # Create summary statistics\n",
    "        tmk_building_areas = {}\n",
    "        with arcpy.da.SearchCursor(building_layer, [\"Assigned_TMK\", \"Building_Area_SF\"]) as cursor:\n",
    "            for row in cursor:\n",
    "                tmk = row[0]\n",
    "                area = row[1] if row[1] else 0\n",
    "                \n",
    "                if tmk and tmk != \"None\":\n",
    "                    tmk_building_areas[tmk] = tmk_building_areas.get(tmk, 0) + area\n",
    "                    total_buildings_processed += 1\n",
    "        \n",
    "        # Update master table with building areas\n",
    "        with arcpy.da.UpdateCursor(master_table, [\"TMK\", \"Area_Under_Structures\", \"county\"]) as cursor:\n",
    "            for row in cursor:\n",
    "                if row[2] == county and str(row[0]) in tmk_building_areas:\n",
    "                    row[1] = tmk_building_areas[str(row[0])]\n",
    "                    cursor.updateRow(row)\n",
    "        \n",
    "        print(f\"    {len(tmk_building_areas)} TMKs with buildings\")\n",
    "\n",
    "print(f\"\\n=== BUILDING FOOTPRINTS PROCESSING COMPLETE ===\")\n",
    "print(f\"County results:\")\n",
    "for county, result in county_results.items():\n",
    "    if result:\n",
    "        print(f\"  {county}: {result['buildings']:,} buildings across {result['parcels']:,} parcels\")\n",
    "        print(f\"           Dataset: {result['output']}\")\n",
    "\n",
    "print(f\"\\nMaster table updated:\")\n",
    "print(f\"  Total buildings processed: {total_buildings_processed:,}\")\n",
    "print(f\"  Field added: Area_Under_Structures\")\n",
    "\n",
    "# Verify master table updates\n",
    "with arcpy.da.SearchCursor(master_table, [\"TMK\", \"Area_Under_Structures\", \"county\"]) as cursor:\n",
    "    county_stats = {}\n",
    "    for row in cursor:\n",
    "        county = row[2]\n",
    "        area = row[1] if row[1] else 0\n",
    "        \n",
    "        if county not in county_stats:\n",
    "            county_stats[county] = {\"parcels_with_buildings\": 0, \"total_building_area\": 0}\n",
    "        \n",
    "        if area > 0:\n",
    "            county_stats[county][\"parcels_with_buildings\"] += 1\n",
    "            county_stats[county][\"total_building_area\"] += area\n",
    "\n",
    "print(f\"\\nBuilding coverage by county:\")\n",
    "for county, stats in county_stats.items():\n",
    "    print(f\"  {county}: {stats['parcels_with_buildings']:,} parcels with buildings\")\n",
    "    print(f\"         {stats['total_building_area']:,.0f} sq ft total building area\")\n",
    "\n",
    "print(f\"\\n‚úÖ Ready for disposal field analysis!\")\n",
    "print(f\"‚úÖ Building footprints available by county\")\n",
    "print(f\"‚úÖ Master table has Area_Under_Structures field\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== IMPORTING BUILDING LAYERS TO GEODATABASE ===\n",
      "Step 1: Export hosted layers to local geodatabase\n",
      "\n",
      "Available layers in current map:\n",
      "\n",
      "Importing building layers to geodatabase...\n",
      "\n",
      "‚ùå Layer not found: Building Footprints CCH\n",
      "\n",
      "‚ùå Layer not found: Maui County Building Footprints circa 2023\n",
      "\n",
      "‚ùå Layer not found: USA_Structures_B\n",
      "\n",
      "=== IMPORT SUMMARY ===\n",
      "\n",
      "‚ùå No building layers imported - check layer names\n",
      "\n",
      "Next step: Run the county processing script with local geodatabase layers\n"
     ]
    }
   ],
   "source": [
    "print(\"=== IMPORTING BUILDING LAYERS TO GEODATABASE ===\")\n",
    "print(\"Step 1: Export hosted layers to local geodatabase\")\n",
    "print()\n",
    "\n",
    "import arcpy\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "gdb_path = r\"C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\ParcelAnalysis.gdb\"\n",
    "\n",
    "# Get current map and layers\n",
    "aprx = arcpy.mp.ArcGISProject(\"CURRENT\")\n",
    "map_obj = aprx.listMaps()[0]\n",
    "\n",
    "# Define layer mappings for import\n",
    "layer_imports = {\n",
    "    \"Building Footprints CCH\": \"Honolulu_Buildings_Raw\",\n",
    "    \"Maui County Building Footprints circa 2023\": \"Maui_Buildings_Raw\", \n",
    "    \"USA_Structures_B\": \"USA_Structures_Raw\"\n",
    "}\n",
    "\n",
    "print(\"Available layers in current map:\")\n",
    "available_layers = {}\n",
    "for layer in map_obj.listLayers():\n",
    "    print(f\"  - {layer.name}\")\n",
    "    available_layers[layer.name] = layer\n",
    "\n",
    "print(f\"\\nImporting building layers to geodatabase...\")\n",
    "\n",
    "imported_layers = {}\n",
    "\n",
    "for source_name, target_name in layer_imports.items():\n",
    "    if source_name in available_layers:\n",
    "        print(f\"\\nImporting {source_name}...\")\n",
    "        \n",
    "        try:\n",
    "            source_layer = available_layers[source_name]\n",
    "            target_path = os.path.join(gdb_path, target_name)\n",
    "            \n",
    "            # Remove existing if present\n",
    "            if arcpy.Exists(target_path):\n",
    "                arcpy.management.Delete(target_path)\n",
    "                print(f\"  Removed existing {target_name}\")\n",
    "            \n",
    "            # Import the layer\n",
    "            print(f\"  Copying features to geodatabase...\")\n",
    "            arcpy.management.CopyFeatures(source_layer, target_path)\n",
    "            \n",
    "            # Verify import\n",
    "            count = int(arcpy.management.GetCount(target_path)[0])\n",
    "            print(f\"  ‚úÖ Imported {count:,} features to {target_name}\")\n",
    "            \n",
    "            imported_layers[source_name] = {\n",
    "                \"target_name\": target_name,\n",
    "                \"target_path\": target_path,\n",
    "                \"count\": count\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Failed to import {source_name}: {str(e)}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Layer not found: {source_name}\")\n",
    "\n",
    "print(f\"\\n=== IMPORT SUMMARY ===\")\n",
    "for source, info in imported_layers.items():\n",
    "    print(f\"‚úÖ {source} ‚Üí {info['target_name']} ({info['count']:,} features)\")\n",
    "\n",
    "if len(imported_layers) > 0:\n",
    "    print(f\"\\n‚úÖ Building layers successfully imported to geodatabase!\")\n",
    "    print(f\"‚úÖ Ready to run county-specific building extraction\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå No building layers imported - check layer names\")\n",
    "\n",
    "print(f\"\\nNext step: Run the county processing script with local geodatabase layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINDING BUILDING LAYERS IN MAP ===\n",
      "Scanning all layers to identify building footprint layers\n",
      "\n",
      "All layers in current map:\n",
      "\n",
      "Building layer candidates found:\n",
      "  No building layers found\n",
      "\n",
      "Please confirm the exact layer names for:\n",
      "1. Honolulu/Oahu building footprints\n",
      "2. Maui County building footprints\n",
      "3. USA Structures layer\n"
     ]
    }
   ],
   "source": [
    "print(\"=== FINDING BUILDING LAYERS IN MAP ===\")\n",
    "print(\"Scanning all layers to identify building footprint layers\")\n",
    "print()\n",
    "\n",
    "import arcpy\n",
    "\n",
    "# Get current map and list all layers\n",
    "aprx = arcpy.mp.ArcGISProject(\"CURRENT\")\n",
    "map_obj = aprx.listMaps()[0]\n",
    "\n",
    "print(\"All layers in current map:\")\n",
    "building_candidates = []\n",
    "\n",
    "for i, layer in enumerate(map_obj.listLayers(), 1):\n",
    "    layer_name = layer.name\n",
    "    \n",
    "    # Look for building-related keywords\n",
    "    is_building_layer = any(keyword in layer_name.lower() for keyword in \n",
    "                           ['building', 'footprint', 'structure', 'usa_structures'])\n",
    "    \n",
    "    print(f\"{i:2}. {layer_name} {'üè¢' if is_building_layer else ''}\")\n",
    "    \n",
    "    if is_building_layer:\n",
    "        try:\n",
    "            # Try to get basic info about the layer\n",
    "            if hasattr(layer, 'dataSource'):\n",
    "                print(f\"     Source: {layer.dataSource}\")\n",
    "            \n",
    "            # Try to get count (this might fail for some hosted layers)\n",
    "            try:\n",
    "                count = int(arcpy.management.GetCount(layer)[0])\n",
    "                print(f\"     Count: {count:,} features\")\n",
    "                \n",
    "                building_candidates.append({\n",
    "                    'name': layer_name,\n",
    "                    'layer': layer,\n",
    "                    'count': count\n",
    "                })\n",
    "            except:\n",
    "                print(f\"     Count: Unable to determine (hosted layer)\")\n",
    "                building_candidates.append({\n",
    "                    'name': layer_name,\n",
    "                    'layer': layer,\n",
    "                    'count': 'Unknown'\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"     Error getting info: {str(e)}\")\n",
    "\n",
    "print(f\"\\nBuilding layer candidates found:\")\n",
    "if building_candidates:\n",
    "    for candidate in building_candidates:\n",
    "        print(f\"  - {candidate['name']} ({candidate['count']} features)\")\n",
    "else:\n",
    "    print(\"  No building layers found\")\n",
    "\n",
    "print(f\"\\nPlease confirm the exact layer names for:\")\n",
    "print(\"1. Honolulu/Oahu building footprints\")\n",
    "print(\"2. Maui County building footprints\") \n",
    "print(\"3. USA Structures layer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== IMPORTING BUILDING LAYERS TO GEODATABASE (FIXED) ===\n",
      "Using exact layer names from Contents pane\n",
      "\n",
      "Debug: Checking layer access method...\n",
      "Total layers found: 0\n",
      "\n",
      "Attempting imports...\n",
      "\n",
      "Processing: Building Footprints CCH\n",
      "  ‚ùå Layer not found: Building Footprints CCH\n",
      "\n",
      "Processing: Maui County Building Footprints circa 2023\n",
      "  ‚ùå Layer not found: Maui County Building Footprints circa 2023\n",
      "\n",
      "Processing: USA_Structures_B\n",
      "  ‚ùå Layer not found: USA_Structures_B\n",
      "\n",
      "=== IMPORT SUMMARY ===\n",
      "\n",
      "‚ùå No building layers imported - need to investigate layer access\n",
      "\n",
      "Next step: Run county processing with imported geodatabase layers\n"
     ]
    }
   ],
   "source": [
    "print(\"=== IMPORTING BUILDING LAYERS TO GEODATABASE (FIXED) ===\")\n",
    "print(\"Using exact layer names from Contents pane\")\n",
    "print()\n",
    "\n",
    "import arcpy\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "gdb_path = r\"C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\ParcelAnalysis.gdb\"\n",
    "\n",
    "# Get current map\n",
    "aprx = arcpy.mp.ArcGISProject(\"CURRENT\")\n",
    "map_obj = aprx.listMaps()[0]\n",
    "\n",
    "# Define exact layer names from your Contents pane\n",
    "layer_imports = {\n",
    "    \"Building Footprints CCH\": \"Honolulu_Buildings_Raw\",\n",
    "    \"Maui County Building Footprints circa 2023\": \"Maui_Buildings_Raw\", \n",
    "    \"USA_Structures_B\": \"USA_Structures_Raw\"\n",
    "}\n",
    "\n",
    "print(\"Debug: Checking layer access method...\")\n",
    "\n",
    "# Try different methods to access layers\n",
    "all_layers = map_obj.listLayers()\n",
    "print(f\"Total layers found: {len(all_layers)}\")\n",
    "\n",
    "for layer in all_layers:\n",
    "    layer_name = layer.name\n",
    "    print(f\"  Found layer: '{layer_name}'\")\n",
    "\n",
    "print(f\"\\nAttempting imports...\")\n",
    "\n",
    "imported_layers = {}\n",
    "\n",
    "for source_name, target_name in layer_imports.items():\n",
    "    print(f\"\\nProcessing: {source_name}\")\n",
    "    \n",
    "    # Find the layer by exact name match\n",
    "    target_layer = None\n",
    "    for layer in all_layers:\n",
    "        if layer.name == source_name:\n",
    "            target_layer = layer\n",
    "            break\n",
    "    \n",
    "    if target_layer:\n",
    "        print(f\"  ‚úÖ Found layer: {source_name}\")\n",
    "        \n",
    "        try:\n",
    "            target_path = os.path.join(gdb_path, target_name)\n",
    "            \n",
    "            # Remove existing if present\n",
    "            if arcpy.Exists(target_path):\n",
    "                arcpy.management.Delete(target_path)\n",
    "                print(f\"  Removed existing {target_name}\")\n",
    "            \n",
    "            # Import the layer - try different approaches\n",
    "            print(f\"  Attempting to copy features...\")\n",
    "            \n",
    "            # Method 1: Direct copy\n",
    "            try:\n",
    "                arcpy.management.CopyFeatures(target_layer, target_path)\n",
    "                print(f\"  Method 1 (direct copy) succeeded\")\n",
    "            except:\n",
    "                print(f\"  Method 1 failed, trying method 2...\")\n",
    "                # Method 2: Copy via layer reference\n",
    "                arcpy.conversion.FeatureClassToFeatureClass(\n",
    "                    target_layer, \n",
    "                    gdb_path, \n",
    "                    target_name\n",
    "                )\n",
    "                print(f\"  Method 2 (conversion) succeeded\")\n",
    "            \n",
    "            # Verify import\n",
    "            count = int(arcpy.management.GetCount(target_path)[0])\n",
    "            print(f\"  ‚úÖ Imported {count:,} features to {target_name}\")\n",
    "            \n",
    "            imported_layers[source_name] = {\n",
    "                \"target_name\": target_name,\n",
    "                \"target_path\": target_path,\n",
    "                \"count\": count\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Failed to import {source_name}: {str(e)}\")\n",
    "            \n",
    "            # Additional debug info\n",
    "            try:\n",
    "                print(f\"     Layer type: {type(target_layer)}\")\n",
    "                print(f\"     Layer data source: {target_layer.dataSource if hasattr(target_layer, 'dataSource') else 'No dataSource'}\")\n",
    "            except:\n",
    "                print(f\"     Could not get layer details\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå Layer not found: {source_name}\")\n",
    "\n",
    "print(f\"\\n=== IMPORT SUMMARY ===\")\n",
    "for source, info in imported_layers.items():\n",
    "    print(f\"‚úÖ {source} ‚Üí {info['target_name']} ({info['count']:,} features)\")\n",
    "\n",
    "if len(imported_layers) > 0:\n",
    "    print(f\"\\n‚úÖ Building layers successfully imported to geodatabase!\")\n",
    "    print(f\"‚úÖ Ready to run county-specific building extraction\")\n",
    "    \n",
    "    # Show what's now in the geodatabase\n",
    "    print(f\"\\nVerifying imports in geodatabase:\")\n",
    "    arcpy.env.workspace = gdb_path\n",
    "    feature_classes = arcpy.ListFeatureClasses(\"*Buildings_Raw\")\n",
    "    for fc in feature_classes:\n",
    "        count = int(arcpy.management.GetCount(fc)[0])\n",
    "        print(f\"  {fc}: {count:,} features\")\n",
    "        \n",
    "else:\n",
    "    print(f\"\\n‚ùå No building layers imported - need to investigate layer access\")\n",
    "\n",
    "print(f\"\\nNext step: Run county processing with imported geodatabase layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXTRACTING HAWAII BUILDINGS VIA PYTHON ===\n",
      "Using spatial intersection with Hawaii parcels\n",
      "\n",
      "‚ùå USA_Structures_B layer not found in map\n"
     ]
    }
   ],
   "source": [
    "print(\"=== EXTRACTING HAWAII BUILDINGS VIA PYTHON ===\")\n",
    "print(\"Using spatial intersection with Hawaii parcels\")\n",
    "print()\n",
    "\n",
    "import arcpy\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "gdb_path = r\"C:\\Users\\rober\\OneDrive\\Documents\\GIS_Projects\\ParcelAnalysis\\ParcelAnalysis.gdb\"\n",
    "master_table = os.path.join(gdb_path, \"Complete_Master_Analysis_Table\")\n",
    "\n",
    "# Get the USA_Structures_B layer from your map\n",
    "aprx = arcpy.mp.ArcGISProject(\"CURRENT\")\n",
    "map_obj = aprx.listMaps()[0]\n",
    "\n",
    "usa_structures = None\n",
    "for layer in map_obj.listLayers():\n",
    "    if layer.name == \"USA_Structures_B\":\n",
    "        usa_structures = layer\n",
    "        break\n",
    "\n",
    "if usa_structures:\n",
    "    print(\"‚úÖ Found USA_Structures_B layer\")\n",
    "    \n",
    "    # Create Hawaii county boundary from master table\n",
    "    print(\"Step 1: Creating Hawaii county boundary...\")\n",
    "    hawaii_parcels = os.path.join(gdb_path, \"temp_hawaii_parcels\")\n",
    "    hawaii_boundary = os.path.join(gdb_path, \"temp_hawaii_boundary\")\n",
    "    \n",
    "    # Select Hawaii county parcels\n",
    "    arcpy.management.Select(master_table, hawaii_parcels, \"county = 'Hawaii'\")\n",
    "    hawaii_parcel_count = int(arcpy.management.GetCount(hawaii_parcels)[0])\n",
    "    print(f\"Hawaii parcels: {hawaii_parcel_count:,}\")\n",
    "    \n",
    "    # Create boundary\n",
    "    arcpy.management.MinimumBoundingGeometry(\n",
    "        hawaii_parcels, \n",
    "        hawaii_boundary,\n",
    "        \"CONVEX_HULL\",\n",
    "        \"ALL\"\n",
    "    )\n",
    "    \n",
    "    print(\"Step 2: Selecting buildings in Hawaii...\")\n",
    "    \n",
    "    # Select buildings that intersect Hawaii boundary\n",
    "    arcpy.management.SelectLayerByLocation(\n",
    "        usa_structures,\n",
    "        \"INTERSECT\",\n",
    "        hawaii_boundary,\n",
    "        selection_type=\"NEW_SELECTION\"\n",
    "    )\n",
    "    \n",
    "    selected_count = int(arcpy.management.GetCount(usa_structures)[0])\n",
    "    print(f\"Buildings selected: {selected_count:,}\")\n",
    "    \n",
    "    if selected_count > 0:\n",
    "        print(\"Step 3: Copying selected buildings...\")\n",
    "        \n",
    "        hawaii_buildings = os.path.join(gdb_path, \"Hawaii_Buildings_Raw\")\n",
    "        \n",
    "        # Remove existing\n",
    "        if arcpy.Exists(hawaii_buildings):\n",
    "            arcpy.management.Delete(hawaii_buildings)\n",
    "        \n",
    "        # Copy selected features\n",
    "        arcpy.management.CopyFeatures(usa_structures, hawaii_buildings)\n",
    "        \n",
    "        final_count = int(arcpy.management.GetCount(hawaii_buildings)[0])\n",
    "        print(f\"‚úÖ Extracted {final_count:,} Hawaii building footprints\")\n",
    "        \n",
    "        # Clear selection\n",
    "        arcpy.management.SelectLayerByAttribute(usa_structures, \"CLEAR_SELECTION\")\n",
    "        \n",
    "        # Clean up temp files\n",
    "        arcpy.management.Delete(hawaii_parcels)\n",
    "        arcpy.management.Delete(hawaii_boundary)\n",
    "        \n",
    "        print(\"\\n‚úÖ Hawaii buildings ready!\")\n",
    "        print(\"Next: Extract Kauai buildings using same method\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No buildings found in Hawaii area\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå USA_Structures_B layer not found in map\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
